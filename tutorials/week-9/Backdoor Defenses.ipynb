{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2125283-7792-400e-ba4a-5df96d240c38",
   "metadata": {},
   "source": [
    "# Backdoor Defenses\n",
    "\n",
    "In this tutorial you will learn how to perform the [Fine-Pruning](https://arxiv.org/pdf/1805.12185.pdf) and [STRIP](https://dl.acm.org/doi/pdf/10.1145/3359789.3359790) defense. Both defenses can be used against a backdoor attack. **Fine-Pruning** is a type of **Blind Backdoor Removal** defense and it is a combination of Pruning and Fine-Tuning. With Pruning one tries to remove backdoor functionality from the trained model by removing the neurons that are dormant for clean inputs. Then Fine-Tuning is applied using clean data to restore the model's performance on the original task. Pruning removes neurons that are used mostly by the poisoned inputs, fine-tuning eliminates backdoors from neurons that are used by both poisoned and clean inputs.\n",
    "\n",
    "**STRIP** is a type of **Input Filtering** defense. It is a run-time black-box defense mechanism that can be used to detect backdoor samples. You try to measure the entropy of input samples by superimposing them with random clean samples from the dataset. Doing so for N replicas will cause a high randomness in predicted output labels. That is at least the idea for clean input. Lets say your model is backdoored and the input contains a backdoor trigger. Then superimposing it with random clean samples will probably have no effect and the trigger will cause the model to predict the attacker intended target label for most of N replicas. So it causes a low randomness. By picking a suitable threshold value, you can use this method to determine if the randomness or entropy is too low and thus if you are dealing with a trojaned input sample. We will go over this entire process in the end of this tutorial. First you will train or load a (pre-trained) MNIST model and also train or load a (pre-trained) MNIST backdoored model. We will compute the clean accuracy as well as the clean accuracy drop and attack succes rate. Then we will start with Fine-Pruning.\n",
    "\n",
    "Just like in the other tutorials, you will be using the MNIST dataset and a simple CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d36db5-7af7-4fe3-8293-2a58f8aef4ee",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a21de-8ba0-4070-ab17-7d2ea2750b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting and computing\n",
    "import copy\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "# for process bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for image loading\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# PyTorch packages\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy, softmax\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096625f8-f6aa-432d-ba8f-0a0c476d497d",
   "metadata": {},
   "source": [
    "## Device\n",
    "\n",
    "We also set the device variable so that we can easily switch from using cpu to gpu (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b758ab-a0f4-4c78-a0b1-45d1a4b0ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what device we are using\n",
    "use_cuda=True\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5cc18-6171-4f9a-8c02-9ad5b9231038",
   "metadata": {},
   "source": [
    "## Random Seed\n",
    "\n",
    "Execute the code snippet below to set the random seed. This will ensure that you can reproduce results over multiple tries. So anyone who re-runs your code will get the exact same outputs.\n",
    "\n",
    "For example: we will set shuffle to True and so the training loader will randomly shuffle the data over multiple runs. If you make changes to your code because training is not going well, then setting the random seed ensures that you can perform the training with the same samples as in previous tries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e86fd-71ca-4bc1-9fb5-c470f947947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this method to be able to reproduce results over multiple tries\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    # GPU operations have a separate seed we also want to set\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "        # We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "seed = 42       \n",
    "setup_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4923933-73a0-49d1-8788-a00237f278b1",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We already introduced the MNIST dataset in a previous tutorial where we trained and tested a MLP and CNN on it. It is an illustrative dataset that is also not to big and so training a new model does not take too much time. We make use of PyTorch's `DataLoader` class to create objects that we can use to sample training and test data using batches of size 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e5569-4063-45e3-acbe-5bbd382fcbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes   = 10\n",
    "img_size    = 28\n",
    "channel     = 1\n",
    "num_workers = 0\n",
    "batch_size  = 128\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root='./data', transform=transforms.ToTensor(), download=True, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,  batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, worker_init_fn=_init_fn)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root='./data', transform=transforms.ToTensor(), download=True, train=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d821c3-6ff8-415a-acca-c2f725535d03",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "Below you find some methods to help you visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018c13f-2171-4d4e-b618-21ecb0f611b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch(images,labels):\n",
    "    # making sure we can view the images\n",
    "    images = images.detach().numpy()\n",
    "    images = images*255\n",
    "    images = [image.astype(np.uint8).reshape((28, 28, 1)) for image in images]\n",
    "    \n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    for idx in np.arange(20):\n",
    "        ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "        ax.imshow(images[idx], cmap='gray')\n",
    "        # print out the correct label for each image\n",
    "        # .item() gets the value contained in a Tensor\n",
    "        ax.set_title(str(labels[idx].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223e908-5b03-4c9e-a7e1-62b413e47389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(image,label):\n",
    "    image = image.detach().numpy()\n",
    "    image = np.squeeze(image)\n",
    "    fig = plt.figure(figsize = (12,12)) \n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(f\"Label is {str(label.item())}\")\n",
    "\n",
    "    # annotate each pixel in the image with its value\n",
    "    width, height = image.shape\n",
    "    thresh = image.max()/2.5\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            val = round(image[x][y],2) if image[x][y] !=0 else 0\n",
    "            ax.annotate(str(val), xy=(y,x),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center',\n",
    "                        color='white' if image[x][y]<thresh else 'black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6eb061-afc2-40be-9cb7-403c0261e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "visualize_batch(images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f640df-5ad6-497a-8fd9-87671e3df863",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = images[0], labels[0]\n",
    "visualize_image(image,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915907bb-3962-401f-8322-edc303be960a",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Below you find a simple CNN network, which you can train on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ecfabc-a374-4172-8bb4-618dcf7c8220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # padding added to match input size of MNIST\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2) \n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Adjusting the size of the first fully connected layer to match the output from the conv layers\n",
    "        # 5x5 is the spatial size of the image after conv and pooling layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # 10 output classes for MNIST\n",
    "        self.fc3 = nn.Linear(84, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # No ReLU after this line, output logits directly\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef83919-47ac-4a9d-afe7-9c85abea3019",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below you find the same training methods as we shared in previous tutorials. You can use these to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817d35e-4ecb-48f8-910b-8d13246997f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to compute accuracy\n",
    "def accuracy(predictions,labels):\n",
    "    _, preds = torch.max(predictions, dim=1)\n",
    "    return (torch.tensor(torch.sum(preds == labels).item() / len(preds)))\n",
    "\n",
    "# Method to log training / running loss and accuracy\n",
    "def log_training(batch_idx, running_loss, running_acc):\n",
    "    print(f\"Batch: {batch_idx}, Running Loss: {running_loss / (batch_idx + 1):.2f}, Running Accuracy: {running_acc:.2f}\")\n",
    "\n",
    "def training_step(model, batch, criterion):\n",
    "    # Prepare batch data\n",
    "    images, labels = batch\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Generate predictions\n",
    "    predictions = model(images)\n",
    "    # Calculate loss\n",
    "    loss = criterion(predictions, labels)\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy(predictions, labels)\n",
    "    return loss, acc\n",
    "\n",
    "# Method to log epoch loss and accuracy\n",
    "def epoch_end(result):\n",
    "    print(f\"val_loss: {result['val_loss']:.2f}, val_acc: {result['val_acc']:.2f}\\n\")\n",
    "\n",
    "def train(model, model_name, criterion, optimizer, train_loader, num_epochs=10, scheduler=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        running_loss = 0\n",
    "        # Training Phase\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Calculate Loss\n",
    "            loss, running_acc = training_step(model, batch, criterion)\n",
    "            # Compute Gradients\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Reset Gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 50 == 0 and batch_idx != 0:\n",
    "                log_training(batch_idx, running_loss, running_acc)\n",
    "\n",
    "        # Scheduling learning rate by stepLR\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "    # Save checkpoint file\n",
    "    torch.save(model.state_dict(), f\"{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b391844a-f304-437d-b8ef-7cf66ab8c30f",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Below you find a simple test method which you can use to test the performance (accuracy and loss) of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414a17f-ae8c-4f7b-9b84-74c4fd9bd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion, DataLoader):\n",
    "    print('\\n\\n[Plain/Test] Under Testing ... Please Wait')\n",
    "    examples = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(DataLoader)):\n",
    "            # Prepare batch data\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Generate predictions\n",
    "            outputs = model(inputs).detach()\n",
    "            # Calculate loss\n",
    "            batch_test_loss = criterion(outputs,targets)\n",
    "            #Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            batch_test_acc = torch.tensor(torch.sum(predicted == targets).item() / len(predicted))\n",
    "            \n",
    "            # Store batch results\n",
    "            test_loss.append(batch_test_loss)\n",
    "            test_acc.append(batch_test_acc)\n",
    "            # Store Examples\n",
    "            ex = inputs[0].squeeze().detach().cpu().numpy()\n",
    "            examples.append((targets[0],predicted[0],ex))\n",
    "\n",
    "    # Display Results\n",
    "    print(\"Test Loss: \", round(torch.stack(test_loss).mean().item(),2))\n",
    "    print(\"Test Accuracy: \", round(torch.stack(test_acc).mean().item()*100.0,2))\n",
    "    return round(torch.stack(test_acc).mean().item(),4), examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e129ff5-c7bc-4bac-90c6-34a472339a49",
   "metadata": {},
   "source": [
    "## Train Clean Model\n",
    "\n",
    "First we will train a clean model on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd929519-8d00-4997-a7b2-46ceb329f992",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b31a55-2548-4e8f-ac56-a87ad2fda50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_model = Model().to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(clean_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d874b157-5190-4d80-82d3-b552e14ea2bf",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Model\n",
    "\n",
    "If you completed the tutorial from last week, you can load your own pre-trained clean model using the code block below. Otherwise, you can use the clean model file we provided with this tutorial. Finally, if you want, you can train a new clean model from scratch by executing the cell block directly below subsection **Training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7592fa-70bb-450a-967a-68063b9f941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_model_path = \"./saved_models/clean_model.pth\"\n",
    "clean_model.load_state_dict(torch.load(clean_model_path))\n",
    "clean_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9a585-a315-4bc5-820a-bdc575419e19",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "SKIP THIS PART IF YOU LOADED A PRE-TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973449cf-23f6-4ebf-9d40-aadb4bf892fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(clean_model,\"clean_model\",criterion,optimizer,train_loader,num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f94a1-54d8-40d9-a80e-49f4614550ac",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eba449-ea58-4640-9cca-230d1ceb8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, examples = test(clean_model,criterion,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba15a2-6e65-4593-a88e-a527c57fd44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,100))\n",
    "cnt=0\n",
    "for i in range(len(examples[:10])):\n",
    "    cnt += 1\n",
    "    plt.subplot(len(examples),50,cnt)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    tar,adv,ex = examples[i]\n",
    "    #ex = np.transpose(ex, (1, 2, 0))\n",
    "    plt.title(\"Target: {} \\n Predicted: {}\".format(tar,adv))\n",
    "    plt.imshow(ex, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2d7b6-b0bd-4992-aac6-ae729f78044c",
   "metadata": {},
   "source": [
    "## Backdoor Attack\n",
    "\n",
    "In order to apply a backdoor defense and test its performance, we will need to apply a backdoor attack on the model. Now, if you completed the tutorial from last week, you might be thinking of using your own backdoored model. You are completely free to do so, however, you will also need the backdoored training and test set from last week. Probably, you had to write some custom dataset code and so to properly load your datasets you will need that code as well. \n",
    "\n",
    "To make your life a little bit easier, we provide you with our own code for you to use and load the correct backdoored data. You can now use the backdoored model, training and test set we attached to this tutorial. If you still want to use your own model, training and test set from last week, you will also need add and execute your code from last week. With this we mean you add the trigger and backdoor dataset code. This is important as the code will also be used later on in this tutorial. Otherwise, just run the code cells below and load the attached data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b598c5-a257-4aa4-8205-e1043375907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison(img, trigger_obj):\n",
    "    \"\"\"Poison the training samples by stamping the trigger.\"\"\"\n",
    "    poisoned_image = trigger_obj.apply_trigger(img)\n",
    "    return poisoned_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327a5a9-4372-4a8f-bceb-b91e18d9721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSQRTrigger:\n",
    "    \"\"\"\n",
    "    A class that creates a random square pattern that is used as a trigger for an\n",
    "    image dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, pos_label, dataset='mnist'):\n",
    "\n",
    "        datasets_dimensions = {\"mnist\": (28, 28, 1),\n",
    "                               \"cifar10\": (32, 32, 3),\n",
    "                               \"fmnist\": (28, 28, 1)}\n",
    "\n",
    "        # Use a hardcoded seed for reproducibility\n",
    "        dims = datasets_dimensions[dataset]\n",
    "\n",
    "        if size[0] != size[1]:\n",
    "            raise Exception(\"The size of the trigger must be square.\")\n",
    "\n",
    "        if pos_label.lower() not in [\"upper-left\", \"upper-mid\", \"upper-right\", \"mid-left\", \"mid-mid\", \"mid-right\",\n",
    "                                     \"lower-left\",\n",
    "                                     \"lower-mid\", \"lower-right\"]:\n",
    "            raise Exception(\n",
    "                \"The position of the trigger must be one of the following: upper-left, upper-mid, upper-right, mid-left, mid-mid, mid-right, lower-left, lower-mid, lower-right\")\n",
    "\n",
    "        if size[0] > dims[0] or size[1] > dims[1]:\n",
    "            raise Exception(\"The size of the trigger is too large for the dataset items.\")\n",
    "\n",
    "        self.dims = dims\n",
    "        self.size = size\n",
    "        self.pos_label = pos_label\n",
    "        # pos == position; coordinates\n",
    "        self.pos_coords = self._gen_pos_square()\n",
    "\n",
    "        trigger = np.zeros(self.dims, dtype=np.float32)\n",
    "        self.crafted_trigger = self.create_trigger_square(trigger)\n",
    "\n",
    "    def _gen_pos_square(self):\n",
    "        if self.pos_label == \"upper-left\":\n",
    "            return (0, 0)\n",
    "        elif self.pos_label == \"upper-mid\":\n",
    "            return (0, self.dims[1] // 2 - self.size[1] // 2)\n",
    "        elif self.pos_label == \"upper-right\":\n",
    "            return (0, self.dims[1] - self.size[1])\n",
    "\n",
    "        elif self.pos_label == \"mid-left\":\n",
    "            return (self.dims[0] // 2 - self.size[0] // 2, 0)\n",
    "        elif self.pos_label == \"mid-mid\":\n",
    "            return (self.dims[0] // 2 - self.size[0] // 2,\n",
    "                    self.dims[1] // 2 - self.size[1] // 2)\n",
    "        elif self.pos_label == \"mid-right\":\n",
    "            return (self.dims[0] // 2 - self.size[0] // 2, self.dims[1] - self.size[1])\n",
    "\n",
    "        elif self.pos_label == \"lower-left\":\n",
    "            return (self.dims[0] - self.size[0], 0)\n",
    "        elif self.pos_label == \"lower-mid\":\n",
    "            return (self.dims[0] - self.size[0], self.dims[1] // 2 - self.size[1] // 2)\n",
    "        elif self.pos_label == \"lower-right\":\n",
    "            return (self.dims[0] - self.size[0], self.dims[1] - self.size[1])\n",
    "\n",
    "    def create_trigger_square(self, trigger):\n",
    "        \"\"\"Create a square trigger.\"\"\"\n",
    "        base_x, base_y = self.pos_coords\n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                trigger[base_x + x][base_y + y] = \\\n",
    "                    np.ones((self.dims[2]))\n",
    "\n",
    "        return trigger\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"applies the trigger on the image.\"\"\"\n",
    "\n",
    "        base_x, base_y = self.pos_coords\n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                img[base_x + x][base_y + y] = self.crafted_trigger[base_x + x][base_y + y]\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee81b3c-3682-458d-8e63-3e294f1d7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackdoorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, clean_dataset, trigger_obj, epsilon=None, target_label=None, source_label=None, train=True):\n",
    "        self.clean_dataset = clean_dataset\n",
    "        self.trigger_obj = trigger_obj\n",
    "        self.epsilon = epsilon\n",
    "        self.target_label = target_label\n",
    "        self.source_label = source_label\n",
    "        self.train = train\n",
    "        if train:\n",
    "            self.backdoor_dataset = self.get_train_set()\n",
    "        else:\n",
    "            self.backdoor_dataset = self.get_test_set()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.backdoor_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.backdoor_dataset[idx]\n",
    "        return image, label\n",
    "\n",
    "    def poison(self, img):\n",
    "        \"\"\"Poison the training samples by stamping the trigger.\"\"\"\n",
    "        return self.trigger_obj.apply_trigger(img)\n",
    "    \n",
    "    def get_train_set(self):\n",
    "        backdoored_ds = []\n",
    "        \n",
    "        trigger_samples = int(self.epsilon * len(self.clean_dataset))\n",
    "        samples_index = np.random.choice(len(self.clean_dataset), size=trigger_samples, replace=False)\n",
    "        \n",
    "        for idx, (image, label) in enumerate(self.clean_dataset):\n",
    "            poisoned_image = torch.from_numpy(self.poison(image.clone().cpu().permute(1, 2, 0).numpy())).permute(2, 0, 1)\n",
    "            \n",
    "            if self.source_label is not None:\n",
    "                if idx in samples_index:\n",
    "                    if label == self.source_label:\n",
    "                        insert = (poisoned_image, self.target_label)\n",
    "                    else:\n",
    "                        insert = (poisoned_image, label)\n",
    "                else:\n",
    "                    insert = (image, label)\n",
    "            else:\n",
    "                if idx in samples_index:\n",
    "                    insert = (poisoned_image, self.target_label)\n",
    "                else:\n",
    "                    insert = (image, label)\n",
    "            backdoored_ds.append(insert)\n",
    "            \n",
    "        return backdoored_ds\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        backdoored_ds = []\n",
    "        \n",
    "        for idx, (image, label) in enumerate(self.clean_dataset):\n",
    "            poisoned_image = torch.from_numpy(self.poison(image.clone().cpu().permute(1, 2, 0).numpy())).permute(2, 0, 1)\n",
    "\n",
    "            if label != self.target_label:\n",
    "                insert = (poisoned_image, label)\n",
    "                backdoored_ds.append(insert)\n",
    "        \n",
    "        return backdoored_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f6bc82-7357-4685-9849-895dffa9f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkdr_model = Model().to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(bkdr_model.parameters(),lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806a35a-af89-487c-b90e-a3d8bf2198af",
   "metadata": {},
   "source": [
    "You can run the code cells below to load our own backdoored model (square trigger in the upper left part of the images). The code cell directly after the next one will load our own backdoored MNIST training and test sets. They have been backdoored with target label set to 1 and no source label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f31ef1-3190-42ac-bb01-8f1aa87dcb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkdr_model_path = \"./saved_models/bkdr_model.pth\"\n",
    "bkdr_model.load_state_dict(torch.load(bkdr_model_path))\n",
    "bkdr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5a0bb-bb8a-4c08-80d5-6b93e3405929",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_obj = GenerateSQRTrigger((4, 4), 'upper-left')\n",
    "\n",
    "target_label = 1\n",
    "source_label = None\n",
    "training_epsilon = 0.06\n",
    "test_epsilon = None\n",
    "\n",
    "bkdr_train_set = torch.load('./data/bkdr_train_set.pt')\n",
    "bkdr_train_loader = torch.utils.data.DataLoader(bkdr_train_set, batch_size=batch_size, shuffle=True,\n",
    "                                                num_workers=num_workers)\n",
    "\n",
    "bkdr_test_set = torch.load('./data/bkdr_test_set.pt')\n",
    "bkdr_test_loader = torch.utils.data.DataLoader(bkdr_test_set,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba7f29-d4ec-408c-9cc5-f2178b672668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(bkdr_train_loader)\n",
    "images, labels = next(dataiter)\n",
    "visualize_batch(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b79be8-5810-4a57-8f17-eb54e7f136f6",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To check if the models are loaded correctly and if the backdoor is present you can run the code cells below to calculate the attack succes rate and the clean accuracy drop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e7a9f4-b43e-4567-9f0a-2ef9643a2a49",
   "metadata": {},
   "source": [
    "#### Attack Succes Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e8a7d-9b41-46a4-8257-9ba32672cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_source_indices(y_test,source_label):\n",
    "    indices = (y_test == source_label).nonzero(as_tuple=False).numpy()\n",
    "    indices = indices.reshape(indices.shape[0])\n",
    "    return indices\n",
    "\n",
    "def find_non_source_indices(y_test,source_label,target_label):\n",
    "    # get indices of samples which do not have source or target label\n",
    "    indices = torch.logical_and((y_test != source_label),(y_test != target_label)).nonzero(as_tuple=False).numpy()\n",
    "    indices = indices.reshape(indices.shape[0])\n",
    "    return indices\n",
    "\n",
    "def count_non_source_misclassifications(original_labels,predicted,source_label,target_label):\n",
    "    sub_non_source_total = 0\n",
    "    sub_misclassifications = 0\n",
    "\n",
    "    # find all the images with a different label than the source or target label\n",
    "    indices = find_non_source_indices(original_labels,source_label,target_label)\n",
    "    sub_non_source_total += indices.shape[0]\n",
    "\n",
    "    # for all non-source and non-target label images check if the prediction is equal to the target label\n",
    "    for index in indices:\n",
    "        if predicted[index].detach().cpu().numpy() == target_label:\n",
    "            sub_misclassifications += 1\n",
    "    return sub_misclassifications, sub_non_source_total\n",
    "\n",
    "def count_source_specific_classifications(original_labels,predicted,source_label,target_label):\n",
    "    sub_total = 0\n",
    "    sub_correct = 0\n",
    "    \n",
    "    # find all the images with the source label\n",
    "    indices = find_source_indices(original_labels,source_label)\n",
    "    sub_total += indices.shape[0]\n",
    "    \n",
    "    # for all source label images check if the prediction is equal to the target label\n",
    "    for i in indices:\n",
    "        if predicted[i].detach().cpu().numpy() == target_label:\n",
    "            sub_correct +=1\n",
    "    return sub_correct, sub_total\n",
    "\n",
    "def calculate_ASR(model,dataloader,target_label,source_label=None,verbose=False):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    non_source_total = 0\n",
    "    misclassifications = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for inputs, original_labels in tqdm(dataloader):\n",
    "            # Use poisoned test image to get predictions of backdoored model\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs).detach()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            \n",
    "            # If source specific attack\n",
    "            if source_label is not None:\n",
    "                sub_correct, sub_total = count_source_specific_classifications(original_labels,predicted,source_label,target_label)\n",
    "                correct += sub_correct\n",
    "                total += sub_total\n",
    "                if verbose:\n",
    "                    sub_misclassifications, sub_non_source_total = count_non_source_misclassifications(original_labels,predicted,source_label,target_label)\n",
    "                    misclassifications += sub_misclassifications\n",
    "                    non_source_total += sub_non_source_total\n",
    "            # if source agnostic attack\n",
    "            else:\n",
    "                # for all test samples check if the predicted label is equal to the target label\n",
    "                for i in range(len(inputs)):\n",
    "                    if original_labels[i] != target_label:\n",
    "                        total += 1\n",
    "                        if predicted[i].detach().cpu().item() == target_label:\n",
    "                            correct += 1\n",
    "\n",
    "    attack_acc = (correct * 100.0) / total\n",
    "    print(f\"Attack accuracy: {round(attack_acc,2)}\")\n",
    "    \n",
    "    if source_label and verbose:\n",
    "        print(misclassifications)\n",
    "        print(non_source_total)\n",
    "        misclassification_rate = (misclassifications * 100.0) / non_source_total\n",
    "        print(f\"False Positive rate: {round(misclassification_rate,2)}\")\n",
    "        \n",
    "    return attack_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da89000-0106-40fc-a58f-1feb39233691",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_acc = calculate_ASR(bkdr_model,bkdr_test_loader,target_label=target_label,source_label=source_label,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72125ec5-0b52-4152-96cd-26514829b458",
   "metadata": {},
   "source": [
    "#### Clean Accuracy Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb15e4-34dd-4301-8076-915aab05db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy clean model\n",
    "clean_acc, examples = test(clean_model,criterion,test_loader)\n",
    "# Compute accuracy poisoned model\n",
    "bkdr_acc, examples = test(bkdr_model,criterion,test_loader)\n",
    "\n",
    "print(f\"Clean Accuracy drop of: {round(clean_acc-bkdr_acc,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fc7b2-c752-42b1-a51a-c659a02f9aae",
   "metadata": {},
   "source": [
    "# Backdoor Defense\n",
    "\n",
    "Now we will start with the actual tutorial part on defenses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5df868-6bf5-4443-9048-3269c58c2c6e",
   "metadata": {},
   "source": [
    "## Fine-Pruning\n",
    "\n",
    "This defense was introduced in [Fine-Pruning](https://arxiv.org/abs/1805.12185) and it is one of the first backdoor countermeasures. This defense combines neuron pruning and fine-tuning. Fine-pruning is a post-training defense that prunes the most “inactive” neurons and then fine-tunes the network for some epochs using clean data. The intuition is based on the fact that some neurons contain the main (clean) task information, others the backdoor task, and the rest a combination of both. Therefore, removing the correct group of neurons will reduce the backdoor effect. Sometimes, pruning is unnecessary, solely fine-tuning is enough to reduce the backdoor effect while maintaining high accuracy in the main task.\n",
    "\n",
    "This defense computes the activity of neurons of a specific ```layer``` (usually the last convolutional layer because it contains the most complete information regarding the input features) and removes (prunes) a percentage of most inactive neurons  (```prune_rate```). This process may drop the model's performance in the original task and for that reason we need to retrain (fine-tune) our model for a few epochs (usually this is the 10% of the number of epochs that the model was trained for)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eaa87f-1b15-4873-adf5-887f869ee957",
   "metadata": {},
   "source": [
    "In the next code cell you find the class ```MaskedLayer(nn.Module)```. You can use this class to apply a binary ```mask``` (which you will need to create yourself) to a specific layer ```base```. The mask will be a tensor with a size equal to the number of neurons. The indices of the neurons that will need to be pruned will hold the value 0 in the mask, other indices of the mask hold the value 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cefa6a-a57a-4940-8a2a-de93fdcbc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLayer(nn.Module):\n",
    "    def __init__(self, base, mask):\n",
    "        super(MaskedLayer, self).__init__()\n",
    "        self.base = base  # The original layer that will be pruned\n",
    "        self.mask = mask  # The mask representing which elements to prune\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.base(input) * self.mask  # Element-wise multiplication to prune the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3bd79-4a85-4959-b7c6-182b64785bee",
   "metadata": {},
   "source": [
    "Below you will find the class ```Pruning```. It is your task to complete the code in order to be able to prune the most inactive neurons of a specific layer. In the code you will find comments indicating where you need to finish the code. The general idea of this class is that the method ```prune()``` can be used to prune a given ```layer``` with the given ```prune_rate```. You can find layer names by executing ```model.eval()``` on your model. The Prune class also needs the clean training set and the backdoored model.  \n",
    "\n",
    "```prune()``` first sets a forward hook which collects the activation of your target layer. Then its your task to compute the average activation for every neuron, sort these activations ascending and then decide how many neurons need to be pruned using the pruning rate. Finally, you will create a binary mask to be able to prune the neurons of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c016022-af93-4959-a835-c9572a5299d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pruning:\n",
    "    \"\"\"Pruning process.\n",
    "    Args:\n",
    "        # Various parameters for the pruning process\n",
    "        dataloader (torch Dataloader): train dataloader containing clean dataset for forward pass.\n",
    "        model (torch.nn.Module): Network.\n",
    "        layer: The layer name to prune\n",
    "        prune_rate (double): the pruning rate\n",
    "    \"\"\"\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self,\n",
    "                 dataloader=None,\n",
    "                 model=None,\n",
    "                 layer=None,\n",
    "                 prune_rate=None):\n",
    "        # Initialize instance variables\n",
    "        self.tr_loader = dataloader\n",
    "        self.model = model\n",
    "        self.layer_to_prune = layer\n",
    "        self.prune_rate = prune_rate\n",
    "        self.pruned_model = model\n",
    "\n",
    "    def get_pruned_model(self):\n",
    "        return self.pruned_model\n",
    "\n",
    "    def get_layer_to_prune(self):\n",
    "        return self.layer_to_prune\n",
    "\n",
    "    def get_prune_rate(self):\n",
    "        return self.prune_rate\n",
    "\n",
    "    def prune(self):\n",
    "        \"\"\"pruning.\"\"\"\n",
    "        \n",
    "        # set model to correct device\n",
    "        #model = copy.deepcopy(self.model.to(device))\n",
    "        model = self.model.to(device)\n",
    "\n",
    "        # prune silent activation\n",
    "        print(\"======== pruning... ========\")\n",
    "\n",
    "        # In the next few lines we create a forward hook to collect activations\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # In this list we will store all activations, you will need it later!\n",
    "            container = []\n",
    "\n",
    "            # Define a forward hook to collect the output of the layer to prune\n",
    "            def forward_hook(module, input, output):\n",
    "                container.append(output)\n",
    "\n",
    "            # Register the forward hook\n",
    "            # Will be called when model.forward() has computed output\n",
    "            hook = getattr(model, self.layer_to_prune).register_forward_hook(forward_hook)\n",
    "            print(\"Forwarding all training set\")\n",
    "\n",
    "            # Run the model on the entire training set to collect layer outputs\n",
    "            model.eval()\n",
    "            for data, _ in self.tr_loader:\n",
    "                model(data.to(device))\n",
    "                \n",
    "            # Remove the forward hook after processing the training set\n",
    "            hook.remove()\n",
    "\n",
    "        '''\n",
    "        FINISH THE CODE BELOW\n",
    "\n",
    "        You will need to replace the three dots ...\n",
    "        '''\n",
    "        # Compute the average activation for the layer to prune\n",
    "        # 1: turn the container list into a torch tensor\n",
    "        container = ...\n",
    "        # 2: get the average activation for the channels by reducing dimensions 0 (nr samples), 2 (width), 3 (height)\n",
    "        activation = ...\n",
    "        # 3: sort from low to high / ascending\n",
    "        seq_sort = ...\n",
    "        # 4: get number of channels / neurons\n",
    "        num_channels = ...\n",
    "        # 5: compute number of channels / neurons to prune\n",
    "        num_pruned_channels = ...\n",
    "        # 6: initialize binary mask using only ones.\n",
    "        mask = ...\n",
    "        '''\n",
    "        END CODE PART\n",
    "        '''\n",
    "        \n",
    "        # Create a mask representing which elements to prune based on the sorted activations\n",
    "        for element in seq_sort[:num_pruned_channels]:\n",
    "            mask[element] = 0\n",
    "        # Reshape mask to fit layer\n",
    "        if len(container.shape) == 4:\n",
    "            mask = mask.reshape(1, -1, 1, 1)\n",
    "\n",
    "        # Replace the layer with a MaskedLayer that applies the pruning mask\n",
    "        setattr(model, self.layer_to_prune, MaskedLayer(getattr(model, self.layer_to_prune), mask))\n",
    "\n",
    "        # store pruned_model\n",
    "        self.pruned_model = model\n",
    "        print(\"======== pruning complete ========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f29a7-547e-44e3-b006-c637807e0a23",
   "metadata": {},
   "source": [
    "Now that you have finished the code you can execute the code cell below to apply the Fine-Pruning defense on your backdoored model. Specify a layer you want to prune and a pruning rate. Then for the fine-tuning part also specify the number of epochs you want to fine-tune. We already specified these values with the following settings:\n",
    "- layer: 'conv2'\n",
    "- prune_rate: 0.5\n",
    "- num_epochs: 2\n",
    "\n",
    "Feel free to change them and see what effect this has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eae843-cbb7-413d-a0e3-4d333f635cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to make changes to these settings\n",
    "layer = 'conv2'\n",
    "prune_rate = 0.5\n",
    "num_epochs = 2\n",
    "\n",
    "print(\"Pruning Model\\n\")\n",
    "prune_obj = Pruning(dataloader=train_loader, model=bkdr_model, layer=layer, prune_rate=prune_rate)\n",
    "prune_obj.prune()\n",
    "#pruned_model = prune_obj.get_pruned_model()\n",
    "print(\"\\n\")\n",
    "pruned_attack_acc = calculate_ASR(prune_obj.model,bkdr_test_loader,target_label=target_label,source_label=source_label)\n",
    "pruned_clean_acc, examples = test(prune_obj.model, criterion, test_loader)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Fine tuning model\\n\")\n",
    "train(prune_obj.model, 'fine_pruned_model', criterion, optimizer, train_loader, num_epochs=num_epochs)\n",
    "print(\"\\n\")\n",
    "fine_tuned_attack_acc = calculate_ASR(prune_obj.model,bkdr_test_loader,target_label=target_label,source_label=source_label)\n",
    "fine_tuned_clean_acc, examples = test(prune_obj.model, criterion, test_loader)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebc240-57b0-47dd-ad7a-09c0bf05bcda",
   "metadata": {},
   "source": [
    "If you could run the code cell without problem you succesfully performed Fine-Pruning as a backdoor defense. At first you will notice that clean accuracy of the model dropped drastically. Then, after fine-tuning, the clean accuracy improved and the attack succes rate should have decreased. Using a different layer, pruning rate or number of epochs can have a different effect. So do use different settings to see what effect each of these settings has.\n",
    "\n",
    "In the next you will apply the STRIP defense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da2b597-b494-4bd9-baab-a2c049d58123",
   "metadata": {},
   "source": [
    "## STRIP\n",
    "\n",
    "[STRIP](https://dl.acm.org/doi/pdf/10.1145/3359789.3359790) stands for STRong Intentional Pertrubation and it is named this way as the idea is that you intentionally perturb incoming input. You then observe the randomness of predicted classes for these perturbed inputs from a given deployed model - either malicious or benign. Low entropy, which means low randomness, in predicted classes violates the input-dependence property of a benigng model and implies the presence of a malicious input - a characteristic of a trojaned input. \n",
    "\n",
    "We already mentioned it at the start of this tutorial, but we will repeat it here once more: STRIP is a type of Input Filtering defense. This means that it can be used to detect if input is either trojaned or clean. The STRIP defense can be wrapped around your model in order to prevent missclassifications or misuse of your model. \n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/strip.png\"/></div>\n",
    "\n",
    "The image above gives a short overview of the idea behind the STRIP defense. You take an input image, which in this case is the number 8. You then replicate this image N times. You also draw N clean images from the dataset. You perturb the input image by superimposing a drawn clean image onto it. Sometimes the input image is called the background image and the clean image, which you use to perturb the input, is called the overlay image. You do this N times, so you end up with N perturbed images.\n",
    "\n",
    "All these perturbed images are fed to the model to get class predictions. Then for every perturbed input its entropy is calculated using the following expression:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/strip3.png\"/></div>\n",
    "\n",
    "Here $y_{i}$ is the probability of the perturbed input belonging to class $i$. $M$ is the total number of classes. Then you take the entropy $H_{n}$ of each perturbed input $x^{Pn}$ and sum them:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/strip4.png\"/></div>\n",
    "\n",
    "Here $H_{sum}$ stands for the chance that input $x$ is trojaned. Higher the $H_{sum}$, lower the probability that input $x$ is trojaned input.\n",
    "\n",
    "Entropy $H_{sum}$ is also normalized using:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/strip5.png\"/></div>\n",
    "\n",
    "*The $H$ is regarded as the entropy of one incoming input $x$. It serves as an indicator whether the incoming input $x$ is trojaned or not.*\n",
    "\n",
    "Below we added an outline of the STRIP algorithm, which can also be found in the original paper. Just like the rest of the information shared here.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/strip2.png\"/></div>\n",
    "\n",
    "As you can see, a detection boundary is used to help detect trojaned images. You can determine this detection boundary in several ways. Either way, you want to keep in mind two metrics that help to assess the detection capability: false rejection rate (FRR) and false acceptance rate (FAR). \n",
    "\n",
    "- The FRR is the probability when the benign input is regarded as a trojaned input by STRIP detection system.\n",
    "- The FAR is the probability that the trojaned input is recognized as the benign input by STRIP detection system.\n",
    "\n",
    "FRR stands for the robustness of the detection, while the FAR introduces a security concern. In the ideal world, both FRR and FAR should be 0%. As this condition may not always be possible, a detection system often attemps to minimize the FAR while using a slightly higher FRR as a trade-off. \n",
    "\n",
    "This means that one way in determining your detection boundary is by utilizing the FRR. By settling on a specific false rejection rate, you can determine what would be the threshold value to detect if inputs are indeed trojaned or not. You will play with this concept at the end of this tutorial.\n",
    "\n",
    "First you will need to finish the STRIP code below by filling in the missing parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c546a-afcb-4af1-8614-ec42b51bb49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STRIP:\n",
    "\n",
    "    def __init__(self, dataset, dims, model, device):\n",
    "        self.dataset = dataset\n",
    "        self.dims = dims\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    # This method is added to convert from tensor to numpy\n",
    "    def to_numpy(self,tensor_image):\n",
    "        return tensor_image.clone().cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "    # This method is added to convert from numpy to tensor\n",
    "    def to_tensor(self,numpy_image):\n",
    "        return torch.from_numpy(numpy_image).to(self.device)\n",
    "\n",
    "    def superimpose(self,background,overlay):\n",
    "        '''\n",
    "        FINISH THE CODE\n",
    "\n",
    "        You will need to combine the background and the overlay\n",
    "        '''\n",
    "        perturbed_image = ...\n",
    "        return (perturbed_image.reshape(self.dims))\n",
    "\n",
    "    def calculate_entropy(self,background, n):\n",
    "        background = self.to_numpy(background)\n",
    "\n",
    "        '''\n",
    "        FINISH THE CODE\n",
    "\n",
    "        You will need to fill the three dots with your own code\n",
    "        '''\n",
    "        # Create list of zeros with size n\n",
    "        entropy_sum = ...\n",
    "        # Create list of zeros with size n\n",
    "        perturbed_x = ...\n",
    "        # Pick n indices which will be the overlay images\n",
    "        index_overlay = ...\n",
    "        \n",
    "        for index in range(n):\n",
    "            # Use overlay indices to pick image from dataset\n",
    "            overlay = self.to_numpy(...)\n",
    "            # Create perturbed image using background and overlay\n",
    "            perturbed_image = ...\n",
    "            perturbed_x[index] = (self.to_tensor(perturbed_image))\n",
    "            \n",
    "        perturbed_inputs = torch.stack(perturbed_x).to(self.device)\n",
    "        perturbed_outputs = self.model(perturbed_inputs)\n",
    "        perturbed_outputs = torch.sigmoid(perturbed_outputs).detach().cpu().numpy()\n",
    "        \n",
    "        # Compute entropy sum\n",
    "        entropy_sum = ...\n",
    "        '''\n",
    "        END OWN CODING PART\n",
    "        '''\n",
    "        return entropy_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468388c2-6d8e-42e1-9b5f-5ed3eba96727",
   "metadata": {},
   "source": [
    "Now initialize a STRIP object by executing the code block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f936ee7-b5ed-400e-80aa-3fc5febfe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip = STRIP(test_set, (1, 28, 28), bkdr_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31688a7b-a647-4126-af69-c9c8395afb3e",
   "metadata": {},
   "source": [
    "Next we will start creating many clean perturbation images and calculate the entropy values. We take 2000 clean images from the MNIST test set and then for each one create 100 perturbated images. For all these images we calculate the entropy value, which we store in ```entropy_benigh```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d94300e-fdb4-4166-8caa-4404142190de",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 2000\n",
    "n_sample = 100\n",
    "clean_start_index = 1000 # we take index 1000-3000 of the test set\n",
    "entropy_benigh = [0] * n_test\n",
    "\n",
    "for index in tqdm(range(n_test)):\n",
    "    x_background, _ = test_set[clean_start_index + index]\n",
    "    entropy_benigh[index] = strip.calculate_entropy(x_background, n_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0eb3b-fc2f-43b2-ae39-1276f5727ef0",
   "metadata": {},
   "source": [
    "We will also create a similar sized subset of poisoned images, which we perturbate and calculate the entropy values of. So we take 2000 images and poison them using the BadNet square trigger. Then for every one of these poisoned images, we create 100 perturbated images for which we calculate the entropy value. These entropy values are stored in ```entropy_trojan```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c9b37-88a1-4b93-93d5-f2989f2977f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_trojan = [0] * n_test\n",
    "trojan_start_index = 4000 # we take index 4000-6000 of the test set\n",
    "for index in tqdm(range(n_test)):\n",
    "    x_poison = train_set[trojan_start_index + index][0].clone().cpu().permute(1, 2, 0).numpy()\n",
    "    x_poison = poison(x_poison,trigger_obj)\n",
    "    x_poison = torch.from_numpy(x_poison).permute(2, 0, 1).to(device)\n",
    "    entropy_trojan[index] = strip.calculate_entropy(x_poison, n_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24807d17-f6dc-4e7a-9841-f70acd2950d5",
   "metadata": {},
   "source": [
    "Now we can use all these entropy values together with a FRR value to get our threshold or decision boundary. Here we follow the way of working from the authors of the [STRIP-ViTA](https://arxiv.org/pdf/1911.10312.pdf) paper:\n",
    "\n",
    "- gaining the entropy of all tested clean inputs and sorting them in ascending manner;\n",
    "- presetting the FRR and using the $x_{th}$ entropy of the clean input that gives such FRR as the detection boundary.\n",
    "\n",
    "For every FRR value and corresponding threshold we calculate the FAR value using the subset of poisoned perturbed image entropies. This will help us determine a threshold that will result in a low FAR value. In a more realistic setting, the defender will not know how the data will be poisoned. So calculating the FAR will not be possible. Here, we do know how the data is poisoned, so we show this to give a better understanding what effect using different thresholds can have on the FAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7cc98-d12e-465d-85e6-c4ff87e3716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRR = [0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "data = []\n",
    "\n",
    "for r in FRR:\n",
    "  threshold_idx = int(n_test * r)\n",
    "  threshold = entropy_benigh[np.argsort(entropy_benigh)[threshold_idx]]\n",
    "  FAR = sum(i > threshold for i in entropy_trojan)/2000 * 100\n",
    "  data.append([r, FAR, threshold])\n",
    "\n",
    "# Create the pandas DataFrame \n",
    "df = pd.DataFrame(data, columns = ['FRR', 'FAR', 'Threshold']) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a6f4d-5ec5-4e4d-9c9d-27f50bcd4ea0",
   "metadata": {},
   "source": [
    "We can also plot all entropies to get a feeling of the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ca7db-04e2-4d2a-ba1a-105b9078f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 30\n",
    "plt.hist(entropy_benigh, bins, weights=np.ones(len(entropy_benigh)) / len(entropy_benigh), alpha=1, label='without trojan')\n",
    "plt.hist(entropy_trojan, bins, weights=np.ones(len(entropy_trojan)) / len(entropy_trojan), alpha=1, label='with trojan')\n",
    "plt.legend(loc='upper right', fontsize = 20)\n",
    "plt.ylabel('Probability (%)', fontsize = 20)\n",
    "plt.title('normalized entropy', fontsize = 20)\n",
    "plt.tick_params(labelsize=20)\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff57798-3ec3-4110-bb3c-9714bf91f0c3",
   "metadata": {},
   "source": [
    "Another way to determine a threshold is by making use of scipy's ```ppf()``` method. This stands for the Percent Point Function and will help to compute the quantile corresponding to the lower tail of a given probability $q$. This is done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb6c31-d913-4b57-b290-cda316220bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRR = 0.1\n",
    "print(f\"FRR: {FRR}\")\n",
    "(mu, sigma) = scipy.stats.norm.fit(entropy_benigh)\n",
    "print(f\"mu: {mu}, sigma: {sigma}\")\n",
    "threshold = scipy.stats.norm.ppf(FRR, loc = mu, scale = sigma)\n",
    "print(f\"Quantile corresponding to the lower tail for given probability {0.01}: {threshold}\")\n",
    "FAR = sum(i > threshold for i in entropy_trojan)\n",
    "print(f\"FAR: {FAR/n_test*100}\")\n",
    "min_benigh_entropy = min(entropy_benigh)\n",
    "max_trojan_entropy = max(entropy_trojan)\n",
    "print(f\"Min Benigh Entropy: {min_benigh_entropy}\")\n",
    "print(f\"Max Trojan Entropy: {max_trojan_entropy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2933a-7579-4de8-80e8-29e718bb3965",
   "metadata": {},
   "source": [
    "By picking a threshold value, you can now use STRIP to check if new input samples are trojaned or not. You will probably notice, that STRIP is not 100% succesfull in detecting what is trojaned and what is not. One possible reason for this, is that we are using the MNIST dataset. Images in this dataset have a lot of black in the background and only the digits themselves are white. When we superimpose two of these images, only a few pixels will be changed while a large portion will remain completely black. The effect of superimposing is thus not so big and probably the amount of randomness is still low with clean images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59106c73-790f-4aed-98a0-058436bc33d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 100\n",
    "backdoored = []\n",
    "\n",
    "dataiter = iter(bkdr_train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "\n",
    "for index, image in enumerate(images):\n",
    "    image_entropy = strip.calculate_entropy(image, n_sample)\n",
    "    if image_entropy < threshold:\n",
    "        backdoored.append(True)\n",
    "    else:\n",
    "        backdoored.append(False)\n",
    "print(\"Images which have been backdoored\")\n",
    "backdoored = torch.tensor(backdoored)\n",
    "visualize_batch(images,backdoored)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a2df9-6335-46a8-8a0f-882f0a8f58c8",
   "metadata": {},
   "source": [
    "This is the end of this tutorial. You have now succefully applied two types of defenses against Backdoor Attacks, well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
