{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db14bfe",
   "metadata": {},
   "source": [
    "# Evasion Attacks (Defenses)\n",
    "\n",
    "In this tutorial you will work with the same evasion attacks from last tutorial: FGSM and PGD. This time you will train a neural network on the MNIST dataset using both attacks to make the network more robust against evasion attacks. You will then attack the network and compare results. Next, you will also apply pruning to remove redundant parameters from a network to increase its efficiency while maintaining accuracy. Finally, we will have a look at one specific input transformation as defense at test time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95ebbe4",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Lets first import important packages. This includes the package [torchattacks](https://adversarial-attacks-pytorch.readthedocs.io/en/latest/attacks.html), a PyTorch library that provides adversarial attacks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c907a3-9c8c-4231-83f1-bd046d8061e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting and computing\n",
    "import copy\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# PyTorch packages\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy, softmax\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# This will install torchattacks if not yet present\n",
    "#!pip install torchattacks\n",
    "import torchattacks\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# For Load Bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b992ea7-b24a-4b36-8d30-8749bcc201b4",
   "metadata": {},
   "source": [
    "## Device\n",
    "\n",
    "We also set the device variable so that we can easily switch from using cpu to gpu (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what device we are using\n",
    "use_cuda=True\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a1860-fe68-439c-a29f-bc41277c453c",
   "metadata": {},
   "source": [
    "## Random Seed\n",
    "\n",
    "Execute the code snippet below to set the random seed. This will ensure that you can reproduce results over multiple tries. So anyone who re-runs your code will get the exact same outputs.\n",
    "\n",
    "For example: we will set shuffle to True and so the training loader will randomly shuffle the data over multiple runs. If you make changes to your code because training is not going well, then setting the random seed ensures that you can perform the training with the same samples as in previous tries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482e68e-4264-4830-912f-5d7bcd5ae106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this method to be able to reproduce results over multiple tries\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    # GPU operations have a separate seed we also want to set\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "        # We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "setup_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560187b",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We already introduced the MNIST dataset in a previous tutorial where we trained and tested a MLP and CNN on it. It is an illustrative dataset that is also not to big and so training a new model does not take too much time. We make use of PyTorch's `DataLoader` class to create objects that we can use to sample training and test data using batches of size 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba413523",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes   = 10\n",
    "img_size    = 28\n",
    "channel     = 1\n",
    "num_workers = 0\n",
    "batch_size  = 100\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root='./data', transform=transforms.ToTensor(), download=True, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,  batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, worker_init_fn=_init_fn)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root='./data', transform=transforms.ToTensor(), download=True, train=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6ad89",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "The two methods below can be used to test a model and compute the performance (accuracy). Use the `test` method to compute the accuracy without adversarial attack and use the `adv_test` method, by providing a specific attack, to test the network using adversarial images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b20f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    print('\\n\\n[Plain/Test] Under Testing ... Please Wait')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Evaluation\n",
    "            outputs = model(inputs).detach()\n",
    "\n",
    "            # Test\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += targets.numel()\n",
    "            correct += (predicted == targets).sum().item() \n",
    "\n",
    "\n",
    "        print('[Plain/Test] Acc: {:.3f}'.format(100.*correct / total))\n",
    "    return 100.*correct / total\n",
    "\n",
    "\n",
    "def adv_test(attack,model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    print('\\n\\n[Adv/Test] Under Testing ... Please Wait')\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        adv_inputs = attack(inputs, targets)\n",
    "\n",
    "        # Evaluation\n",
    "        outputs = model(adv_inputs).detach()\n",
    "\n",
    "        # Test\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += targets.numel()\n",
    "        correct += (predicted == targets).sum().item() \n",
    "\n",
    "        \n",
    "    print('[Adv/Test] Acc: {:.3f}'.format(100.*correct / total))\n",
    "    return 100.*correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e7717",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Here we provide a basic CNN network that we will train on the MNIST dataset and then use throughout the notebook. There are some new important parts added to this network, namely `self.record` and `self.mask`. We will use `self.record` to specify if we want the output of the second convolutional layer to be returned when `forward` is executed. This output we will need to perform pruning. Also used for pruning is `self.mask`. This is a special layer containing all ones and thus will simply output the input it gets. However, we can use this mask to specify which feature channel we want to prune by setting it to zero. More information on pruning at the end of this notebook. We will start with adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260abdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):          \n",
    "    def __init__(self, **kwargs): \n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.record=False\n",
    "\n",
    "        self.record_layer = {'conv1': True, 'conv2': True, 'fc1': True, 'fc2': True}\n",
    "\n",
    "        self.prune_layer = {'conv1': False, 'conv2': True}\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(1, 16, 5, 1, 2) # in_channels, out_channels, kernel, stride, padding\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        \n",
    "        self.mask = {'conv1': torch.ones(batch_size,16,28,28).to(device), 'conv2': torch.ones(batch_size,32,14,14).to(device)}\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(1568, 1024) \n",
    "        self.fc2 = torch.nn.Linear(1024, 10)\n",
    "\n",
    "    def set_record(self,record):\n",
    "        self.record=record\n",
    "\n",
    "    def get_record(self):\n",
    "        return self.record\n",
    "\n",
    "    def set_mask(self,layer,mask):\n",
    "        self.mask[layer] = mask\n",
    "\n",
    "    def get_mask(self,layer):\n",
    "        return self.mask[layer]\n",
    "\n",
    "    def record_layer(self,layer: str,record: bool):\n",
    "        self.record_layer[layer] = record\n",
    "\n",
    "    def prune_layer(self,layer: str,prune: bool):\n",
    "        self.prune_layer[layer] = prune\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output_dict = {}\n",
    "\n",
    "        # convolutional layer 1\n",
    "        x = self.conv1(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # prune convolutional layer 1\n",
    "        if self.prune_layer['conv1']:\n",
    "            x = x * self.mask['conv1']\n",
    "        # record convolutional layer 1\n",
    "        if self.record_layer['conv1']:\n",
    "            output_dict['conv1'] = x\n",
    "        # max pooling \n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        # convolutional layer 2\n",
    "        x = self.conv2(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # prune convolutional layer 2\n",
    "        if self.prune_layer['conv2']:\n",
    "            x = x * self.mask['conv2']\n",
    "        # record convolutional layer 2\n",
    "        if self.record_layer['conv2']:\n",
    "            output_dict['conv2'] =  x\n",
    "        # max pooling\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = x.view(-1, np.prod(x.size()[1:]))\n",
    "\n",
    "        # fully connected layer 1\n",
    "        x = self.fc1(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # record fully connected layer 1\n",
    "        if self.record_layer['fc1']:\n",
    "            output_dict['fc1'] = x\n",
    "\n",
    "        # fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "        # softmax layer\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        # record fully connected layer 2\n",
    "        if self.record_layer['fc2']:\n",
    "            output_dict['fc2'] = x\n",
    "        \n",
    "        if self.record:\n",
    "            self.record = False\n",
    "            return x, output_dict\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338777b6",
   "metadata": {},
   "source": [
    "# Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590049a1-9ade-43a0-a8e4-3220aafcd751",
   "metadata": {},
   "source": [
    "With adversarial training you will use an evasion attack to create adversarial images and train your model using these images but then with the correct label. This way you can make a model robust against similar evasion attacks. Below you will find an adversarial training method that you can use to perform adversarial training. Examine the code to understand what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a61a2",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "You can use the `adv_train` method below to perform adversarial training of a provided model. You will need to provide:\n",
    "- neural network\n",
    "- loss function\n",
    "- optimization function\n",
    "- attack\n",
    "- number of epochs\n",
    "\n",
    "We also added a normal `train` method for you to compare each type of training. As you can see only a few steps differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08aa87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_train(model,criterion,optimizer,attack=None,epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        print('\\n\\n[Adv/Epoch] : {}'.format(epoch+1))\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            # Prepare adversarial examples            \n",
    "            adv_inputs = attack(inputs,targets)\n",
    "            \n",
    "            # Learning network parameters\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Using adversarial example to train\n",
    "            outputs = model(adv_inputs.type(torch.FloatTensor).to(device))\n",
    "            \n",
    "            targets = targets.to(device)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation\n",
    "            pred = torch.max(model(adv_inputs.type(torch.FloatTensor).to(device)).detach(), dim=1)[1]\n",
    "            correct += torch.sum(pred.eq(targets)).item()\n",
    "            total += targets.numel()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 50 == 0 and batch_idx != 0:\n",
    "                print('[Adv/Train] Iter: {}, Acc: {:.3f}, Loss: {:.3f}'.format(\n",
    "                    batch_idx, # Iter\n",
    "                    100.*correct / total, # Acc\n",
    "                    running_loss / (batch_idx+1) # CrossEntropy\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # Test\n",
    "        test(model)\n",
    "        adv_test(attack,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845faef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,criterion,optimizer,scheduler=None,epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        print('\\n\\n[Plain/Epoch] : {}'.format(epoch+1))\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            # No adversarial examples prepared\n",
    "\n",
    "            # Learning network parameters\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Using normal input images for training\n",
    "            outputs = model(inputs.type(torch.FloatTensor).to(device))\n",
    "            \n",
    "            targets = targets.to(device)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation\n",
    "            pred = torch.max(model(inputs.type(torch.FloatTensor).to(device)).detach(), dim=1)[1]\n",
    "            correct += torch.sum(pred.eq(targets)).item()\n",
    "            total += targets.numel()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 50 == 0 and batch_idx != 0:\n",
    "                print('[Plain/Train] Iter: {}, Acc: {:.3f}, Loss: {:.3f}'.format(\n",
    "                    batch_idx, # Iter\n",
    "                    100.*correct / total, # Acc\n",
    "                    running_loss / (batch_idx+1) # CrossEntropy\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Test\n",
    "        test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee02f3d-bf2a-4bd6-ad3a-33b8ba0568e5",
   "metadata": {},
   "source": [
    "Using the `adv_train()` method you will train two robust models. One using FGSM and one using PGD. Then you will test your model using clean images, FGSM adversarial images and PGD adversarial images. In the end you will compare the accuracy in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70a2ed",
   "metadata": {},
   "source": [
    "## FGSM Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a245dd6",
   "metadata": {},
   "source": [
    "First we define some hyperparameters and a optimization and loss function. We also use the `torchattacks` library to define a FGSM and PGD attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8eb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=0.3\n",
    "alpha=0.01\n",
    "lr=0.1\n",
    "\n",
    "fgsm_net = Model().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(fgsm_net.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create attacks for FGSM network\n",
    "attack_fgsm = torchattacks.FGSM(model=fgsm_net, eps=eps)\n",
    "attack_pgd = torchattacks.PGD(model=fgsm_net,eps=eps,alpha=alpha,steps=10,random_start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e689621",
   "metadata": {},
   "source": [
    "We start adversarial training using FGSM. Execute the following code block to perform adversarial training. See the `adv_train` code block above for details to check how the attack is used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_train(fgsm_net,criterion,optimizer,attack=attack_fgsm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93f6f5",
   "metadata": {},
   "source": [
    "Now we want to check the performance of the FGSM trained model. First we will test the model using clean input. Then we will use FGSM to test the model using adversarial images and finally we will do the same using PGD. Attacking the model using PGD should result in a very low performance as FGSM does not make the model robust against it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e40f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_net_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_net_scores.append(test(fgsm_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f686998",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_net_scores.append(adv_test(attack_fgsm,fgsm_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fbfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_net_scores.append(adv_test(attack_pgd,fgsm_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b54e5b",
   "metadata": {},
   "source": [
    "## PGD Adversarial Training\n",
    "\n",
    "Now lets train the model using PGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=0.3\n",
    "alpha=0.01\n",
    "lr=0.1\n",
    "\n",
    "pgd_net = Model().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(pgd_net.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create attacks for PGD network\n",
    "attack_pgd = torchattacks.PGD(model=pgd_net,eps=eps,alpha=alpha,steps=10,random_start=True)\n",
    "attack_fgsm = torchattacks.FGSM(model=pgd_net, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04debff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_train(pgd_net,criterion,optimizer,attack=attack_pgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ad341",
   "metadata": {},
   "source": [
    "After training the model with PGD compute the different performance scores below. This time attacking the model using PGD should not result in a bad performance. Also attacking with FGSM should result in a lower performance but still not too bad. What is interesting to see is that the performance using clean images is high, which was not the case when training with FGSM. This is due to with FGSM training we encounter the problem of label leaking. Label leaking makes the accuracy on adversarial examples higher than on clean images. The reason is explained in the paper [*Adversarial Machine Learning At Scale, by Kurakin, Goodfellow and Bengio*](https://arxiv.org/abs/1611.01236), which is that FGSM is actually just one step. This means that the adversarial examples found are not that diverse and we use the true label when training FGSM. So the network learns a special pattern related to the true label in the adversarial examples and does not learn enough information in clean examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_net_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_net_scores.append(test(pgd_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c271700",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_net_scores.append(adv_test(attack_fgsm,pgd_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0dba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_net_scores.append(adv_test(attack_pgd,pgd_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a1baa",
   "metadata": {},
   "source": [
    "Use the code below to plot all performance scores and compare the scores for normal training and adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251404f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Clean', 'FGSM', 'PGD']\n",
    "plt.plot(labels,pgd_net_scores,'b*-',label=\"PGD trained\")\n",
    "plt.plot(labels,fgsm_net_scores,'*-',color='orange',label=\"FGSM trained\")\n",
    "plt.ylim(ymin=0,ymax=100)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Input Images\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d123316",
   "metadata": {},
   "source": [
    "# Pruning as a defense\n",
    "\n",
    "Next we will show you how to perform the very simple method of adversarial examples related pruning. The idea is to check the distance between outputs of hidden layers when you input clean data and adversarial data. The distance can be quantified by $L_2$ norm.\n",
    "\n",
    "\n",
    "The parts with a large distance should be pruned. To make it simple, we can just set the elements in that part to 0. \n",
    "For example, we can use feature channels as unit for pruning. The second convolutional layer of our network (`Model()`) outputs a feature map which consists of a few 32 feature channels.\n",
    "As the input is passed in `forward()` layer by layer, we can add a mask with the same shape for output of the second convolutional layer.\n",
    "If want to remove a feature channels, we just need to set the corresponding part in the mask to 0.\n",
    "\n",
    "First lets define new hyperparameters, a new model named `net` and the FGSM attack for this section of the notebook. After this train the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.2\n",
    "alpha = 0.01\n",
    "lr = 0.1\n",
    "\n",
    "net = Model().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4aac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_fgsm = torchattacks.FGSM(model=net,eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45e4c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(net,criterion,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e4b3c",
   "metadata": {},
   "source": [
    "Lets see what the clean and FGSM attacked accuracies are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(net)\n",
    "adv_test(attack_fgsm,net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ddf8e",
   "metadata": {},
   "source": [
    "## Compute the distance between outputs of the second convolutional layer (net.conv2)\n",
    "\n",
    "Here we use a simple way to get the output of a specific hidden layer.\n",
    "In the `forward()` function, we keep some of the intermediate results in the list `output_list`.\n",
    "The `output_list` is returned when `net.record` is `True`.\n",
    "\n",
    "Then we search for the part most likely related to adversarial attacks.\n",
    "If we prune (set it to 0) it, the model is supposed to be less sensitive to adversarial examples.\n",
    "\n",
    "\n",
    "**How to evaluate the relevance of hidden layers and adversarial attack?**\n",
    "\n",
    "\n",
    "We use feature channels as the unit to evaluate it. A convolutional layer will output multiple feature channels. Each channel represent a feature learned by the model. \n",
    "\n",
    "When we input clean data and adversarial examples, if a feature channel then changes a lot, it means the channel is sensitive to adversarial perturbation. The channel is very related to adversarial attack.\n",
    "\n",
    "This difference of feature channels can be quantified by $L_2$ norm, Euclidean distance.\n",
    "\n",
    "Now run the following code to compute the distance between the output of conv2 with clean and adversarial input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d826e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_output_conv2(net,attack):\n",
    "    distance_results = [0 for i in range(32)]\n",
    "    topn = 1\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Get the output of convoluational layer 2 (conv2) with clean inputs   \n",
    "        net.set_record(True)\n",
    "        clean_output, clean_features = net(inputs)\n",
    "\n",
    "        adv_inputs = attack_fgsm(inputs, targets)\n",
    "\n",
    "        # Get the output of convoluational layer 2 (conv2) with adversarial inputs   \n",
    "        net.set_record(True)\n",
    "        adv_output, adv_features = net(adv_inputs)\n",
    "\n",
    "        # In order to facilitate the calculation of the distance, \n",
    "        # reshape the feature channels of conv2, from 14*14 to 196.\n",
    "        clean_conv2_features = clean_features['conv2'].view(100, 32, -1) \n",
    "        adv_conv2_features = adv_features['conv2'].view(100, 32, -1)\n",
    "\n",
    "        # 100 pieces of data at each iteration, 32 feature channels\n",
    "        for data_id in range(100):\n",
    "            distance_tensor = torch.Tensor(32)\n",
    "            for feature_id in range(32):\n",
    "                # Compute Euclidean distance\n",
    "                distance = (clean_conv2_features[data_id][feature_id] - adv_conv2_features[data_id][feature_id]).pow(2).sum().sqrt()\n",
    "                distance_tensor[feature_id] = distance\n",
    "\n",
    "\n",
    "            # From every 32 channels, we record the furthest distance, top 1 (topn=1).\n",
    "            # You could also decide to record the top 2 or top 3\n",
    "            # Feel free to experiment with this number\n",
    "            for feature_id in torch.topk(distance_tensor, topn)[1]:\n",
    "                distance_results[feature_id] += 1\n",
    "                \n",
    "    return distance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_results = compute_distance_output_conv2(net,attack_fgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distances(dis_results):\n",
    "    data = []\n",
    "    for i, count in enumerate(dis_results):\n",
    "        for _ in range(count):\n",
    "            data.append(i)\n",
    "    bins=np.arange(33)-0.5\n",
    "    plt.hist(data,bins=bins)\n",
    "    plt.xticks(range(0,33,2))\n",
    "    plt.xlim([-1,34])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc91dc48",
   "metadata": {},
   "source": [
    "Using the `plot_distance()` method will help to visualize how many times each feature channel resulted in the greatest distance. We are interested in those channels that most times resulted in the greatest distance. We can then decide to prune the top one or multiple in order to see what has the greatest effect on robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distances(dis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518096d4",
   "metadata": {},
   "source": [
    "Note that, even when a channel is recorded many time in `dis_results`, pruning it will not always increase the robustness. It is possible for these channels to still contain useful information. We cannot completely separate information related to adversarial attacks from other information.\n",
    "However, removing these recorded channels is more likely to improve robustness.\n",
    "\n",
    "Below you can specify which channel you want to prune in our network and then test it to get a performance score. Try out different channels and see for yourself what effect pruning these channels has on the robustness of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbb17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset network mask to contain all ones\n",
    "net.set_mask('conv2',torch.ones(batch_size,32,14,14).to(device))\n",
    "\n",
    "# Try out different channel ids for yourself\n",
    "remove_ids = [ADD CHANNEL ID(S) HERE!]\n",
    "\n",
    "# Prune specified channel ids\n",
    "for i in remove_ids:\n",
    "    mask = net.get_mask('conv2')\n",
    "    mask[:,i,:,:] = mask[:,i,:,:] * 0\n",
    "    net.set_mask('conv2',mask)\n",
    "    \n",
    "clean_acc_pruning = test(net)\n",
    "adv_acc_pruning = adv_test(attack_fgsm,net)\n",
    "\n",
    "# Reset network mask to contain all ones\n",
    "net.set_mask('conv2',torch.ones(batch_size,32,14,14).to(device))\n",
    "\n",
    "clean_acc = test(net)\n",
    "adv_acc = adv_test(attack_fgsm,net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dce675",
   "metadata": {},
   "source": [
    "Use the code below to plot the performance of the network using either clean or adversarial input and with or without pruning. Even when you tried many different channel ids or combination of channel ids to prune, it could be that the defense effect is only small. This is the case because pruning is a very naive defense. Like we mentioned before, some feature channels that seem might be an excellent candidate to prune could still hold useful information for the task at hand. As we cannot completely separate this information, pruning these channels can also harm the performance of the model. With this kind of defense, only a small improvement in accuracy is already very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6acb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pruning = [clean_acc,adv_acc]\n",
    "pruning = [clean_acc_pruning,adv_acc_pruning]\n",
    "\n",
    "ind = np.arange(2)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "width=0.3\n",
    "\n",
    "plt.bar(ind,no_pruning, width, label='No Pruning')\n",
    "plt.bar(ind+width,pruning,width,label=\"Pruning\")\n",
    "\n",
    "plt.xlabel('Images used during testing')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "# xticks()\n",
    "# First argument - A list of positions at which ticks should be placed\n",
    "# Second argument -  A list of labels to place at the given locations\n",
    "plt.xticks(ind + width / 2, ('Clean Images', 'Adversarial Images'))\n",
    "\n",
    "# Finding the best position for legends and putting it\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5327b-0d91-413c-80a6-6c5e82c072f2",
   "metadata": {},
   "source": [
    "The previous two defenses are defenses applied during training (pruning after a first iteration of training but at certainly before deployment). Next we will show you a defense you can apply after training. It is very easy and also not the best defense, but still it can be effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de344e3f-fe47-4963-a37c-93a6d8b03a26",
   "metadata": {},
   "source": [
    "# Input Transformation as Defense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b6d17-b0f8-4484-9eee-9cf662ba5e72",
   "metadata": {},
   "source": [
    "In this section we will show you how you can implement one kind of input transformation as defense. This defense is applied at inference time, so after training. In this tutorial we will apply JPEG compression as input transformation defense. This means that after training the model and deploying it we will apply JPEG compression to any input to the model. This is as simple as saving the images in JPEG format and then loading them again just before we feed them to the model. By compressing the images to JPEG, you are lowering the quality of the input images. The idea is that the effect of the adversarial perturbation on the input image will be removed by compressing the image to a lower quality. In this section, you will apply this defense and see its effect on the accuracy of your CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6abf02b-a102-4fad-b056-f7cda8a3b6c8",
   "metadata": {},
   "source": [
    "First we train a model, lets call it `cnn_model` as it is a CNN, on the clean training data of MNIST. You can run the code snippet below to start training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4408de-f85f-4c73-b670-8473691eb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "eps = 0.2\n",
    "alpha = 0.01\n",
    "lr = 0.1\n",
    "\n",
    "cnn_model = Model().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train(cnn_model,criterion,optimizer,scheduler=None,epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17e721-f93b-4c49-91dc-e3cb3531e105",
   "metadata": {},
   "source": [
    "Next compute clean accuracy without making use of JPEG compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3f9bd-745f-4f6a-bc24-7675d0e05226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Clean Accuracy\n",
    "\n",
    "clean_acc_no_compression = test(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ce103-2f88-4313-bff0-496a6fbe7064",
   "metadata": {},
   "source": [
    "Next we define the FGSM attack, but this time we use a smaller epsilon value of `0.2`. This is done to show you the effect of this defense. The defense itself is again not as effective as adversarial training. So with higher epsilon values, the effect in accuracy gain can be very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6572ce-6c8f-44bd-b380-12caf6d2ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack Model\n",
    "\n",
    "# Create attacks for FGSM network\n",
    "attack_fgsm = torchattacks.FGSM(model=cnn_model, eps=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5658d-8ce8-4693-bebb-705428329d5c",
   "metadata": {},
   "source": [
    "Now we compute accuracy using adversarial images, but we do not use compression yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5cfa3f-e4e5-4fa7-988a-f61e2f96df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy after Attack\n",
    "\n",
    "attack_acc_no_compression = adv_test(attack_fgsm,cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae48413-9641-480d-a33b-546918c5bc23",
   "metadata": {},
   "source": [
    "Now that we have computed both clean and adversarial accuracy without compression, we will introduce compression to the pipeline. Below you will find a method which takes a PIL image, saves it as a JPEG file, then loads it again as PIL image. This method can be used as transformation and so we also provide the `jpeg_transform` which is also composed of the `ToPILImage()` and `ToTensor()` transforms. If you make use of the dataloaders from earlier in this notebook, you will already have transformed all images to tensors. Now this combination of transforms takes a tensor and turns it into a PIL image. Next our jpeg_compression transform compresses the image to JPEG. Then finally, the `ToTensor()` transforms it back into tensor. The `ToPILImage()` transform only works on single images and not on entire batches. So you should be aware of this when trying to transform entire batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79adc062-28bc-4618-b753-a13932065318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jpeg_compression(image):\n",
    "    qf = 25\n",
    "    outputIoStream = BytesIO()\n",
    "    image.save(outputIoStream, \"JPEG\", quality=qf, optimice=True)\n",
    "    outputIoStream.seek(0)\n",
    "    return Image.open(outputIoStream)\n",
    "\n",
    "jpeg_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Lambda(jpeg_compression),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d2832-49f9-42e1-bc3a-fa3c582fa191",
   "metadata": {},
   "source": [
    "Below you will find two different test methods. They do exactly the same as our earlier test methods, but this time we introduced an extra parameter `compress`. This is a Boolean which you can use to specify if you want to compress the images before creating predictions with your model. There is the `test()` method which calculates accuracy using the test set without adversarial images and the `adv_test()` method which does this with adversarial images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431016c4-f61b-4817-a35b-b36ffa7a195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compression(model,compress=False):\n",
    "    print('\\n\\n[Plain/Test] Under Testing ... Please Wait')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # compress to jpeg\n",
    "            if compress:\n",
    "                inputs = torch.stack([jpeg_transform(input) for input in inputs]).to(device)\n",
    "            \n",
    "            # Evaluation\n",
    "            outputs = model(inputs).detach()\n",
    "\n",
    "            # Test\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += targets.numel()\n",
    "            correct += (predicted == targets).sum().item() \n",
    "\n",
    "\n",
    "        print('[Plain/Test] Acc: {:.3f}'.format(100.*correct / total))\n",
    "    return 100.*correct / total\n",
    "\n",
    "\n",
    "def adv_test_compression(attack,model,compress=False):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    print('\\n\\n[Adv/Test] Under Testing ... Please Wait')\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        adv_inputs = attack(inputs, targets)\n",
    "\n",
    "        # compress to jpeg \n",
    "        if compress:\n",
    "            adv_inputs = torch.stack([jpeg_transform(input) for input in adv_inputs]).to(device)\n",
    "        \n",
    "        # Evaluation\n",
    "        outputs = model(adv_inputs).detach()\n",
    "\n",
    "        # Test\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += targets.numel()\n",
    "        correct += (predicted == targets).sum().item() \n",
    "\n",
    "        \n",
    "    print('[Adv/Test] Acc: {:.3f}'.format(100.*correct / total))\n",
    "    return 100.*correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ea65f-b5c9-47d2-b2e7-82d731277714",
   "metadata": {},
   "source": [
    "Now we use both methods setting `compress` to `True` to apply JPEG compression and then calculate accuracy for using clean images or adversarial images during test time. What we should see is that the accuracy using adversarial images, should not be as low as when we do not apply compression. We should see a, slight, improvement compared to not using compression. Let us first compute the performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9362d4-3124-413b-a77a-7148ed9cf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM Attack on model with compression\n",
    "\n",
    "# Compute Clean Accuracy (Using compression)\n",
    "\n",
    "clean_acc_compression = test_compression(cnn_model,compress=True)\n",
    "\n",
    "# Attack Model\n",
    "\n",
    "# Compute Accuracy after Attack with compression\n",
    "\n",
    "attack_acc_compression = adv_test_compression(attack_fgsm,cnn_model,compress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031d78f-3ad0-4ba1-8765-d3cc9f2fe6fb",
   "metadata": {},
   "source": [
    "Lets plot all accuracies in one graph to make it more clear what the effect of the defense is on the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba95a56-8892-4547-9235-1faf054a78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies\n",
    "no_compression = [clean_acc_no_compression,attack_acc_no_compression]\n",
    "compression = [clean_acc_compression,attack_acc_compression]\n",
    "\n",
    "ind = np.arange(2)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "width=0.3\n",
    "\n",
    "plt.bar(ind,no_compression, width, label='No Compression')\n",
    "plt.bar(ind+width,compression,width,label=\"Compression\")\n",
    "\n",
    "plt.xlabel('Images used during testing')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "# xticks()\n",
    "# First argument - A list of positions at which ticks should be placed\n",
    "# Second argument -  A list of labels to place at the given locations\n",
    "plt.xticks(ind + width / 2, ('Clean Images', 'Adversarial Images'))\n",
    "\n",
    "# Finding the best position for legends and putting it\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b85765-9d3b-4bd0-a6e4-d3cfc7087e3f",
   "metadata": {},
   "source": [
    "Using an epsilon value of `0.2`, which means that the perturbations are not as strong as when we would have used a higher epsilon and so the effect of the attack should be lower, we do see a clear effect of the defense. The difference in accuracy when using adversarial images in both cases of using compression or not is very clear. If everything went well, using compression should have increased the accuracy in case of adversarial images, compared to not using compression.\n",
    "\n",
    "So you see, with only a few lines of codes you can already apply a simple defense against evasion attacks. To apply a good defense, will take some more effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
