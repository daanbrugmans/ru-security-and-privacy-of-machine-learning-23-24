{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c098375",
   "metadata": {},
   "source": [
    "# More Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d44f63-cf1f-4257-b64e-c6dcc18d06a1",
   "metadata": {},
   "source": [
    "In this tutorial we will explain how you can use existing python packages to load data. Data which we will ask you to use during the next tutorials. We will be using the MNIST and CIFAR-10 dataset. These are two widely known image datasets used for training machine learning models. We already explained something about these datasets in previous tutorials. In this tutorial we will specifically examine how we can use PyTorch to load these datasets and how the data is structured. \n",
    "\n",
    "We will start with the Dataset class from PyTorch which we can use to download and load many different existing datasets. Then we will perform several transformations on the data in order to practice how we could preprocess the data. Finally, we will focus on DataLoaders which will make our lives somewhat easier as they help us to create batches of the data which we can then use to train machine learning models without having to load the entire dataset or to perform randomized training. \n",
    "\n",
    "Below you can see we load several python packages. The first few are there to help us visualize the data to get a better sense of what we are working on. The last few, the torch packages, will be the main focus of this tutorial. They help us download and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e5a69-4213-49f0-b115-54ca7ed9bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dff26e-69f7-4611-bc85-e2971727f5a9",
   "metadata": {},
   "source": [
    "## PyTorch Dataset (MNIST Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862fed62-4539-43ab-af66-49a383dcba11",
   "metadata": {},
   "source": [
    "### PyTorch, Torchvision & Built-in Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3bb19-8c2f-4e19-a2f3-0dd9eb3b0e34",
   "metadata": {},
   "source": [
    "To load MNIST (or later CIFAR-10) we will make use of PyTorch and more specifically PyTorch's package named [torchvision](https://pytorch.org/vision/stable/index.html). This is a package containing popular datasets, model architectures and common image transformations (to be used during preprocessing for example) for computer vision.\n",
    "\n",
    "We can load many datasets using the [torchvision.datasets](https://pytorch.org/vision/0.16/datasets.html) module. You can also use this module to create your own dataset. Executing the code cell below will list all function (or variable) names which can be found inside the datasets module. In between this list you can spot the different built-in datasets, such as MNIST and CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7195f9f-e446-460e-8a40-20ea6820cd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CIFAR10',\n",
       " 'CIFAR100',\n",
       " 'CLEVRClassification',\n",
       " 'CREStereo',\n",
       " 'Caltech101',\n",
       " 'Caltech256',\n",
       " 'CarlaStereo',\n",
       " 'CelebA',\n",
       " 'Cityscapes',\n",
       " 'CocoCaptions',\n",
       " 'CocoDetection',\n",
       " 'Country211',\n",
       " 'DTD',\n",
       " 'DatasetFolder',\n",
       " 'EMNIST',\n",
       " 'ETH3DStereo',\n",
       " 'EuroSAT',\n",
       " 'FER2013',\n",
       " 'FGVCAircraft',\n",
       " 'FakeData',\n",
       " 'FallingThingsStereo',\n",
       " 'FashionMNIST',\n",
       " 'Flickr30k',\n",
       " 'Flickr8k',\n",
       " 'Flowers102',\n",
       " 'FlyingChairs',\n",
       " 'FlyingThings3D',\n",
       " 'Food101',\n",
       " 'GTSRB',\n",
       " 'HD1K',\n",
       " 'HMDB51',\n",
       " 'INaturalist',\n",
       " 'ImageFolder',\n",
       " 'ImageNet',\n",
       " 'Imagenette',\n",
       " 'InStereo2k',\n",
       " 'KMNIST',\n",
       " 'Kinetics',\n",
       " 'Kitti',\n",
       " 'Kitti2012Stereo',\n",
       " 'Kitti2015Stereo',\n",
       " 'KittiFlow',\n",
       " 'LFWPairs',\n",
       " 'LFWPeople',\n",
       " 'LSUN',\n",
       " 'LSUNClass',\n",
       " 'MNIST',\n",
       " 'Middlebury2014Stereo',\n",
       " 'MovingMNIST',\n",
       " 'Omniglot',\n",
       " 'OxfordIIITPet',\n",
       " 'PCAM',\n",
       " 'PhotoTour',\n",
       " 'Places365',\n",
       " 'QMNIST',\n",
       " 'RenderedSST2',\n",
       " 'SBDataset',\n",
       " 'SBU',\n",
       " 'SEMEION',\n",
       " 'STL10',\n",
       " 'SUN397',\n",
       " 'SVHN',\n",
       " 'SceneFlowStereo',\n",
       " 'Sintel',\n",
       " 'SintelStereo',\n",
       " 'StanfordCars',\n",
       " 'UCF101',\n",
       " 'USPS',\n",
       " 'VOCDetection',\n",
       " 'VOCSegmentation',\n",
       " 'VisionDataset',\n",
       " 'WIDERFace',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__getattr__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_optical_flow',\n",
       " '_stereo_matching',\n",
       " 'caltech',\n",
       " 'celeba',\n",
       " 'cifar',\n",
       " 'cityscapes',\n",
       " 'clevr',\n",
       " 'coco',\n",
       " 'country211',\n",
       " 'dtd',\n",
       " 'eurosat',\n",
       " 'fakedata',\n",
       " 'fer2013',\n",
       " 'fgvc_aircraft',\n",
       " 'flickr',\n",
       " 'flowers102',\n",
       " 'folder',\n",
       " 'food101',\n",
       " 'gtsrb',\n",
       " 'hmdb51',\n",
       " 'imagenet',\n",
       " 'imagenette',\n",
       " 'inaturalist',\n",
       " 'kinetics',\n",
       " 'kitti',\n",
       " 'lfw',\n",
       " 'lsun',\n",
       " 'mnist',\n",
       " 'moving_mnist',\n",
       " 'omniglot',\n",
       " 'oxford_iiit_pet',\n",
       " 'pcam',\n",
       " 'phototour',\n",
       " 'places365',\n",
       " 'rendered_sst2',\n",
       " 'sbd',\n",
       " 'sbu',\n",
       " 'semeion',\n",
       " 'stanford_cars',\n",
       " 'stl10',\n",
       " 'sun397',\n",
       " 'svhn',\n",
       " 'ucf101',\n",
       " 'usps',\n",
       " 'utils',\n",
       " 'video_utils',\n",
       " 'vision',\n",
       " 'voc',\n",
       " 'widerface']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203bbe56-05a9-4d0a-b9e0-2ae471afe2c1",
   "metadata": {},
   "source": [
    "All built-in datasets are a subclass of the module [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). Running the code cell below will show the function (or variable) names of the MNIST dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c948962b-e40e-43e2-aa1b-4076af3c4c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_check_exists',\n",
       " '_check_legacy_exist',\n",
       " '_format_transform_repr',\n",
       " '_is_protocol',\n",
       " '_load_data',\n",
       " '_load_legacy_data',\n",
       " '_repr_indent',\n",
       " 'class_to_idx',\n",
       " 'classes',\n",
       " 'download',\n",
       " 'extra_repr',\n",
       " 'mirrors',\n",
       " 'processed_folder',\n",
       " 'raw_folder',\n",
       " 'resources',\n",
       " 'test_data',\n",
       " 'test_file',\n",
       " 'test_labels',\n",
       " 'train_data',\n",
       " 'train_labels',\n",
       " 'training_file']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(datasets.MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5539d7ff-cde0-4dfb-ba96-ccb144fb004b",
   "metadata": {},
   "source": [
    "If we take a closer look at the the [datasets.MNIST](https://pytorch.org/vision/0.16/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) documentation we can see how we can load the dataset into our jupyter notebook.\n",
    "\n",
    "It takes several parameters:\n",
    "- root (string): Root directory of the dataset where the MNIST data should exist. This is a directory you can specify yourself. Another parameter named 'download' will download the MNIST dataset from the internet and place it in this directory.\n",
    "- train (bool, optional): If this parameter is set to True, it creates a training dataset. Otherwise, it will create a test dataset. As mentioned this parameter is optional, so you do not need to pass it with a value. Its default value is set to True.\n",
    "- download (bool, optional): If this parameter is set to True, it downloads the dataset from the internet and puts it in the specified root directory. If the dataset is already downloaded, it is not downloaded again. This parameter is optional, so you do not need to pass it with a value. Its default value is set to False.\n",
    "- transfrom (callable, optional): A function/transform can be passed that takes in a PIL type image and returns a transformed version. See e.g. [torchvision.transforms](https://pytorch.org/vision/0.9/transforms.html). This parameter is optional, so you do not need to pass it with a value. Its default value is set to None.\n",
    "- target_transform (callable, optional): a function/transform can be passed that takes in the target and transforms it. This parameter is optional, so you do not need to pass it with a value. Its default value is set to None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ec331-e53e-4197-9d13-b046d9e2c340",
   "metadata": {},
   "source": [
    "Lets now load the MNIST training set, by executing the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3da8452-4e0c-4925-9ac9-78a4362f00a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 21102226.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2599808.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.MNIST(root='./data', download=True, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e5974-7fcd-42e6-8a60-9473f8cf71d2",
   "metadata": {},
   "source": [
    "If everything went correct, a new directory was created in the same directory as this jupyter notebook called 'data'. Inside this directory you will find the MNIST training data (images and labels). The dataset object is now loaded in the variable named 'train_set'. Like mentioned before, this dataset object has several functions which can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19cd36d8-ba93-4a88-81db-32fe1fad9178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_check_exists',\n",
       " '_check_legacy_exist',\n",
       " '_format_transform_repr',\n",
       " '_is_protocol',\n",
       " '_load_data',\n",
       " '_load_legacy_data',\n",
       " '_repr_indent',\n",
       " 'class_to_idx',\n",
       " 'classes',\n",
       " 'data',\n",
       " 'download',\n",
       " 'extra_repr',\n",
       " 'mirrors',\n",
       " 'processed_folder',\n",
       " 'raw_folder',\n",
       " 'resources',\n",
       " 'root',\n",
       " 'target_transform',\n",
       " 'targets',\n",
       " 'test_data',\n",
       " 'test_file',\n",
       " 'test_labels',\n",
       " 'train',\n",
       " 'train_data',\n",
       " 'train_labels',\n",
       " 'training_file',\n",
       " 'transform',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401452a2-020c-4099-a7c7-07f6946486d2",
   "metadata": {},
   "source": [
    "For example we can ask for the training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d99998d-eff6-42a1-ba6d-f288c8488db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 4,  ..., 5, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "print(train_set.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca492010-8d2d-48da-9629-fdaa96abc4b6",
   "metadata": {},
   "source": [
    "Or we can ask for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1991a52-9fc0-44c2-a6b7-43c8e9bff712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e560f-7d27-4150-8bf6-9c28552c849e",
   "metadata": {},
   "source": [
    "You can get the number of training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa57c756-027f-476e-b017-19e68f414de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df253c10-4253-482e-8f4e-1e4f9e330dca",
   "metadata": {},
   "source": [
    "You can also pick a specific training sample, which is a tuple of an [PIL](https://pillow.readthedocs.io/en/stable/) image and its corresponding label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "241841f2-3b82-4771-b144-93e801bb595d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350fac7f-7c61-4b6a-8f8e-ecea72075f98",
   "metadata": {},
   "source": [
    "Maybe you already noticed, but this is different from when you would take the image data via `train_set.data` and `train_set.target` where, as you could see in the example above, the image data is stored as tensors. This is the case because the `Dataset` class comes with a [`__getitem__()`](https://pytorch.org/vision/0.16/_modules/torchvision/datasets/mnist.html#MNIST) method that will turn the tensor into a PIL image. This `__getitem__()` method is used when you access a data sample by using the index on the entire dataset object. The method will also perform a transform on the data if a transform function is available, but more on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21f895-26ae-48e6-a4d6-2b8fd06795ef",
   "metadata": {},
   "source": [
    "We then use display, which is a method that can be used on PIL image objects, on the image related to training sample 2, by taking the first element (at index zero) of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b21fc70d-67cc-48b7-8375-7ba8ee08bac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCADIAMgBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiipIY/MfbXceFfBY1qdUZetdB4p+GSaLZecFA4zXldzB5MhX0qvRRRRRRWhp1n9pbGM10EXhvfHnbWFq1j9jk24xWZRRRRRRRRRRWho8XnXqrXuvgi3+wskgWtP4haq1xppQj+GvnfUf8AXMfes+iiiiiiuv8AB1kbqUALnmvXLLQFFrl0xxXlPj21W2uyF9a4qiiiiiiiiiitrwzEZdTUAZ5FfRXhbTWWBC64GKzviFHFHYtgjpXz5qLAzNj1rPoooooor1r4RWcd1cDcO9e2avZpa2fyADivnP4hsWvTn1rhaKKKKKKKKKciM7bVGTXf/DrQJ59YjeSM7SR2r6g/s+Gx0dWUAMEFeG/EjU3MUig+teJySGRiTTKKKKKKUAmvZPg3A4uAcd69p8QqRZnPpXzT8Qf+P0/WuHooooooooorS0KJZtViRuhNfUPgTQbWOCGVUG7ArrvE9x9m0w84GK+afHOoLcNIoOeteaUUUUUUVd0+2NzKABnmvoL4UaUbdkYrXpHilMWhx6V8wfEH/j9P+9XD0UUUUUUUUVr+Gk36zCPevrfwRAyWETHptFJ8Q5TForMD/Ca+UNevGlu5AT3NYNFFFFFFdP4QtxPeAEd6+mfAlkIYlIGK0fGFysVuwJ7V8w+PJhLeMR/erjKKKKKKKKKVFLOFHUnFeh+CPCk019FOVOM19SeHrT7JpsaEYOBXLfE+4C6My5/hNfJmpOWvpPrVOiiiiiiux8B4+3jP96vqLweB9nXHpXO/Em++zRsM181+I7r7RcMc96wqKKKKKKK07DSZLwjCnmuz0T4eTXNxFIVOM5r3rwn4VWwt48pyB6V2N5crp9lv6ACvEPiJ4rW5gkiDeorwK4fzLh29TUVFFFFFFdj4E/4/x/vV9ReDv+PYfSuN+K0bsjbRXzdqqyLcsGBAzWfQAScAZNXYtPldN2w/lVq00aWeTbsP5V0Vt4IeZQdh/Krg+HzcfJW9pHwlF/jKV0EfwPjQg7RW9YfC6CwUEgcVu2Nnb6dKI+OK7WwkRoRt9KyfGUvlaM7Zxwa+UfFWoNJeSru7muQPU0UUUUUUV2HgVlW/G4/xV9QeELiHyFAYZxV3xH4eh1iI7sEmvMtU+FdvNITsFZw+Elv/AHB+VSW/wlt/OHyD8q6m0+E9oIRlVq9bfC+xt33YWtmHwhYwLj5Khu9EsYl4K8VTTVbfSfuleKzdQ+Iywg4YcVx2q/Fx4wyq1cpbfEy5vdVjXLYZq+iPB9217piSt1K1W+IEnl+HpD7Gvj7WLgzajPk9GNZ1FFFFFFFXdP1GSwk3pnNeneDfH06SKjuRg+tevad40SaNd0g/OrkniS3f+NfzqP8A4SG3/vr+dKniK3Vwd6/nUt340igtztkXj3ritS+J0sTsFk/WuU1P4tXkYO1z+dc7N8V9Slzkv+dZlz4/vbnO4t+dZM/iS4nzknmsuW4e5kAYnk123g/wytzdwykZ5r6m8MWYstJjXpxXOfEm8QaNLGGHQ18j3/N/P/vmq1FFFFFFFFT2109q+5Dg1rw+Kr6EYVj+dTf8JpqP98/nR/wmeo/3z+dH/CZ6j/fP50yXxfqEq7Wc4+tZ0urTzElifzqnLK0pyxqOiipLcZuIx/tV9A/DuwjeGJiBnivaiRa6ZkcYFeJ/EHXvMjli3+teA3TbrmRvVqioooooooooooooooooooqW2/4+Y/8AeFfRPw5J+zw/hXrWsMU0JyP7tfK3jW/kbUJlJOMmuCc5cmm0UUUUUUUUUUUUUUUUUUVLb/8AHzH/ALwr6I+HP/HvD+Fet6wu7QnA/u18q+NbVhqEzY7muDcYcim0UUUUUUUUUUUUUUUUUUVLb/8AHzH/ALwr6I+HP/HvF+Fev6j/AMgZs/3a+afHKr9pm47mvLJf9a31plFFFFFFFFFFFFFFFFFFFS2//HzH/vCvoj4c/wDHvF+FewXa+ZpJUf3a+e/HmmMrzPjua8anGJ3HoajoooooooooooooooooooqW2/4+Y/8AeFfRXw5A+zRc+lexuMad+FeK/EHHkzcetfPt1/x9Sf71Q0UUUUUUUUUUUUUUUUUUU+E4mQ+hr2zwBrSxGKMtjkV9BW00M+mK29dpTnmvGfiJ5XlTbWB6189XX/H1J/vVDRRRRRRRRRRRRRRRRRRRSg4INbGm6/Np7qyE8V2UHxb1CG2EIL4xisDVvGtzqgYOW59a5V23uWPc02iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiv/Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=200x200>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_set[2][0].resize((200,200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dcb9c8-6f8d-4281-bd86-8b2e72773679",
   "metadata": {},
   "source": [
    "The label the corresponds to this image is stored as second element (index 1) of this tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f7b5b2-7ff9-4a5f-8cdf-612bdce8b59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Label =\", train_set[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89e24d-4a2b-4458-9930-b32a8ff6185e",
   "metadata": {},
   "source": [
    "We can also load the MNIST test set by switching the train parameter to False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "922bf533-429d-40f7-8dd3-620eb052b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torchvision.datasets.MNIST(root='./data', download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7ab7ea4-fcda-4fa9-803c-d809630f786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f9f0aa3-c513-4a4e-ac68-5ee974382fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a0e72-ed38-4f09-9701-c3e9ba855d46",
   "metadata": {},
   "source": [
    "As you can see, the test set contains similar data tuples where the first element holds the image and the second element the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66071d0c-171f-4793-ad37-a7830475a60c",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba8258-c6b4-46b0-9174-cd9a524e0542",
   "metadata": {},
   "source": [
    "Now you could think we are done as we downloaded the dataset from the internet and loaded it into a dataset object/variable. Yes this is correct. However, to train machine learning models you will often need to make sure the data is in the correct format or preprocessed in a certain way. This can easily be done by making use of the transform parameter which we can pass to the dataset module. Lets now take a closer look into this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d2c79-59db-4a72-ab7d-cd05295bd852",
   "metadata": {},
   "source": [
    "Before we will transform entire datasets, lets zoom in a bit to see what is meant with transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b67dd30d-239a-4bd0-9038-3011960558d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=200x200>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label is of type:  <class 'int'>\n",
      "\n",
      "Image is of type:  <class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "train_image, train_label = train_set[0]\n",
    "\n",
    "print(\"Label =\", train_label)\n",
    "print(\"\\n\")\n",
    "display(train_image.resize((200,200)))\n",
    "\n",
    "print(\"\\nLabel is of type: \", type(train_label))\n",
    "print(\"\\nImage is of type: \", type(train_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03366bd-68cc-41bc-b28f-c591d5f711fc",
   "metadata": {},
   "source": [
    "#### Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f774a-e342-42c8-89e6-7b930c16258d",
   "metadata": {},
   "source": [
    "We know that the dataset contains PIL images and Integer labels. However, to be able to train neural networks with PyTorch we will need to transform the PIL images to tensors. In a previous tutorial we explained what tensors are and what you can do with them. If you do not remember or did not follow this tutorial we encourage you to take a look at it before continuing with this tutorial. \n",
    "\n",
    "Now lets first transform one PIL image into tensor format to get an idea of what this does to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21b8bd2e-e7fd-4d74-bcfa-f083d5ea79f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=L size=28x28 at 0x2072FAB90D0>\n"
     ]
    }
   ],
   "source": [
    "print(train_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d1242-5591-4912-a67b-1c8ecd44e38d",
   "metadata": {},
   "source": [
    "To transform this PIL image into tensor format we can make use of the [PILToTensor](https://pytorch.org/vision/main/generated/torchvision.transforms.PILToTensor.html#torchvision.transforms.PILToTensor) class of the transforms package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b149b23-ff3e-47b0-9e7f-79180321c8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
      "           18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
      "          253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
      "          253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
      "          198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
      "           11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
      "            2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
      "           70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
      "          225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
      "          240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
      "          229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
      "          253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
      "          253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
      "           80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]],\n",
      "       dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# First make a PILToTensor() object\n",
    "p2t = transforms.PILToTensor()\n",
    "\n",
    "# Then use the object to transform the PIL image to tensor\n",
    "train_image_tensor = p2t(train_image)\n",
    "\n",
    "print(train_image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fc562-c2d9-46c0-a375-59b145533db6",
   "metadata": {},
   "source": [
    "We can also make use of the Compose class inside the transforms package to chain several transforms together. Applying the PILToTensor transform with the Compose would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdcac119-705f-47a1-93e2-8216590c6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "train_image_tensor = transform(train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3b681ac-d317-4d4a-bd70-4b10494a9484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
      "           18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
      "          253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
      "          253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
      "          198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
      "           11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
      "            2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
      "           70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
      "          225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
      "          240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
      "          229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
      "          253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
      "          253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
      "           80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]],\n",
      "       dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(train_image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf446f-41d1-40ff-aaad-d4d6310bf0b1",
   "metadata": {},
   "source": [
    "Below you will find a method which you can use to plot the image tensors. It takes the image data in tensor format and the Integer label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc934cba-a89e-477b-9cc7-32a49f9d79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image,label,colored=False):\n",
    "    figure = plt.figure(figsize=(4, 4))\n",
    "    if not colored:\n",
    "        plt.imshow(image[0].cpu(), cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(\"label = \" + str(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba6c15a9-a5ae-4931-8cd4-80a5ca57c396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(train_image_tensor,train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbb8f3-8434-4a23-ba6b-bf1bfdd45243",
   "metadata": {},
   "source": [
    "#### Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c7114-619f-44eb-932d-9d781a261141",
   "metadata": {},
   "source": [
    "Another transform we might want to perform is to normalize all data. By normalizing all data we can ensure, in the case of using image data, that all pixel values are in the same range (for example in the range 0 to 1 or -1 to 1) and so we could reduce learning time. The process of training a machine learning model could be affected by the different pixel values as some images might include higher or lower pixel values than similar cases.\n",
    "\n",
    "To get the data between 0 and 1 is as simply as dividing it by the maximum pixel value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bf09ad1-28a8-4eee-b146-135724d20ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum pixel value:  0\n",
      "Maximum pixel value:  255\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum pixel value: \", train_set.data.min().item())\n",
    "print(\"Maximum pixel value: \", train_set.data.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8b3a329-8777-45b5-9c54-5c11d1fadd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum pixel value:  0.0\n",
      "Maximum pixel value:  1.0\n"
     ]
    }
   ],
   "source": [
    "normalized_train_set = train_set.data/255\n",
    "\n",
    "print(\"Minimum pixel value: \", normalized_train_set.data.min().item())\n",
    "print(\"Maximum pixel value: \", normalized_train_set.data.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b5437e-75cc-49eb-ad16-ca3df5c84f17",
   "metadata": {},
   "source": [
    "Or we could use the `transforms.ToTensor()` class on the PIL images. Later on we will learn how we can apply this on the entire dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4b8cfe3-fce9-487d-b5e2-522dbe3b707e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum pixel value:  0.0\n",
      "Maximum pixel value:  1.0\n",
      "\n",
      "Minimum pixel value:  0.0\n",
      "Maximum pixel value:  1.0\n"
     ]
    }
   ],
   "source": [
    "train_image, train_label = train_set[0]\n",
    "\n",
    "# Example 1 \n",
    "\n",
    "d2t = transforms.ToTensor()\n",
    "train_image_tensor = d2t(train_image)\n",
    "print(\"Minimum pixel value: \", train_image_tensor.min().item())\n",
    "print(\"Maximum pixel value: \", train_image_tensor.max().item())\n",
    "\n",
    "# Example 2\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_image_tensor = transform(train_image)\n",
    "print(\"\\nMinimum pixel value: \", train_image_tensor.min().item())\n",
    "print(\"Maximum pixel value: \", train_image_tensor.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd95615f-ff76-432e-b763-cbe322074816",
   "metadata": {},
   "source": [
    "To get them in the range of -1 and 1 we can use [`transforms.Normalize()`](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.Normalize) to apply normalization on the dataset samples. It does not support PIL images so you would first need to transform them into tensors. Also the Normalization method expects Float instead of Integer, so we will make use of the [`transforms.ConvertImageDType()`](https://pytorch.org/vision/0.10/transforms.html#torchvision.transforms.ConvertImageDtype) to convert the image to float.\n",
    "\n",
    "It also takes a few parameters which are the mean and standard deviation of the entire dataset. To be able to get the MNIST data in the range of -1 and 1 we need to divide the data by 255 and use a mean and standard deviation of 0.5. This is because the `transforms.Normalize` method will transform the data by applying: \n",
    "\n",
    "$$\n",
    "new\\_pixel\\_value = \\frac{pixel\\_value - mean}{standard\\_deviation}\n",
    "$$\n",
    "\n",
    "So if we divide all data by the maximum pixel value 255 we will get the values between 0 and 1. Then if we use these mean and standard deviation values with the Normalization method we will get the values between -1 and 1.\n",
    "\n",
    "$$\n",
    "-1 = \\frac{0 - 0.5}{0.5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "1 = \\frac{1 - 0.5}{0.5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50d56c-d037-49d2-ae99-b74da3aeca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_label = train_set[0]\n",
    "\n",
    "# Example 1:\n",
    "\n",
    "p2t = transforms.PILToTensor()\n",
    "norm_img = transforms.Normalize(0.5, 0.5)\n",
    "tensor_train_image = p2t(train_image)\n",
    "normalized_train_image = norm_img(tensor_train_image.float()/255)\n",
    "\n",
    "print(\"Minimum pixel value: \", normalized_train_image.min().item())\n",
    "print(\"Maximum pixel value: \", normalized_train_image.max().item())\n",
    "\n",
    "# Example 2:\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float64), #this transform will also normalize between 0 and 1\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])\n",
    "\n",
    "normalized_train_image = transform(train_image)\n",
    "print(\"\\nMinimum pixel value: \", normalized_train_image.min().item())\n",
    "print(\"Maximum pixel value: \", normalized_train_image.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc4b07-86ee-499a-991c-f99b2aed7ef3",
   "metadata": {},
   "source": [
    "#### Other transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494d588-4139-4f51-83bc-7bb2e68c4ff8",
   "metadata": {},
   "source": [
    "There is an entire list of pre-defined transforms you could apply on the data. Here we show only a few examples. There can be many reasons to apply these transforms. For example, by flipping the images in a certain way you could create more data yourself. With numbers this is not a wise thing to do. However, if the task is to determine the color of the objects in the images, then it does not matter if the image is upside down or if an object is directed to the right or to the left. As long as the color of the object can be recognized.\n",
    "\n",
    "To flip image you can make use of [`transforms.RandomHorizontalFlip()`](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomHorizontalFlip) or [`transforms.RandomVerticalFlip()`](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomVerticalFlip) which both take as parameter value `p` which is the probability that the flip is applied. If you set this value to 1.0 it will always flip the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd6d90-51e4-405d-b897-c89a22e96be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal and Vertical Flip code\n",
    "train_image, train_label = train_set[0]\n",
    "\n",
    "hor_flip = transforms.RandomHorizontalFlip(p=1.0)\n",
    "ver_flip = transforms.RandomVerticalFlip(p=1.0)\n",
    "\n",
    "print(\"Original view training image:\")\n",
    "display(train_image.resize((200,200)))\n",
    "\n",
    "print(\"\\nHorizontally flipped view training image:\")\n",
    "display(hor_flip(train_image).resize((200,200)))\n",
    "\n",
    "print(\"\\nVertically flipped view training image:\")\n",
    "display(ver_flip(train_image).resize((200,200)))\n",
    "\n",
    "print(\"\\nVertically flipped view of horizontally flipped training image:\")\n",
    "display(ver_flip(hor_flip(train_image)).resize((200,200)))\n",
    "\n",
    "print(\"\\nHorizontally flipped view of vertically flipped training image:\")\n",
    "display(hor_flip(ver_flip(train_image)).resize((200,200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b37ee6-2a7f-4bdd-8296-af3cbfa7da5c",
   "metadata": {},
   "source": [
    "Or maybe it does not matter what color the object is but it is of importance that your model recognizes the edges of an object. In this case you want to make the edges more clear and to do so you first want to turn the colored image into grayscale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2d32e-398f-4ddd-be81-2b589823447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grayscale Code\n",
    "color_image = Image.open(\"hello_kitty.jpg\")\n",
    "\n",
    "img2gray = transforms.RandomGrayscale(p=1.0)\n",
    "\n",
    "print(\"Original Color Image:\")\n",
    "display(color_image.resize((200,200)))\n",
    "print(\"\\nGrayscale Image:\")\n",
    "display(img2gray(color_image).resize((200,200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f7780-7c3b-4da2-ba3d-d905b749b417",
   "metadata": {},
   "source": [
    "While we hope this short section on transformations helped you understand what transformations are, what they are used for and how you can apply them. It is a whole topic on its own and there are many other transformations which we did not mention. For this course you will mainly need to know how to transform your data into tensors, the focus will not be on improving results by experimenting with different preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeebfa7-dc81-40b9-9ffb-ac783a66becc",
   "metadata": {},
   "source": [
    "### Download, Load and Transform All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85464077-19ac-4e98-a51b-ace237daa95e",
   "metadata": {},
   "source": [
    "Now that we have explained what transformations are we would like to show you how you can download, load and transform all at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16c0ac-e4af-46d9-955c-f090b32ff071",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.MNIST(root='./data', transform=transforms.Compose([transforms.ToTensor()]), download=True, train=True)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', transform=transforms.Compose([transforms.ToTensor()]), download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b94d9d-2b3e-46c2-ab85-5197ca155500",
   "metadata": {},
   "source": [
    "### Create your own Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f0958-0d8d-430b-a770-36836e5d0ca0",
   "metadata": {},
   "source": [
    "You can also use Pytorch's Dataset class to [create your own custom dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files). A custom Dataset class must implement three functions: `__init__`,`__len__` and `__getitem__`.  \n",
    "\n",
    "What a class is and how to build one is explained in a previous tutorial. Please take a look at that tutorial if this part is not clear to you. \n",
    "\n",
    "The `__init__` function is run once when instantiating the Dataset object. Here you can set class variables that can be used throughout the class. The `__len__` function returns the total number of samples found in your custom dataset. finally, `__getitem__` function loads and returns a sample from the dataset at the given index `idx`. So when you use `custom_dataset[1]`, this function is used to load and return the sample at index 1. \n",
    "\n",
    "In the code snippet below we will create a custom dataset class based on the MNIST dataset. The only difference is that when we load a data sample we will flip it horizontally. Just to show you how to create a custom dataset. You can ofcourse create a totally different dataset using images locally stored. Or you use this custom dataset class to perform different operations on the images when you load them with `__getitem__`. This is all up to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6db7f2-7db6-403c-8e97-cd44b445fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMNISTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, original_mnist_dataset):\n",
    "        self.original_set = original_mnist_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_set)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.original_set[idx]\n",
    "        horizontal_transform = transforms.RandomHorizontalFlip(p=1.0)\n",
    "        return horizontal_transform(img), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae49ea2-5cee-4205-a32a-194de0410e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_set = CustomMNISTDataset(train_set)\n",
    "\n",
    "train_image, train_label = train_set[50]\n",
    "custom_image, custom_label = custom_set[50]\n",
    "\n",
    "plot_image(train_image,train_label)\n",
    "plot_image(custom_image,custom_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef7aa7-c80e-46a0-97bb-c6c79be2792b",
   "metadata": {},
   "source": [
    "## PyTorch DataLoaders (MNIST Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aaaecb-0712-4883-b42f-43da567591eb",
   "metadata": {},
   "source": [
    "In the first part of this tutorial you have learned how to load and transform the MNIST dataset. Now, you can use this data to train a machine learning model in several ways. Either you train the model once on the entire dataset. Or you leave a part aside to validate your model inbetween training. You can also train your model in multiple runs. This can be done sequentially each run, where you feed the training samples in the same order or you shuffle them each round. Moreover, you can also divide your training set in smaller groups, otherwise known as batches, to reduce the memory needed as you do not load the training data all at once. This has the additional benefit that you can process data in parallel for faster data loading. Either way, you have multiple options on how to feed the data to your model. \n",
    "\n",
    "PyTorch [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class is an utility class that provides you the tools to load data from a Dataset and create mini-batches for training your models. It is designed to handle large datasets and enables you to perform [data augmentation, shuffling, and other preprocessing tasks](https://saturncloud.io/blog/pytorch-dataloader-features-benefits-and-how-to-use-it/). Instead of having to implement your data training strategy yourself, you can simply use this class to make your life easier. The following code snippet shows how to wrap a DataLoader around the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a9678-57d9-4dfc-a40d-7cfd0593c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders code\n",
    "train_set = torchvision.datasets.MNIST(root='./data', transform=transforms.Compose([transforms.ToTensor()]), download=True, train=True)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', transform=transforms.Compose([transforms.ToTensor()]), download=True, train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aaba11-8d7c-4db4-ba60-ff5885fa1f6f",
   "metadata": {},
   "source": [
    "As you can see from this code snippet and from the pytorch documentation, the `DataLoader` class takes a few parameters. The most important for now are:\n",
    "- dataset (Dataset): dataset from which to load the data.\n",
    "- batch_size (int, optional): how many samples per batch to load (default: 1).\n",
    "- shuffle (bool, optional): set to `True` to have the data reshuffled at every epoch (default: `False`).\n",
    "- sampler (Sampler or Iterable, optional): defines the strategy to draw samples from the dataset. Can be any `Iterable` with `__len__` implemented. If specified, `shuffle` must not be specified. (default: None).\n",
    "- batch_sampler (Sampler or Iterable, optional): like `sampler`, but returns a batch of indices at a time. Mutually exclusive with `batch_size`, `shuffle`, `sampler` and `drop_last`. (default: None).\n",
    "- num_workers (int, optional): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0).\n",
    "- pin_memory (bool, optional): If `True`, the data loader will copy Tensors into device/CUDA pinned memory before returning them. (default: `False`).\n",
    "\n",
    "The other parameters can be found [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Below we show you how to iterate over the dataloader and get images with corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f4884a-7191-4b0f-839e-3bccecae8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through dataloader\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b549a3d4-b647-474d-916e-c60bf219dd5c",
   "metadata": {},
   "source": [
    "Now you know everything there is to know for this course on loading data and using the dataloader to efficiently feed data to your machine learning model. In a next tutorial we will focus on building machine learning models and we will then use the dataloader to actually start training models.\n",
    "\n",
    "Before you start on the next tutorial you can test your knowledge on Datasets and Dataloaders with the exercises below. This time you will be asked to load the data from the CIFAR-10 dataset yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c6c23-f8eb-47f5-a040-e8c9be79a8a8",
   "metadata": {},
   "source": [
    "## Exercises (CIFAR-10 Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d806d-cd5c-41d0-b4f7-cf4258fa7efe",
   "metadata": {},
   "source": [
    "We showed you how to load the MNIST dataset and now you are going to try to do the same but load the CIFAR10 dataset. In some cases you will need to finish the code we already added and in other cases you might need to add some more yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf2cf8-cf18-4080-89a7-15436716adb7",
   "metadata": {},
   "source": [
    "### Load CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd873ff-ec7f-4ed8-a3d6-0fc6cbee5b75",
   "metadata": {},
   "source": [
    "Finish the code below in such a way you do not get an assertion error or any other error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4228cad-9d36-4a9c-8245-4c62e32e756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.CIFAR10(root='./data', download=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205aaa6-235d-47a4-a083-b8787ee397be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torchvision.datasets.CIFAR10(root='./data', download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067970f0-899c-4254-96db-ea6e23137107",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ...\n",
    "assert type(train_set) ==  datasets.CIFAR10\n",
    "assert len(train_set) == 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574cb801-6d9f-4129-91ee-28992427a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ...\n",
    "assert type(test_set) == datasets.CIFAR10\n",
    "assert len(test_set) == 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52be878-33cd-4511-903b-b3ebad9caecf",
   "metadata": {},
   "source": [
    "Lets visualize the data by executing the code snippet below. This snippet should run without errors if you loaded the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0fb3e-35c5-4d5e-b474-2d79492af94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_label = train_set[1]\n",
    "print(\"Label = \", train_label)\n",
    "display(train_image.resize((200,200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e580ad9c-95ad-4a78-b2d3-8491d24e8617",
   "metadata": {},
   "source": [
    "As you can probably see we are dealing with colored PIL images and Integer labels. CIFAR10 contains [10 different category images](https://www.cs.toronto.edu/~kriz/cifar.html). The 10 categories/classes are in order of label number:\n",
    "- airplane\n",
    "- automobile\n",
    "- bird\n",
    "- cat\n",
    "- deer\n",
    "- dog\n",
    "- frog\n",
    "- horse\n",
    "- ship\n",
    "- truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ea285-42e3-4e52-a4d4-4596460e4b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['airplaine', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a40aca-d30c-4f84-8c87-b98c733d3c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label = \", classes[train_label])\n",
    "display(train_image.resize((200,200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fbca5-a4e1-4152-9baa-015aadf18ac3",
   "metadata": {},
   "source": [
    "Execute the code snippet below to examine the shape of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e943b4-7433-49cf-84fc-68c5abf13073",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_set.data\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e1713-585c-4e3c-9b10-3ab64bd8a84d",
   "metadata": {},
   "source": [
    "As you can see there are 50.000 images of shape 32x32x3. So not only are they a different size then the MNIST images, which were of size 28x28. Also they include 3 channels instead of only 1. As these are colored images, we are dealing with RGB color channels. Keep this in mind when working with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90985a8-faab-4721-b292-6405cb576126",
   "metadata": {},
   "source": [
    "### Transform CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cb0a2-c903-4ced-bddc-d42c606e6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_label = train_set[1]\n",
    "\n",
    "transform = ...\n",
    "\n",
    "transformed_image = ...\n",
    "\n",
    "plot_image(transformed_image,classes[train_label],colored=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e71b0a-092b-4f13-9268-f1ffd2077589",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_label = train_set[1]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.RandomVerticalFlip(p=1.0)\n",
    "])\n",
    "\n",
    "transformed_image = transform(train_image)\n",
    "\n",
    "plot_image(transformed_image,classes[train_label],colored=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cdabbc-570c-4a25-9b15-46544e9890de",
   "metadata": {},
   "source": [
    "### Create Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7587c7-1f42-4fdb-8bdc-587edd3cb10d",
   "metadata": {},
   "source": [
    "Now create a custom CIFAR10 dataset that when, `__getitem__` is called, returns grayscale vertically flipped tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00163f21-149a-42dc-a09a-c1c857c661d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class __(Dataset):\n",
    "\n",
    "    def ...():\n",
    "\n",
    "    def ...():\n",
    "\n",
    "    def ...():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a143053-70ea-4f26-934c-53bca4c47834",
   "metadata": {},
   "source": [
    "Finish the code snippet below. The image displayed should be gray and vertically flipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1edf4-8a30-4adc-9bb8-1e86522b0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_train = ...\n",
    "\n",
    "img, label = custom_train[1]\n",
    "plot_image(img,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee33a95f-ea2b-4974-903d-0e8b5d2ff99b",
   "metadata": {},
   "source": [
    "### Use DataLoaders on the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fc9ce-dd4b-4c95-a6e8-f6dc3e17de6a",
   "metadata": {},
   "source": [
    "Finally, you are going to wrap the loaded CIFAR10 dataset into a DataLoader. Create DataLoader of your custom dataset with batch size of 128 and shuffle set to True. Then load one batch and display the data of one batch like done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c68c10-97b1-46ef-aac1-d56f4e0ba7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(..., ..., ...)\n",
    "\n",
    "train_features, train_labels = ...\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plot_image(img,classes[label.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
