{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4066a7",
   "metadata": {},
   "source": [
    "# More on Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edd375",
   "metadata": {},
   "source": [
    "This is the last Python/Pytorch refresher tutorial before we start with the actual course related labs. \n",
    "\n",
    "We already showed you how you can create machine learning models with PyTorch and also how to train them. In the previous tutorial we also showed you how to download and load the MNIST and CIFAR-10 datasets using PyTorch. This final tutorial will give a more detailed explanation on what options you have to train your machine learning model using these same datasets. We will show you how you can directly use the datasets or first wrap them in DataLoaders and then batch by batch train your model. Moreover, we will split the training set in a training and validation set for improved training and also show you how you can perform cross validation. \n",
    "\n",
    "We want to make sure you know how to perform the training and testing of your models. After this tutorial we will expect that you will know how to prepare your data, design your model and then train and test it. This gives us more time to help you with course related topics. So if you are stuck during the labs or with any of the homework assignments, please remember to take another look at these tutorials. You might find some answers here.\n",
    "\n",
    "Below we will first import some necessary packages, set the device and already load the MNIST data. We will provide a CNN model for you to use during this tutorial as well as some helper methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b14ac-c6ca-47b9-9987-602e75927f1e",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Below we import some necessary PyTorch packages and some others for plotting or computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47413282-0ba5-457b-83c1-0d4ccf73b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "#PyTorch Specific libraries\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6a5ee-41e4-4183-958a-2ec8e147a596",
   "metadata": {},
   "source": [
    "## Device\n",
    "This piece of code is required to make use of the GPU instead of CPU for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29a6d1-a370-47d9-98e0-50696f1b8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code is required to make use of the GPU instead of CPU for faster processing\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98aed56-b515-4d4d-b1a8-0da40d874fc5",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Like explained in a previous tutorial, we will load the training and test set of the MNIST dataset. We already transform all data to tensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c09c3-1ccc-42cd-9958-970cafb650ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders code\n",
    "train_set = torchvision.datasets.MNIST(root='./data', transform=transforms.Compose([transforms.ToTensor()]), download=True, train=True)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', transform=transforms.Compose([transforms.ToTensor()]), download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2dcf54-cecd-42cf-b498-3cb8935bf985",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Below you will find a standard CNN model architecture which you may use during this tutorial. Layers and related parts are explained in a previous tutorial, so for information on these topics please take another look at that specific notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0ff6c-845f-4efa-bdc1-f335468e056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 128, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 224, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.drop2 = nn.Dropout(p=0.4)\n",
    "        \n",
    "        self.fc3 = nn.Linear(224*4*4, 64)\n",
    "        self.drop3 = nn.Dropout(p=0.4)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.drop4 = nn.Dropout(p=0.4)\n",
    "        \n",
    "        self.fc5 = nn.Linear(32, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.pool1(F.relu(self.conv1(x))))\n",
    "        x = self.drop2(self.pool2(F.relu(self.conv2(x))))\n",
    "        x = x.view(-1,224*4*4)\n",
    "        x = self.drop3(F.relu(self.fc3(x)))\n",
    "        x = self.drop4(F.relu(self.fc4(x)))\n",
    "        x = self.softmax(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ee79c-f730-425f-9491-dbbd60110918",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n",
    "\n",
    "Below is a method you can use to compute the accuracy of your model. You need to provide output labels (predictions) and original labels. Outputs should be the output of your model and labels should be a tensor filled with Integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df714d1-31cc-4895-96a2-5baae2975ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to compute accuracy\n",
    "def accuracy(outputs,labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return (torch.tensor(torch.sum(preds == labels).item() / len(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28f665-410b-45fb-b4e1-c7b759538da5",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92ecf6-49e6-4ae1-aea7-9a6a69602b7f",
   "metadata": {},
   "source": [
    "`plot_predictions()` can be used to plot images with their correct label and the prediction label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e4f427-263c-423c-adc8-aa6dd1a42c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(images,labels,preds):\n",
    "    # convert images to numpy arrays\n",
    "    np_images = images.detach().cpu().numpy()\n",
    "\n",
    "    # Make sure we can view the images\n",
    "    # First multiplying by 255 so the values are between 0 and 255\n",
    "    # Then reshaping the image so the rows and columns are in the correct order.\n",
    "    np_images = np_images*255\n",
    "    np_images = [image.astype(np.uint8).reshape((28, 28, 1)) for image in np_images]\n",
    "\n",
    "    # Plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(25, 8))\n",
    "    for idx in np.arange(20):\n",
    "        ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "        ax.imshow(np_images[idx], cmap='gray')\n",
    "        ax.set_title(f\"True Label = {labels[idx]};\\n Prediction = {preds[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ebbabd-a6fd-45d7-a591-1f7ee0ee50a4",
   "metadata": {},
   "source": [
    "`visualize_batch()` can be used to plot a batch of images and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d85c4-3acb-47af-8408-46533357bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch(images,labels):\n",
    "    # making sure we can view the images\n",
    "    images = images.detach().numpy()\n",
    "    images = images*255\n",
    "    images = [image.astype(np.uint8).reshape((28, 28, 1)) for image in images]\n",
    "    \n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    for idx in np.arange(20):\n",
    "        ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "        ax.imshow(images[idx], cmap='gray')\n",
    "        # print out the correct label for each image\n",
    "        # .item() gets the value contained in a Tensor\n",
    "        ax.set_title(str(labels[idx].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd5828-d20a-4787-81d6-ea8295cd9754",
   "metadata": {},
   "source": [
    "`get_batch_predictions()` can be used to obtain just one batch of images and labels from a dataloader and then create predictions/outputs using your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25799388-2772-4537-8a84-eabcb52c9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_predictions(dataloader):\n",
    "    # obtain one batch of images and labels\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        dataiter = iter(test_loader)\n",
    "        images, labels = next(dataiter)\n",
    "        images = (images.view(-1,1,28,28)).type(torch.FloatTensor).to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Generate predictions\n",
    "        predictions = model(images)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218ebff-a64a-42d5-ad8a-bf8818827bee",
   "metadata": {},
   "source": [
    "## Training Without DataLoader\n",
    "\n",
    "First we will show you how you can train the CNN model without using the DataLoader wrapper, just by iterating over all samples from the training set. We picked Cross Entropy Loss as the loss function and Adam with a learning rate of 0.0015 as the optimizer to be used during training. We will keep it like this for the other parts as well, however, just to be sure that every training is clean we will keep initializing them and the model at the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f4359-69e7-48d0-9a26-b12fa419a1a0",
   "metadata": {},
   "source": [
    "First we train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1b7bb-6584-4dfe-9344-5241fb754842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0015)\n",
    "\n",
    "# Execute Training\n",
    "for image, label in tqdm(train_set):\n",
    "    # Prepare single image \n",
    "    image = image.type(torch.FloatTensor).to(device)\n",
    "    # Prepare target label\n",
    "    label = torch.tensor([label], dtype=torch.int8).type(torch.LongTensor).to(device)\n",
    "    # Generate prediction\n",
    "    prediction = model(image)\n",
    "    # Calculate loss\n",
    "    loss = criterion(prediction, label)\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeb5d4-8677-4774-add2-3f9343fc9367",
   "metadata": {},
   "source": [
    "Next, we evaluate the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76466c8-38f7-46e4-a10c-69c74197481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Testing\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    for image, label in tqdm(test_set):\n",
    "        # Prepare single image\n",
    "        image = image.type(torch.FloatTensor).to(device)\n",
    "        # Prepare target label\n",
    "        label = torch.tensor([label], dtype=torch.int8).type(torch.LongTensor).to(device)\n",
    "        # Generate prediction\n",
    "        prediction = model(image)\n",
    "        # Calculate loss\n",
    "        loss = criterion(prediction, label)\n",
    "        # Compute gradients\n",
    "        acc = accuracy(prediction, label)\n",
    "        # Update weights\n",
    "        test_loss.append(loss)\n",
    "        # Reset gradients\n",
    "        test_acc.append(acc)\n",
    "        # Save prediction for plotting\n",
    "        test_predictions.append(torch.max(prediction,dim=1)[1].item())\n",
    "        \n",
    "\n",
    "# Display Results\n",
    "print(\"Test Loss: \", round(torch.stack(test_loss).mean().item(),2))\n",
    "print(\"Test Accuracy: \", round(torch.stack(test_acc).mean().item()*100.0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6ffcf-af21-4d47-bf6f-25114277b5d7",
   "metadata": {},
   "source": [
    "Then we plot some test images, their original labels and the predicted labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f22a4-0832-46bc-93f2-ba5b74310d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [test_set[i][0] for i in range(0,20)]\n",
    "test_images = torch.stack(test_images)\n",
    "test_labels = [test_set[i][1] for i in range(0,20)]\n",
    "test_preds = [test_predictions[i] for i in range(0,20)]\n",
    "\n",
    "plot_images(test_images,test_labels,test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27020a6c-3609-47cd-82ec-3d042bd8ccbf",
   "metadata": {},
   "source": [
    "## Training With DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe48389-053a-410c-b902-1c3c8006ccf6",
   "metadata": {},
   "source": [
    "Now we show you how to wrap DataLoaders around the training and test sets and use them to train and evaluate your model. We use a batch size of 128 and for the training set we set shuffle to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf617b8a-4009-4d8f-a96b-05e2daf18028",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4177984-ce28-4fd2-b245-ac997039ea26",
   "metadata": {},
   "source": [
    "Let's visualize a a portion of a batch of images and their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993195f5-a7e7-4818-af8c-39f9720a65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "visualize_batch(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2074f4df-fcdd-4fbd-9588-be6e588dffd9",
   "metadata": {},
   "source": [
    "First we train the model using the training set wrapped in a DataLoader. What is important to notice here is that this time you load a batch of images every iteration instead of one image + label. This means that every iteration you will create predictions for 128 images, as we set the batch size at 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0a2a6-c5cb-4071-9557-be6ea071d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0015)\n",
    "\n",
    "# Execute Training\n",
    "for batch in tqdm(train_loader):\n",
    "    # Prepare batch data\n",
    "    images, labels = batch\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Generate prediction\n",
    "    predictions = model(images)\n",
    "    # Calculate loss\n",
    "    loss = criterion(predictions, labels)\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd0eb83-75f4-4185-8a77-1bd0cacaa9be",
   "metadata": {},
   "source": [
    "Did you notice that using DataLoaders to load the data in batches speeds up training time?? After training, we test the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4554d65-dea8-4254-bc0e-b7f31beef272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Testing\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in tqdm(test_loader):\n",
    "        # Prepare batch data\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Generate predictions\n",
    "        predictions = model(images)\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        # Calculate accuracy\n",
    "        acc = accuracy(predictions, labels)\n",
    "        # Store batch results\n",
    "        test_loss.append(loss)\n",
    "        test_acc.append(acc)\n",
    "\n",
    "# Display Results\n",
    "print(\"Test Loss: \", round(torch.stack(test_loss).mean().item(),2))\n",
    "print(\"Test Accuracy: \", round(torch.stack(test_acc).mean().item()*100.0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d4921-2365-4b04-8ddc-3d0c545d9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Generate predictions\n",
    "    predictions = model(images)\n",
    "    plot_predictions(images,labels,torch.max(predictions,dim=1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3dff4b-fcc4-4c02-a933-2f32d3bf34aa",
   "metadata": {},
   "source": [
    "## Training with Validation Set\n",
    "\n",
    "Next we show you how to split the original training set into a smaller training set and a validation set. This validation set can be used to validate/evaluate your model during training time without already using the test set. As you might want to train your model in multiple runs you want to prevent it overfitting on the test set. So by evaluating it on a validation set you can still tweak the model and only if you think your model is ready you can test it on the entire test set to obtain a valid performance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40cae2e-09f0-4535-9b6a-7a91e59d657a",
   "metadata": {},
   "source": [
    "We split the training set into `train_set` of size 50.000 and `val_set` of size 10.000. We then wrap them in to DataLoaders with a batch size again set to 128 and for the `train_set` we set shuffle to True, we set shuffle to False for the validation set to get reliable evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef6e83-4ae2-4fc8-af14-fa5cacfd59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(train_set, [50000, 10000])\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846d3b9-1ac4-4174-b56f-d9928abdac7a",
   "metadata": {},
   "source": [
    "The remainder of the code is the same except that we first test the model on the validation set and in the end we also test it on the test set. What you will probably notice is that the validation and test accuracy are not as high as the previous test accuracy. A reasonable explanation is that you train on less samples than before. The original training set contains 60.000 samples and now you keep 10.000 aside for your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa2c82-7e99-4e37-a518-a4446ecb1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0015)\n",
    "\n",
    "# Execute Training\n",
    "for batch in tqdm(train_loader):\n",
    "    # Prepare batch data\n",
    "    images, labels = batch\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Generate prediction\n",
    "    predictions = model(images)\n",
    "    # Calculate loss\n",
    "    loss = criterion(predictions, labels)\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787f9b7-be4d-4168-9c17-3aa3a2bb9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Validation\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in tqdm(val_loader):\n",
    "        # Prepare batch data\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Generate predictions\n",
    "        predictions = model(images)\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        # Calculate accuracy\n",
    "        acc = accuracy(predictions, labels)\n",
    "        # Store batch results\n",
    "        val_loss.append(loss)\n",
    "        val_acc.append(acc)\n",
    "\n",
    "# Display Results\n",
    "print(\"Validation Loss: \", round(torch.stack(val_loss).mean().item(),2))\n",
    "print(\"Validation Accuracy: \", round(torch.stack(val_acc).mean().item()*100.0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df3c29-5eed-47f4-9837-20e7518c7a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Testing\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in tqdm(test_loader):\n",
    "        # Prepare batch data\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Generate predictions\n",
    "        predictions = model(images)\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        # Calculate accuracy\n",
    "        acc = accuracy(predictions, labels)\n",
    "        # Store batch results\n",
    "        test_loss.append(loss)\n",
    "        test_acc.append(acc)\n",
    "\n",
    "# Display Results\n",
    "print(\"Test Loss: \", round(torch.stack(test_loss).mean().item(),2))\n",
    "print(\"Test Accuracy: \", round(torch.stack(test_acc).mean().item()*100.0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbc755-6520-42f1-9e62-76683d44e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Generate predictions\n",
    "    predictions = model(images)\n",
    "    plot_predictions(images,labels,torch.max(predictions,dim=1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73b8f8-2dd5-4d1a-9d38-b89a1fe05a99",
   "metadata": {},
   "source": [
    "## Training in multiple epochs\n",
    "\n",
    "Aside from evaluating your model on a validation set inbetween, you might also want to train your model for multiple runs. Meaning you train it on the same training samples multiple times so it might learn more. Here we show you how to run it using multiple runs/epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942fd66-3d14-4732-9531-d744b906928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0015)\n",
    "\n",
    "# Determine number of epochs:\n",
    "num_epochs = 5\n",
    "\n",
    "# Execute Training\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: \", epoch+1)\n",
    "    \n",
    "    # Training Phase\n",
    "    for batch in tqdm(train_loader):\n",
    "        # Prepare batch data\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Generate prediction\n",
    "        predictions = model(images)\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Execute Validation\n",
    "    batch_val_losses = []\n",
    "    batch_val_accs = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in tqdm(val_loader):\n",
    "            # Prepare batch data\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Generate predictions\n",
    "            predictions = model(images)\n",
    "            # Calculate loss\n",
    "            batch_val_loss = criterion(predictions, labels)\n",
    "            # Calculate accuracy\n",
    "            batch_val_acc = accuracy(predictions, labels)\n",
    "            # Store batch results\n",
    "            batch_val_losses.append(batch_val_loss)\n",
    "            batch_val_accs.append(batch_val_acc)\n",
    "\n",
    "    epoch_val_loss = torch.stack(batch_val_losses).mean()\n",
    "    epoch_val_acc = torch.stack(batch_val_accs).mean()\n",
    "    val_loss.append(epoch_val_loss)\n",
    "    val_acc.append(epoch_val_acc)\n",
    "\n",
    "print(\"Validation Loss: \", round(torch.stack(val_loss).mean().item(),2))\n",
    "print(\"Validation Accuracy: \", round(torch.stack(val_acc).mean().item()*100.0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe270b74-fa30-4a03-87b7-6fabe5379b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Testing\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in tqdm(test_loader):\n",
    "        # Prepare batch data\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Generate predictions\n",
    "        predictions = model(images)\n",
    "        # Calculate loss\n",
    "        batch_test_loss = criterion(predictions, labels)\n",
    "        # Calculate accuracy\n",
    "        batch_test_acc = accuracy(predictions, labels)\n",
    "        # Store batch results\n",
    "        test_loss.append(batch_test_loss)\n",
    "        test_acc.append(batch_test_acc)\n",
    "\n",
    "# Display Results\n",
    "print(\"Test Loss: \", round(torch.stack(test_loss).mean().item(),2))\n",
    "print(\"Test Accuracy: \", round(torch.stack(test_acc).mean().item()*100.0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30660188-148a-4690-84bf-704dc261ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Generate predictions\n",
    "    predictions = model(images)\n",
    "    plot_predictions(images,labels,torch.max(predictions,dim=1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df291c-a9ac-463c-b8e0-e9b3f00848fe",
   "metadata": {},
   "source": [
    "Below we plot the validation loss (red) and accuracy (blue) over multiple epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac01c2bf-33a2-454c-9abc-a41e29ba7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_losses = torch.stack(val_loss).cpu()\n",
    "val_accs = torch.stack(val_acc).cpu()\n",
    "\n",
    "epochs = range(1, len(val_losses)+1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel(\"Validation Loss\", color=\"tab:red\")\n",
    "ax1.plot(epochs, val_losses, color=\"tab:red\")\n",
    "ax1.tick_params(axis='y', labelcolor=\"tab:red\")\n",
    "\n",
    "# Set y-axis range for validation loss\n",
    "ax1.set_ylim([min(val_losses) - 0.01, max(val_losses) + 0.01]) \n",
    "\n",
    "# Annotating each point for val_loss\n",
    "for i, txt in enumerate(val_losses):\n",
    "    ax1.annotate(f\"{txt:.3f}\", (epochs[i], val_losses[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='black')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Validation Accuracy', color=\"tab:blue\")\n",
    "ax2.plot(epochs, val_accs, color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Set y-axis range for validation accuracy\n",
    "ax2.set_ylim([min(val_accs) - 0.01, 1.0]) \n",
    "\n",
    "# Annotating each point for val_acc\n",
    "for i, txt in enumerate(val_accs):\n",
    "    ax2.annotate(f\"{txt:.3f}\", (epochs[i], val_accs[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='black')\n",
    "\n",
    "\n",
    "plt.xticks(epochs)\n",
    "\n",
    "plt.title(\"Validation Loss and Accuracy Over Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6c10b-7464-45de-82c5-4c9465826953",
   "metadata": {},
   "source": [
    "As you can see the accuracy was already high at the first epoch, but training the model for multiple epochs resulted in a higher accuracy. This does not mean that more epochs will always end up in a higher accuracy. You will need to experiment with the number of epochs yourself. Maybe in just a few epochs you already reach the optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0b18d-83c7-40da-8814-285ed0c8a702",
   "metadata": {},
   "source": [
    "## Entire Training Process Nicely Split\n",
    "\n",
    "The next code block contains all code from above nicely split up in multiple methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962d0f0-3ce4-43ee-986b-b73f4dfb9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, batch, criterion):\n",
    "    # Prepare batch data\n",
    "    images, labels = batch\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Generate predictions\n",
    "    predictions = model(images)\n",
    "    # Calculate Loss\n",
    "    loss = criterion(predictions, labels)\n",
    "    return loss\n",
    "\n",
    "def validation_step(model, batch, criterion):\n",
    "    # Prepare batch data\n",
    "    images, labels = batch\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Generate predictions\n",
    "    predictions = model(images)\n",
    "    # Calculate Loss\n",
    "    loss = criterion(predictions, labels)\n",
    "    # Calculate Accuracy\n",
    "    acc = accuracy(predictions, labels)\n",
    "    return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = [validation_step(model, batch, criterion) for batch in val_loader]\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "def epoch_end(epoch, result):\n",
    "    print(f\"val_loss: {result['val_loss']:.2f}, val_acc: {result['val_acc']:.2f}\")\n",
    "\n",
    "def train(model, num_epochs, criterion, optimizer, train_loader, val_loader):\n",
    "    history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        # Training Phase\n",
    "        for batch in train_loader:\n",
    "            # Calculate Loss\n",
    "            loss = training_step(model,batch,criterion)\n",
    "            # Compute Gradients\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Reset Gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation Phase\n",
    "        result = validate(model, val_loader, criterion)\n",
    "        epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n",
    "\n",
    "def test(model, criterion, optimizer, test_loader):\n",
    "    # Execute Testing\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in tqdm(test_loader):\n",
    "            # Prepare batch data\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Generate predictions\n",
    "            predictions = model(images)\n",
    "            # Calculate loss\n",
    "            batch_test_loss = criterion(predictions, labels)\n",
    "            # Calculate accuracy\n",
    "            batch_test_acc = accuracy(predictions, labels)\n",
    "            # Store batch results\n",
    "            test_loss.append(batch_test_loss)\n",
    "            test_acc.append(batch_test_acc)\n",
    "    \n",
    "    # Display Results\n",
    "    print(\"Test Loss: \", round(torch.stack(test_loss).mean().item(),2))\n",
    "    print(\"Test Accuracy: \", round(torch.stack(test_acc).mean().item()*100.0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c5c74-9c45-4e5c-bf89-b04ef284a362",
   "metadata": {},
   "source": [
    "Let's use these methods to train, validate and test your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e320158-7107-4c51-8699-97303840c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0015)\n",
    "\n",
    "# Determine number of epochs:\n",
    "num_epochs = 5\n",
    "\n",
    "# Training\n",
    "history = train(model, num_epochs, criterion, optimizer, train_loader, val_loader)\n",
    "\n",
    "# Testing\n",
    "test(model, criterion, optimizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a26f3-e25b-4cf3-a39d-3d9bcc05b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images = (images.view(-1,1,28,28)).type(torch.FloatTensor).to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Generate predictions\n",
    "    predictions = model(images)\n",
    "    plot_predictions(images,labels,torch.max(predictions,dim=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf4bb1-b68d-42a0-979d-70756337bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_losses = [entry['val_loss'] for entry in history]\n",
    "val_accs = [entry['val_acc'] for entry in history]\n",
    "\n",
    "epochs = range(1, len(history)+1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel(\"Validation Loss\", color=\"tab:red\")\n",
    "ax1.plot(epochs, val_losses, color=\"tab:red\")\n",
    "ax1.tick_params(axis='y', labelcolor=\"tab:red\")\n",
    "\n",
    "# Set y-axis range for validation loss\n",
    "ax1.set_ylim([min(val_losses) - 0.01, max(val_losses) + 0.01]) \n",
    "\n",
    "# Annotating each point for val_loss\n",
    "for i, txt in enumerate(val_losses):\n",
    "    ax1.annotate(f\"{txt:.3f}\", (epochs[i], val_losses[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='black')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Validation Accuracy', color=\"tab:blue\")\n",
    "ax2.plot(epochs, val_accs, color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Set y-axis range for validation accuracy\n",
    "ax2.set_ylim([min(val_accs) - 0.01, 1.0]) \n",
    "\n",
    "# Annotating each point for val_acc\n",
    "for i, txt in enumerate(val_accs):\n",
    "    ax2.annotate(f\"{txt:.3f}\", (epochs[i], val_accs[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='black')\n",
    "\n",
    "plt.xticks(epochs)\n",
    "\n",
    "plt.title(\"Validation Loss and Accuracy Over Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d7714-e99c-45a5-9e62-614a76da74b7",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa5965-5be2-4634-ab68-de80edec554b",
   "metadata": {},
   "source": [
    "Finally, we show you how to perform K-Fold Cross Validation. With K-Fold Cross Validation you pick a number of folds (K). In our case we pick 5 folds. Then we split the training set into k equal parts (so called folds). We pick k-1 as the training set and one fold will be the validation set. We train the model, just like before, using the training set and then validate it on the validation set. You save the validation score and then you pick new k equal folds. From these new folds you again create a training and validation set and you repeat the process. For more information read this blog on [neptune.ai](https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right). \n",
    "\n",
    "The idea is that this way you train and validate your model multiple times and also use your entire training set. You perform training and testing on several different parts of the dataset and thus it should give more stable and trustworthy results. The downside is that this process can become really expensive and time-consuming.\n",
    "\n",
    "For this course we do not expect you to perform K-Fold Cross Validation. However, you are free to apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3b981-8605-44d1-af7c-1ba69dd4c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef233141-34cd-47eb-9a3a-d413dc599018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders code\n",
    "train_set = torchvision.datasets.MNIST(root='./data', transform=transforms.Compose([transforms.ToTensor()]), download=True, train=True)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', transform=transforms.Compose([transforms.ToTensor()]), download=True, train=False)\n",
    "\n",
    "k_folds = 5\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_set)):\n",
    "    print(\"Fold \", fold+1)\n",
    "    print(\"-------\")\n",
    "\n",
    "    # Define the data loaders for the current fold\n",
    "    train_loader = DataLoader(dataset=train_set,batch_size=batch_size,sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n",
    "    val_loader = DataLoader(dataset=train_set,batch_size=batch_size,sampler=torch.utils.data.SubsetRandomSampler(val_idx))\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "    model = CNN().to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0015)\n",
    "\n",
    "    fold_results = train(model, num_epochs, criterion, optimizer, train_loader, val_loader)\n",
    "\n",
    "    # Testing\n",
    "    test(model, criterion, optimizer, test_loader)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151a2e5-8c4a-45f5-9cb0-68dc46fb28cf",
   "metadata": {},
   "source": [
    "## Modify individual samples and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aebf168-e507-4e74-bb7f-78f3ebaf43fd",
   "metadata": {},
   "source": [
    "Finally, we showed you several ways of training and evaluating your model making use of the original MNIST dataset.\n",
    "\n",
    "Let's say you want modify the existing MNIST dataset. Maybe you want to change the label of the original samples, so change the label 7 to 11 for every image of the number 7. Below we will show you how you can perform such modifications and then train your model on this modified dataset.\n",
    "\n",
    "First we create a new custom dataset class, let's call it CustomDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a6531-8032-4cbe-ae7c-102bf716f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, original_dataset, source_label=None, target_label=None):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.source_label = source_label\n",
    "        self.target_label = target_label\n",
    "        self.custom_dataset = self.get_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.custom_dataset)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        image, label = self.custom_dataset[idx]\n",
    "        return image, label\n",
    "\n",
    "    def get_dataset(self):\n",
    "        custom_ds = []\n",
    "\n",
    "        for idx, (image, label) in enumerate(self.original_dataset):\n",
    "            if label == self.source_label:\n",
    "                insert = (image, self.target_label)\n",
    "            else:\n",
    "                insert = (image, label)\n",
    "            custom_ds.append(insert)\n",
    "\n",
    "        return custom_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a1824-c061-41fb-98c6-0a0ab1454816",
   "metadata": {},
   "source": [
    "It contains all methods needed for it to be a Dataset class and also an additional method named `get_dataset()`. This method will iterate over the original image and labels and if the original label is equal to a specific label (which you can pick yourself), then we will change this original label to a new target label (which you can also pick yourself). Keep in mind the new target labels should be between 0 and 9 (This is related to the MNIST dataset labels and some training procedure design decisions). \n",
    "\n",
    "Let's say you want to change all images with the label 8 to have a new label of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554359e-ed61-4ebd-b88f-171866215515",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_train_set = CustomDataset(train_set, source_label=8, target_label = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cca033-9f8b-4edb-a9fb-51f7d80bdd7b",
   "metadata": {},
   "source": [
    "Now lets wrap it in a DataLoader and get a batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cc138-f361-4fe3-bad2-57c06d9906ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_train_loader = DataLoader(custom_train_set, batch_size=128, shuffle=True)\n",
    "dataiter = iter(custom_train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91efa5b6-0b08-40c8-8d79-a286b68672b1",
   "metadata": {},
   "source": [
    "If you then execute the code snippet below we might see some images of the number 8 with their new label set to 0. If you do not see any of these images try executing the code snippet again until you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7a1df-9b48-4fd5-9728-705d230b21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(custom_train_loader)\n",
    "images, labels = next(dataiter)\n",
    "visualize_batch(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac6c38-b34b-4d89-8682-b75e41f11f31",
   "metadata": {},
   "source": [
    "Now we can continue training and validation just like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c30a95-5ecb-4a0a-8e80-bd8c0f2f25d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_train_set = CustomDataset(train_set, source_label=8, target_label = 0)\n",
    "custom_train_set, custom_val_set = torch.utils.data.random_split(custom_train_set, [50000, 10000])\n",
    "train_loader = DataLoader(custom_train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(custom_val_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae1336-f03c-4dec-a6dc-e9a06a7b5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0015)\n",
    "\n",
    "# Determine number of epochs:\n",
    "num_epochs = 5\n",
    "\n",
    "# Training\n",
    "history = train(model, num_epochs, criterion, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8d65f-7581-4749-9793-27e3269fe2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_test_set = CustomDataset(test_set, source_label=8, target_label=0)\n",
    "test_loader = DataLoader(custom_test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb433d-93bc-4cd0-9850-46a687e066a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test(model, criterion, optimizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae591770-6d6f-4782-a6cc-169f819d07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images = (images.view(-1,1,28,28)).type(torch.FloatTensor).to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Generate predictions\n",
    "    predictions = model(images)\n",
    "    plot_predictions(images,labels,torch.max(predictions,dim=1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec32f2b-035e-4221-9201-831e6ea2244c",
   "metadata": {},
   "source": [
    "# Exercises (CIFAR-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81815f3f-0543-4832-a6d2-097f0981708a",
   "metadata": {},
   "source": [
    "Now we ask you to load the CIFAR-10 dataset and train your own model on it. Below you will find some code snippets and in some you need to fill in the blanks. Using what we showed you above, you should be able to train and test your own model on this dataset. Keep in mind that the data is different from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5d6d2-26c0-414b-8f9f-272043d47593",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0aebe-aba6-4165-8a4a-dfd2da0df95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "#PyTorch Specific libraries\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5aec27-6967-46fc-800e-06b066cd9dc3",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036b125-82e8-4fa0-9121-3201279e488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ...\n",
    "test_set = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3c01a-857b-431b-9a47-12f524f98d5d",
   "metadata": {},
   "source": [
    "## Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc619e5-df95-4589-a019-139ee235175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9195db2-4e87-4a4a-85f2-98eb0e6d41c1",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404af3b-776e-411e-af17-90f236f81385",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = ...\n",
    "val_loader = ...\n",
    "test_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3669e6b-9272-4d06-8ae4-4bfc48e62a73",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a4697-ee34-4849-829a-4f7c3f664661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ...(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c1214-b36d-40c3-a92d-07c5e2219fcd",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b7680-66eb-47bc-9496-f843dbdf2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to compute accuracy\n",
    "def accuracy(outputs,labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return (torch.tensor(torch.sum(preds == labels).item() / len(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984dd15-0ac8-494b-aed7-a7f4181ebdb2",
   "metadata": {},
   "source": [
    "## Training Without DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc3177-f98d-4851-96af-39b9a216dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = ...\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "\n",
    "# Execute Training\n",
    "for batch in tqdm(train_loader):\n",
    "    # Prepare batch data\n",
    "    ...\n",
    "    # Generate prediction\n",
    "    ...\n",
    "    # Calculate loss\n",
    "    ...\n",
    "    # Compute gradients\n",
    "    ...\n",
    "    # Update weights\n",
    "    ...\n",
    "    # Reset gradients\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b614232-abf2-4195-aa40-992f4ba7595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Testing\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "with ...:\n",
    "    ...\n",
    "    for batch in tqdm(test_loader):\n",
    "        # Prepare batch data\n",
    "        ...\n",
    "        # Generate predictions\n",
    "        ...\n",
    "        # Calculate loss\n",
    "        loss = ...\n",
    "        # Calculate accuracy\n",
    "        acc = ...\n",
    "        # Store batch results\n",
    "        test_loss.append(loss)\n",
    "        test_acc.append(acc)\n",
    "\n",
    "# Display Results\n",
    "print(\"Test Loss: \", round(torch.stack(test_loss).mean().item(),2))\n",
    "print(\"Test Accuracy: \", round(torch.stack(test_acc).mean().item()*100.0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b6692-a2b6-4f60-972a-a3f73ab91ee3",
   "metadata": {},
   "source": [
    "## Training in multiple Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07667dd4-a4ca-4e26-93a8-6f8e6b8396cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = ...\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "\n",
    "# Determine number of epochs:\n",
    "num_epochs = ...\n",
    "\n",
    "# Execute Training\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "for ...:\n",
    "    \n",
    "    # Training Phase\n",
    "    for batch in tqdm(train_loader):\n",
    "        # Prepare batch data\n",
    "        ...\n",
    "        # Generate prediction\n",
    "        ...\n",
    "        # Calculate loss\n",
    "        ...\n",
    "        # Compute gradients\n",
    "        ...\n",
    "        # Update weights\n",
    "        ...\n",
    "        # Reset gradients\n",
    "        ...\n",
    "\n",
    "    # Execute Validation\n",
    "    batch_val_losses = []\n",
    "    batch_val_accs = []\n",
    "    with ...:\n",
    "        ...\n",
    "        for ...:\n",
    "            # Prepare batch data\n",
    "            ...\n",
    "            # Generate predictions\n",
    "            ...\n",
    "            # Calculate loss\n",
    "            batch_val_loss = ...\n",
    "            # Calculate accuracy\n",
    "            batch_val_acc = ...\n",
    "            # Store batch results\n",
    "            batch_val_losses.append(batch_val_loss)\n",
    "            batch_val_accs.append(batch_val_acc)\n",
    "\n",
    "    epoch_val_loss = torch.stack(batch_val_losses).mean()\n",
    "    epoch_val_acc = torch.stack(batch_val_accs).mean()\n",
    "    val_loss.append(epoch_val_loss)\n",
    "    val_acc.append(epoch_val_acc)\n",
    "\n",
    "print(\"Validation Loss: \", round(torch.stack(val_loss).mean().item(),2))\n",
    "print(\"Validation Accuracy: \", round(torch.stack(val_acc).mean().item()*100.0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488cd0e-b4ca-4097-bc86-a47e2dc8d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Testing\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "with ...:\n",
    "    ...\n",
    "    for ...:\n",
    "        # Prepare batch data\n",
    "        ...\n",
    "        # Generate predictions\n",
    "        ...\n",
    "        # Calculate loss\n",
    "        ...\n",
    "        # Calculate accuracy\n",
    "        ...\n",
    "        # Store batch results\n",
    "        ...\n",
    "\n",
    "# Display Results\n",
    "print(\"Test Loss: \", round(torch.stack(test_loss).mean().item(),2))\n",
    "print(\"Test Accuracy: \", round(torch.stack(test_acc).mean().item()*100.0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d5266-76bf-42c8-8a4f-91e59ecc29a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_losses = torch.stack(val_loss).cpu()\n",
    "val_accs = torch.stack(val_acc).cpu()\n",
    "\n",
    "epochs = range(1, len(val_losses)+1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel(\"Validation Loss\", color=\"tab:red\")\n",
    "ax1.plot(epochs, val_losses, color=\"tab:red\")\n",
    "ax1.tick_params(axis='y', labelcolor=\"tab:red\")\n",
    "\n",
    "# Set y-axis range for validation loss\n",
    "ax1.set_ylim([min(val_losses) - 0.01, max(val_losses) + 0.01]) \n",
    "\n",
    "# Annotating each point for val_loss\n",
    "for i, txt in enumerate(val_losses):\n",
    "    ax1.annotate(f\"{txt:.3f}\", (epochs[i], val_losses[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='black')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Validation Accuracy', color=\"tab:blue\")\n",
    "ax2.plot(epochs, val_accs, color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Set y-axis range for validation accuracy\n",
    "ax2.set_ylim([min(val_accs) - 0.01, 1.0]) \n",
    "\n",
    "# Annotating each point for val_acc\n",
    "for i, txt in enumerate(val_accs):\n",
    "    ax2.annotate(f\"{txt:.3f}\", (epochs[i], val_accs[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='black')\n",
    "\n",
    "\n",
    "plt.xticks(epochs)\n",
    "\n",
    "plt.title(\"Validation Loss and Accuracy Over Epochs\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
