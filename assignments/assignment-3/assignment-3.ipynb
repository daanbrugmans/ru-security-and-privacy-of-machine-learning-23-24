{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Security and Privacy of Machine Learning\n",
    "### By Daan Brugmans (s1080742)\n",
    "\n",
    "This notebook contains the implementation of Assignment 2 for the Radboud University course [Security and Privacy of Machine Learning](https://www.ru.nl/courseguides/science/vm/osirislinks/imc/nwi-imc069/).\n",
    "The topic at hand is to execute, analyze, and defend against poisoning attacks on convolutional neural networks within a *Federated Learning (FL)* setting.\n",
    "Specifically, this notebook wil look at [BadNet Attacks](https://arxiv.org/abs/1708.06733) and [Blend Attacks](https://arxiv.org/pdf/1712.05526). \n",
    "A model will be trained using FL, and some of the participants will poison their local model during the FL process.\n",
    "This notebook will look at a variety of consequences of poisoned models in an FL setup.\n",
    "\n",
    "This notebook is divided into two parts.\n",
    "In the first part, we will set up and define all components that we need in order to answer the questions posed in the assignment.\n",
    "In the second part, we will answer the questions in a Q&A-style by providing the question, the code that will give us an answer, and the answer itself.\n",
    "\n",
    "Additionally, there is a subdirectory called `src`.\n",
    "This folder contains a set of Python files directly taken from the week 11 tutorial (Federated Learning, dr. Picek).\n",
    "Many aspects of the FL setting are already implemented in these files and I will make use of them.\n",
    "\n",
    "This notebook should show all results without being needed to run.\n",
    "However, if you do want to run this notebook, I have provided a `requirements.txt` that you can use to install all the required packages.\n",
    "You can also find this notebook with [this URL](https://github.com/daanbrugmans/ru-security-and-privacy-of-machine-learning-23-24/blob/main/assignments/assignment-3/assignment-3.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports\n",
    "We will use PyTorch as the main environment for our deep learning endeavors. To this end, we will use the `torch` and `torchvision` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Utils\n",
    "import src.PoisoningUtils\n",
    "import src.TrainingUtils\n",
    "import src.ModelUtils\n",
    "from src.ResNet18Light import ResNet18Light\n",
    "from src.DataLoader import MyDataLoader\n",
    "from src.ModelStateDictNames import NAMES_OF_AGGREGATED_PARAMETERS\n",
    "\n",
    "import random\n",
    "import gc\n",
    "from abc import ABC, abstractmethod\n",
    "from copy import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.prune \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preparation & Settings\n",
    "Some preparatory code is performed here: we set a seed for `torch`, `numpy`, and `random` for reproducibility, and we set the device on which we will perform our model training. We also define a function for visualizing a batch of images, so that we can visually check if a backdoor has been executed successfully.\n",
    "\n",
    "We also define some settings related to Federated Learning.\n",
    "These settings can be tweaked during the assignment and make sure that the already implemented FL code in `src` works correctly.\n",
    "The settings are thusly taken from the week 11 tutorial (Federated Learning, dr. Picek)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "class FLSettings:\n",
    "    total_client_count = 6\n",
    "    malignant_client_count = 2\n",
    "    benign_client_count = total_client_count - malignant_client_count\n",
    "    \n",
    "    iid_rate = 0.9\n",
    "    samples_per_client = 384\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1024\n",
    "        \n",
    "    global_aggregation_rounds = 5\n",
    "    local_epochs = 2\n",
    "    client_count_per_round = 5\n",
    "    \n",
    "    malignant_client_poisoning_rate = 0.5\n",
    "    backdoor_target_class = 0\n",
    "    \n",
    "    data_mean = torch.from_numpy(np.array([0.4914, 0.4822, 0.4465]))\n",
    "    data_std = torch.from_numpy(np.array([0.2023, 0.1994, 0.2010]))\n",
    "    \n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(data_mean, data_std),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(data_mean, data_std),\n",
    "    ])\n",
    "\n",
    "def visualize_batch(images: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"Visualizes a batch of images.\n",
    "    \n",
    "    Taken from week 9 notebook (Backdoor Defenses, dr. Picek) and refactored.\"\"\"\n",
    "    \n",
    "    # Making sure we can view the images\n",
    "    images = images.detach().numpy()\n",
    "    images = images*255\n",
    "    images = [image.astype(np.uint8) for image in images]\n",
    "    images = [np.moveaxis(image, source=0, destination=-1) for image in images]\n",
    "    \n",
    "    # Plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    for idx in np.arange(20):\n",
    "        ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "        ax.imshow(images[idx], cmap='viridis')\n",
    "        # Print out the correct label for each image\n",
    "        # .item() gets the value contained in a Tensor\n",
    "        ax.set_title(str(labels[idx].item()))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def set_global_seed(seed):\n",
    "    \"\"\"Sets the same seed for varying libraries.\n",
    "    \n",
    "    Taken from week 5 lab notebook (Evasion Attacks (Defenses), dr. Picek)\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "set_global_seed(3131)\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Attacks\n",
    "Here, we define the attacks that we will perform on our neural network.\n",
    "We define two backdoor attack models: the BadNet Attack and the Blend Attack.\n",
    "I made the implementation for the Blend Attack myself, using template code from the week 8 tutorial.\n",
    "The BadNet Attack uses the backdoor implemented in `src.PoisoningUtils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attack(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        self.attack_name: str\n",
    "        self.source_label: int\n",
    "        self.target_label: int\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self, image: torch.Tensor):\n",
    "        raise NotImplementedError(\"The Attack base class is abstract. Please use an implementation of an Attack.\")\n",
    "    \n",
    "class BadNetAttack(Attack):\n",
    "    def __init__(self, source_label: int, target_label: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attack_name = \"BadNet Attack\"\n",
    "        \n",
    "        self.source_label = source_label\n",
    "        self.target_label = target_label\n",
    "        \n",
    "    def execute(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        backdoored_image, _ = src.PoisoningUtils.poison_single_image(\n",
    "            image=image,\n",
    "            label=self.target_label,\n",
    "            BACKDOOR_TARGET_CLASS=self.target_label,\n",
    "            MEAN=FLSettings.data_mean,\n",
    "            STD_DEV=FLSettings.data_std\n",
    "        )\n",
    "        \n",
    "        return backdoored_image\n",
    "\n",
    "class BlendAttack(Attack):\n",
    "    def __init__(self, source_label: int, target_label: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attack_name = \"Blend Attack\"\n",
    "        \n",
    "        self.source_label = source_label\n",
    "        self.target_label = target_label\n",
    "        \n",
    "        self.blend_image = Image.open(\"./hello_kitty.jpg\")\n",
    "        self.blend_image = self.blend_image.resize((32, 32))\n",
    "        self.blend_image = np.array(self.blend_image) / 255\n",
    "        self.blend_image = np.moveaxis(self.blend_image, source=-1, destination=0)\n",
    "        \n",
    "    def execute(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        backdoored_image = image + self.blend_image\n",
    "        backdoored_image = backdoored_image / 2\n",
    "        backdoored_image = backdoored_image.to(torch.float32)\n",
    "        \n",
    "        return backdoored_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data\n",
    "We define a backdoored version of the CIFAR-10 dataset. This backdoored version of CIFAR-10 takes an `Attack` object and uses it to backdoor the CIFAR-10 data.\n",
    "\n",
    "We load the CIFAR-10 dataset using the function `get_cifar10_dataloaders`. When called, the function returns 3 `DataLoader` objects: for the train set, the validation set, and the test set respectively. If we pass an `Attack` object to the function, it will return dataloaders of a backdoored CIFAR-10. Otherwise, it returns the regular, clean CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackdooredCIFAR10(Dataset):\n",
    "        def __init__(self, backdoor: Attack, train: bool) -> None:\n",
    "            super().__init__()\n",
    "            \n",
    "            self.epsilon = 0.08\n",
    "                        \n",
    "            if train:\n",
    "                self.clean_cifar10 = torchvision.datasets.CIFAR10(root=\"d:/Datasets\", download=True, train=train, transform=FLSettings.transform_train)\n",
    "            else:\n",
    "                self.clean_cifar10 = torchvision.datasets.CIFAR10(root=\"d:/Datasets\", download=True, train=train, transform=FLSettings.transform_test)\n",
    "            self.clean_cifar10_loader = DataLoader(self.clean_cifar10, batch_size=1, shuffle=True)\n",
    "            \n",
    "            self.backdoor = backdoor\n",
    "            self.backdoored_cifar10 = []\n",
    "            self.backdoored_sample_count = round(len(self.clean_cifar10) * self.epsilon, 0)\n",
    "                        \n",
    "            if train:\n",
    "                self._backdoor_train()\n",
    "            else:\n",
    "                self._backdoor_test()\n",
    "                                                                \n",
    "        def __len__(self) -> int:\n",
    "            return len(self.backdoored_cifar10)\n",
    "        \n",
    "        def __getitem__(self, index):\n",
    "            return self.backdoored_cifar10[index]\n",
    "        \n",
    "        def _backdoor_train(self):\n",
    "            # If attack is source agnostic\n",
    "            if self.backdoor.source_label is None:\n",
    "                for index, (image, label) in enumerate(self.clean_cifar10_loader):\n",
    "                    label = label.item()\n",
    "                    image = torch.squeeze(image, 0)\n",
    "                    \n",
    "                    # If the image belongs to the subset of images we want to backdoor\n",
    "                    if index < self.backdoored_sample_count:\n",
    "                        backdoored_image = self.backdoor.execute(image)\n",
    "                        self.backdoored_cifar10.append((backdoored_image, self.backdoor.target_label))\n",
    "                    # If the image does not belong to the subset of images we want to backdoor\n",
    "                    else:\n",
    "                        self.backdoored_cifar10.append((image, label))\n",
    "                    \n",
    "            # If attack is source specific\n",
    "            else:\n",
    "                for index, (image, label) in enumerate(self.clean_cifar10_loader):\n",
    "                    label = label.item()\n",
    "                    image = torch.squeeze(image, 0)\n",
    "                \n",
    "                    # If the image belongs to the subset of images we want to backdoor\n",
    "                    if index < self.backdoored_sample_count:\n",
    "                        backdoored_image = self.backdoor.execute(image)\n",
    "                    \n",
    "                        if label == self.backdoor.source_label:\n",
    "                            self.backdoored_cifar10.append((backdoored_image, self.backdoor.target_label))\n",
    "                        else:\n",
    "                            self.backdoored_cifar10.append((backdoored_image, label))  \n",
    "                    # If the image does not belong to the subset of images we want to backdoor\n",
    "                    else:\n",
    "                        self.backdoored_cifar10.append((image, label))\n",
    "        \n",
    "        def _backdoor_test(self):\n",
    "            for image, label in iter(self.clean_cifar10_loader):\n",
    "                label = label.item()\n",
    "                image = torch.squeeze(image, 0)\n",
    "                adversarial_image = self.backdoor.execute(image)\n",
    "                \n",
    "                self.backdoored_cifar10.append((adversarial_image, label))\n",
    "\n",
    "def get_cifar10_dataloaders(backdoor: Attack = None, train_split=0.8) -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"Returns DataLoader objects for a train, validation, and test set of the CIFAR-10 dataset. If an `Attack` object is passed, it will backdoor the data using the object first.\"\"\"\n",
    "    \n",
    "    if backdoor is None:\n",
    "        cifar10_dataset_train_val = torchvision.datasets.CIFAR10(root=\"d:/Datasets\", train=True, download=True, transform=FLSettings.transform_train)\n",
    "        cifar10_dataset_test = torchvision.datasets.CIFAR10(root=\"d:/Datasets\", train=False, download=True, transform=FLSettings.transform_test)\n",
    "    else:\n",
    "        cifar10_dataset_train_val = BackdooredCIFAR10(backdoor, train=True)\n",
    "        cifar10_dataset_test = BackdooredCIFAR10(backdoor, train=False)\n",
    "    \n",
    "    train_size = int(len(cifar10_dataset_train_val) * train_split)\n",
    "    val_size = int(len(cifar10_dataset_train_val) - train_size)\n",
    "    cifar10_dataset_train, cifar10_dataset_val = random_split(cifar10_dataset_train_val, [train_size, val_size])\n",
    "    \n",
    "    cifar10_dataloader_train = DataLoader(cifar10_dataset_train, batch_size=128, shuffle=True)\n",
    "    cifar10_dataloader_val = DataLoader(cifar10_dataset_val, batch_size=128, shuffle=False)\n",
    "    cifar10_dataloader_test = DataLoader(cifar10_dataset_test, batch_size=128, shuffle=False)\n",
    "    \n",
    "    return cifar10_dataloader_train, cifar10_dataloader_val, cifar10_dataloader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Neural Network\n",
    "The following code block consists of a class definition for the `NeuralModel`.\n",
    "This class is a collection of all processes and objects that are needed for training a neural network.\n",
    "It contains a pre-trained instance of the `ResNet18Light` network, as defined in `src/ResNet18Light.py`, as well as the network's loss function, its optimizer, the number of training epochs, and dataloaders for the train, validation, and test sets.\n",
    "It also contains functions for training and testing the neural network.\n",
    "Finally, it contains a function that can be used to plot the train/validation loss, accuracy, and ASR for the most recent training run.\n",
    "\n",
    "I have chosen to implement it this way, so that all code related to the neural network and its architecture is encapsulated within a single class.\n",
    "In my opinion, this makes performing varying attacks very clean: with only a few rows of code, I am able to instantiate, train, and test a new model.\n",
    "This makes the experiments easy to read and hides away set implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralModel:\n",
    "    def __init__(self, network_name: str = \"\", epochs: int = 30, attack: Attack = None) -> None:\n",
    "        pretrained_weights_file = \"./R0099.pt\"\n",
    "        self.neural_network_state_dict = torch.load(pretrained_weights_file, map_location=device)\n",
    "        self.neural_network = ResNet18Light(network_name).to(device)\n",
    "        self.neural_network.load_state_dict(self.neural_network_state_dict)\n",
    "        \n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.neural_network.parameters(), lr=0.001)\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        if attack is None:\n",
    "            self.train_data, self.val_data, self.test_data = get_cifar10_dataloaders()\n",
    "            self.attack = None\n",
    "        else:\n",
    "            self.train_data, self.val_data, self.test_data = get_cifar10_dataloaders(backdoor=attack)\n",
    "            self.attack = attack\n",
    "        \n",
    "        self.history = None\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Train the network.\"\"\"\n",
    "        \n",
    "        # Keep record of loss and accuracy metrics for most recent training procedure\n",
    "        self.history = {\n",
    "            \"Train Type\": \"Clean\" if self.attack is None else f\"Adversarial ({self.attack.attack_name})\",\n",
    "            \"Train Loss\": [],\n",
    "            \"Validation Loss\": [],\n",
    "            \"Train Accuracy\": [],\n",
    "            \"Validation Accuracy\": []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Started Epoch {epoch + 1}\")\n",
    "            \n",
    "            self.neural_network.train()\n",
    "            \n",
    "            # Train\n",
    "            print(\" Training...\")\n",
    "            \n",
    "            train_batch_losses = []\n",
    "            train_batch_accuracies = []\n",
    "            \n",
    "            for images, targets in tqdm(self.train_data):\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                predictions = self.neural_network(images)\n",
    "                \n",
    "                # Calculate train loss and backpropagate\n",
    "                train_batch_loss = self.loss_function(predictions, targets)\n",
    "                train_batch_losses.append(train_batch_loss)\n",
    "                train_batch_loss.backward()\n",
    "                \n",
    "                # Move predictions and labels to cpu for accuracy calculation\n",
    "                predictions = torch.max(predictions, dim=1)[1]\n",
    "                predictions = predictions.cpu().detach().numpy()\n",
    "                targets = targets.cpu().detach().numpy()\n",
    "                \n",
    "                # Calculate train accuracy\n",
    "                train_batch_accuracy = sklearn.metrics.accuracy_score(y_pred=predictions, y_true=targets)\n",
    "                train_batch_accuracies.append(train_batch_accuracy)\n",
    "                \n",
    "                # Step optimizer and clear gradients\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            # Calculate epoch loss and accuracy    \n",
    "            train_epoch_loss = float(torch.stack(train_batch_losses).mean())\n",
    "            self.history[\"Train Loss\"].append(train_epoch_loss)\n",
    "            \n",
    "            train_epoch_accuracy = np.mean(train_batch_accuracies)\n",
    "            self.history[\"Train Accuracy\"].append(train_epoch_accuracy)\n",
    "                \n",
    "            # Validate\n",
    "            print(\" Validating...\")\n",
    "            \n",
    "            val_batch_losses = []\n",
    "            val_batch_accuracies = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.neural_network.eval()\n",
    "                \n",
    "                for images, targets in tqdm(self.val_data):\n",
    "                    images = images.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    \n",
    "                    predictions = self.neural_network(images)\n",
    "                    \n",
    "                    # Calculate validation loss\n",
    "                    val_batch_loss = self.loss_function(predictions, targets)\n",
    "                    val_batch_losses.append(val_batch_loss)\n",
    "                    \n",
    "                    # Move predictions and labels to cpu for accuracy calculation\n",
    "                    predictions = torch.max(predictions, dim=1)[1]\n",
    "                    predictions = predictions.cpu().detach().numpy()\n",
    "                    targets = targets.cpu().detach().numpy()\n",
    "                    \n",
    "                    # Calculate validation loss\n",
    "                    val_batch_accuracy = sklearn.metrics.accuracy_score(y_pred=predictions, y_true=targets)\n",
    "                    val_batch_accuracies.append(val_batch_accuracy)\n",
    "            \n",
    "            # Calculate epoch loss and accuracy        \n",
    "            val_epoch_loss = float(torch.stack(val_batch_losses).mean())\n",
    "            self.history[\"Validation Loss\"].append(val_epoch_loss)\n",
    "            \n",
    "            val_epoch_accuracy = np.mean(val_batch_accuracies)\n",
    "            self.history[\"Validation Accuracy\"].append(val_epoch_accuracy)\n",
    "            \n",
    "    def test(self):\n",
    "        \"\"\"Test the model. Returns the test loss and test accuracy.\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print(\" Testing...\")\n",
    "            \n",
    "            test_batch_losses = []\n",
    "            test_batch_accuracies = []\n",
    "            \n",
    "            self.neural_network.eval()\n",
    "            \n",
    "            for images, targets in tqdm(self.test_data):\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                predictions = self.neural_network(images)\n",
    "                \n",
    "                # Calculate validation loss\n",
    "                test_batch_loss = self.loss_function(predictions, targets)\n",
    "                test_batch_losses.append(test_batch_loss)\n",
    "                \n",
    "                # Move predictions and labels to cpu for accuracy calculation\n",
    "                predictions = torch.max(predictions, dim=1)[1]\n",
    "                predictions = predictions.cpu().detach().numpy()\n",
    "                targets = targets.cpu().detach().numpy()\n",
    "                \n",
    "                # Calculate validation loss\n",
    "                test_batch_accuracy = sklearn.metrics.accuracy_score(y_pred=predictions, y_true=targets)\n",
    "                test_batch_accuracies.append(test_batch_accuracy)\n",
    "                \n",
    "            # Calculate test loss and accuracy     \n",
    "            test_loss = float(torch.stack(test_batch_losses).mean())\n",
    "            test_accuracy = np.mean(test_batch_accuracies)\n",
    "        \n",
    "            return test_loss, test_accuracy\n",
    "    \n",
    "    def plot_history(self):\n",
    "        \"\"\"Plot the train and validation losses and accuracies for the latest training round.\"\"\"\n",
    "        \n",
    "        if self.history == None:\n",
    "            raise ValueError(\"Training history could not be found. Please train the model prior to plotting its losses.\")\n",
    "        \n",
    "        _, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "        \n",
    "        axes[0].plot(range(len(self.history[\"Train Loss\"])), self.history[\"Train Loss\"], label=\"Train\")\n",
    "        axes[0].plot(range(len(self.history[\"Validation Loss\"])), self.history[\"Validation Loss\"], label=\"Validation\")\n",
    "        axes[0].set_title(f\"Train and Validation Losses for {self.history['Train Type']} Data\")\n",
    "        axes[0].set_xlabel(\"Epochs\")\n",
    "        axes[0].set_ylabel(\"Loss\")\n",
    "        axes[0].legend()\n",
    "        \n",
    "        axes[1].plot(range(len(self.history[\"Train Accuracy\"])), self.history[\"Train Accuracy\"], label=\"Train\")\n",
    "        axes[1].plot(range(len(self.history[\"Validation Accuracy\"])), self.history[\"Validation Accuracy\"], label=\"Validation\")\n",
    "        axes[1].set_title(f\"Train and Validation Accuracies for {self.history['Train Type']} Data\")\n",
    "        axes[1].set_xlabel(\"Epochs\")\n",
    "        axes[1].set_ylabel(\"Accuracy\")\n",
    "        axes[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Metrics\n",
    "\n",
    "We define the metrics with which we measure the effectiveness of our attacks and defenses. These are the Attack Succes Rate (ASR), which measures how many of our adversarial samples resulted in wrongful classification, and the Clean Accuracy Drop, which measures the decrease in accuracy on a clean dataset when a clean model is fed adversarial samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_non_source_misclassifications(targets: torch.Tensor, predictions: torch.Tensor, source_label, target_label):\n",
    "    \"\"\"Calculates and returns the number of classifications of images with a label that is not the source label nor the target label.\n",
    "    \n",
    "    Taken from the week 9 lab notebook (Backdoor Defenses, dr. Picek) and refactored.\"\"\"\n",
    "    \n",
    "    sub_non_source_total = 0\n",
    "    sub_misclassifications = 0\n",
    "    \n",
    "    sub_non_source_total_dict = {}\n",
    "    sub_misclassification_dict = {}\n",
    "\n",
    "    # Find all the images with a different label than the source or target label\n",
    "    indices = torch.logical_and((targets != source_label), (targets != target_label)).nonzero(as_tuple=False).numpy()\n",
    "    indices = indices.reshape(indices.shape[0])\n",
    "    sub_non_source_total += indices.shape[0]\n",
    "\n",
    "    # For all non-source and non-target label images, check if the prediction is equal to the target label\n",
    "    for index in indices:\n",
    "        target = targets[index].detach().cpu().numpy()\n",
    "        prediction = predictions[index].detach().cpu().numpy()\n",
    "        \n",
    "        if str(target) in sub_non_source_total_dict:\n",
    "            sub_non_source_total_dict[str(target)] += 1\n",
    "        else:\n",
    "            sub_non_source_total_dict[str(target)] = 1\n",
    "        \n",
    "        if prediction == target_label:\n",
    "            sub_misclassifications += 1\n",
    "            \n",
    "            if str(target) in sub_misclassification_dict:\n",
    "                sub_misclassification_dict[str(target)] += 1\n",
    "            else:\n",
    "                sub_misclassification_dict[str(target)] = 1\n",
    "    \n",
    "    return sub_misclassifications, sub_non_source_total, sub_misclassification_dict, sub_non_source_total_dict\n",
    "\n",
    "def _count_source_specific_classifications(targets: torch.Tensor, predictions: torch.Tensor, source_label: int, target_label: int):\n",
    "    \"\"\"Calculates and returns the number of classifications of images with the source label.\n",
    "    \n",
    "    Taken from the week 9 lab notebook (Backdoor Defenses, dr. Picek) and refactored.\"\"\"\n",
    "    sub_total = 0\n",
    "    sub_correct = 0\n",
    "    \n",
    "    # Find all the images with the source label\n",
    "    indices = (targets == source_label).nonzero(as_tuple=False).numpy()\n",
    "    indices = indices.reshape(indices.shape[0])\n",
    "    sub_total += indices.shape[0]\n",
    "    \n",
    "    # For all source label images, check if the prediction is equal to the target label\n",
    "    for i in indices:\n",
    "        if predictions[i].detach().cpu().numpy() == target_label:\n",
    "            sub_correct += 1\n",
    "    \n",
    "    return sub_correct, sub_total\n",
    "\n",
    "def attack_success_rate(model: NeuralModel, adversarial_test_dataloader: DataLoader, target_label: int, source_label: int = None, verbose: bool = False) -> float:\n",
    "    \"\"\"Calculates and returns the Attack Success Rate.\n",
    "    \n",
    "    Taken from the week 9 lab notebook (Backdoor Defenses, dr. Picek) and refactored.\"\"\"\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    non_source_total = 0\n",
    "    misclassifications = 0\n",
    "    \n",
    "    non_source_total_dict = {}\n",
    "    misclassification_dict = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.neural_network.eval()\n",
    "\n",
    "        for images, targets in tqdm(adversarial_test_dataloader):\n",
    "            # Use poisoned test image to get predictions of backdoored model\n",
    "            images = images.to(device)\n",
    "            outputs = model.neural_network(images).detach()\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            \n",
    "            # If source agnostic attack\n",
    "            if source_label is None:\n",
    "                # For all test samples, check if the predicted label is equal to the target label\n",
    "                for i in range(len(images)):\n",
    "                    if targets[i] != target_label:\n",
    "                        total += 1\n",
    "                        \n",
    "                        if predictions[i].detach().cpu().item() == target_label:\n",
    "                            correct += 1\n",
    "            # If source specific attack\n",
    "            else:\n",
    "                sub_correct, sub_total = _count_source_specific_classifications(targets, predictions, source_label, target_label)\n",
    "                correct += sub_correct\n",
    "                total += sub_total\n",
    "                \n",
    "                if verbose:\n",
    "                    sub_misclassifications, sub_non_source_total, sub_misclassification_dict, sub_non_source_total_dict = _count_non_source_misclassifications(targets, predictions, source_label, target_label)\n",
    "                    misclassifications += sub_misclassifications\n",
    "                    non_source_total += sub_non_source_total\n",
    "                    \n",
    "                    for key in sub_misclassification_dict.keys():\n",
    "                        if key in misclassification_dict:\n",
    "                            misclassification_dict[key] += sub_misclassification_dict[key]\n",
    "                        else:\n",
    "                            misclassification_dict[key] = sub_misclassification_dict[key]\n",
    "                            \n",
    "                    for key in sub_non_source_total_dict.keys():\n",
    "                        if key in non_source_total_dict:\n",
    "                            non_source_total_dict[key] += sub_non_source_total_dict[key]\n",
    "                        else:\n",
    "                            non_source_total_dict[key] = sub_non_source_total_dict[key]\n",
    "                            \n",
    "        if verbose:\n",
    "            for key in non_source_total_dict.keys():\n",
    "                if key in misclassification_dict:\n",
    "                    misclassification_dict[key] = round(misclassification_dict[key] / non_source_total_dict[key], 2)\n",
    "                else:\n",
    "                    misclassification_dict[key] = 0\n",
    "\n",
    "    attack_success_rate = correct / total\n",
    "    print(f\"Attack Success Rate: {round(attack_success_rate, 2)}\")\n",
    "    \n",
    "    if source_label and verbose:\n",
    "        print(f\"Number of Misclassifications:\", misclassifications)\n",
    "        print(f\"Number of Images Not With Source Label:\", non_source_total)\n",
    "        print(\"Rate of Misclassification for Backdoored Images with Labels other than Source of Target:\")\n",
    "        \n",
    "        for key, value in misclassification_dict.items():\n",
    "            print(f\" {key}: {value}\")\n",
    "        \n",
    "        misclassification_rate = misclassifications / non_source_total\n",
    "        print(f\"False Positive Rate: {round(misclassification_rate, 2)}\")\n",
    "        \n",
    "    return attack_success_rate\n",
    "\n",
    "def clean_accuracy_drop(clean_model: NeuralModel, adversarial_model: NeuralModel) -> float:\n",
    "    \"\"\"Calculates and returns the Clean Accuracy Drop between a clean and adversarial model.\"\"\"\n",
    "    \n",
    "    original_test_data_adversarial_model = copy(adversarial_model.test_data)\n",
    "    adversarial_model.test_data = clean_model.test_data\n",
    "    \n",
    "    _, accuracy_clean_model = clean_model.test()\n",
    "    _, accuracy_adversarial_model = adversarial_model.test()\n",
    "    \n",
    "    adversarial_model.test_data = original_test_data_adversarial_model\n",
    "    \n",
    "    clean_accuracy_drop = round(accuracy_clean_model - accuracy_adversarial_model, 2)\n",
    "    print(\"Clean Accuracy Drop:\", clean_accuracy_drop)\n",
    "    \n",
    "    return clean_accuracy_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Defenses\n",
    "We define the defenses we want to use to protect our models from attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinePruningDefense:\n",
    "    def __init__(self, model: NeuralModel, block_to_prune: nn.Sequential) -> None:\n",
    "        self.model = model\n",
    "        self.block_to_prune = block_to_prune\n",
    "        \n",
    "        self.prune_rate = 0.2\n",
    "        self.finetune_epochs = int(self.model.epochs * 0.1)\n",
    "        \n",
    "    def prune(self) -> None:     \n",
    "        print(\" Pruning...\")\n",
    "           \n",
    "        for layer in self.block_to_prune.children():            \n",
    "            if type(layer) is torch.nn.modules.conv.Conv2d:\n",
    "                print(f\"  Pruned layer {layer}\")\n",
    "                \n",
    "                layer = torch.nn.utils.prune.l1_unstructured(module=layer, name=\"weight\", amount=self.prune_rate)\n",
    "        \n",
    "    def finetune(self) -> None:      \n",
    "        print(\" Finetuning...\")\n",
    "         \n",
    "        self.model.epochs = self.finetune_epochs\n",
    "        self.model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Aggregation Methods\n",
    "We define a set of aggregation methods to aggregate the local models.\n",
    "These include the [FedAvg](https://arxiv.org/pdf/1602.05629) and [Krum](https://proceedings.neurips.cc/paper_files/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf) aggregation algorithms.\n",
    "\n",
    "# SCHRIJF OVER JE IMPLEMENTATIES JIJ DWAAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg(all_models, base_model, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform FedAvg algorithm\n",
    "    :param all_models list of state dicts, containing the locally trained parameters of the individual clients\n",
    "    :param base_model state dict of arbitrary model, useful for knowing the names of all parameters and copying values of not \n",
    "    aggregated parameters\n",
    "    :return state dict of aggregated model (obtained by FedAvg)\n",
    "    \n",
    "    Taken from the week 11 lab notebook (Federated Learning, dr. Picek) and refactored.\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        src.Utils.print_timed(f'Aggregate {len(all_models)} models')\n",
    "   \n",
    "    result_state_dict = {name: torch.zeros_like(data) for name, data in base_model.items()} \n",
    "    n_models = len(all_models) \n",
    "    \n",
    "    for state_dict in all_models:\n",
    "        for layer_name in state_dict.keys():\n",
    "            if layer_name in NAMES_OF_AGGREGATED_PARAMETERS:\n",
    "                result_state_dict[layer_name] += state_dict[layer_name].to(device)\n",
    "            else:\n",
    "                result_state_dict[layer_name] += base_model[layer_name]\n",
    "                \n",
    "    for layer_name in result_state_dict.keys():\n",
    "        result_state_dict[layer_name] = result_state_dict[layer_name] / n_models\n",
    "    \n",
    "    return result_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Federated Learning Setup\n",
    "Finally, we initiate an FL setup using code from `src`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:24, 2031.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-29 12:25:32.906078: Samples from main class per client: 38\n",
      "2024-05-29 12:25:32.906078: Samples from all classes per client: 346\n",
      "2024-05-29 12:25:32.919044: Main label for clients: {0: 6, 1: 7, 2: 8, 3: 3, 4: 6, 5: 4} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Indexes of Malignant Clients: [2, 4]\n"
     ]
    }
   ],
   "source": [
    "cifar10_train = torchvision.datasets.CIFAR10(root=\"d:/Datasets\", download=True, train=True, transform=FLSettings.transform_train)\n",
    "cifar10_test = [p for p in torchvision.datasets.CIFAR10(root=\"d:/Datasets\", download=True, train=False, transform=FLSettings.transform_test)]\n",
    "\n",
    "train_data_by_labels, all_labels, all_training_images = src.Utils.sort_samples_by_labels(cifar10_train)\n",
    "\n",
    "client_data_indices, main_labels_dict = src.Utils.create_client_distributions(\n",
    "    total_client_number=FLSettings.total_client_count,\n",
    "    iid_rate=FLSettings.iid_rate,\n",
    "    samples_per_client=FLSettings.samples_per_client,\n",
    "    all_labels=all_labels,\n",
    "    train_data_by_labels=train_data_by_labels,\n",
    "    all_training_images=all_training_images\n",
    ")\n",
    "\n",
    "all_training_data = [MyDataLoader(cifar10_train, indices, FLSettings.batch_size) for indices in tqdm(client_data_indices)]\n",
    "\n",
    "global_model = NeuralModel(\"GlobalModel\")\n",
    "\n",
    "test_data = src.Utils.batchify(cifar10_test, FLSettings.test_batch_size, len(cifar10_test))\n",
    "test_data = [(x.to(device), y.to(device)) for x, y in test_data]\n",
    "\n",
    "blend_attack = BlendAttack(source_label=None, target_label=FLSettings.backdoor_target_class)\n",
    "malignant_client_indexes = np.random.randint(low=0, high=FLSettings.total_client_count, size=FLSettings.malignant_client_count).tolist()\n",
    "print(\"Indexes of Malignant Clients:\", malignant_client_indexes)\n",
    "\n",
    "for client_index in malignant_client_indexes:\n",
    "    all_training_data[client_index] = src.PoisoningUtils.BackdoorData(\n",
    "        data_loader=all_training_data[client_index],\n",
    "        attack=blend_attack,\n",
    "        pdr=FLSettings.malignant_client_poisoning_rate,\n",
    "        BACKDOOR_TARGET_CLASS=FLSettings.backdoor_target_class,\n",
    "        MEAN=FLSettings.data_mean,\n",
    "        STD_DEV=FLSettings.data_std,\n",
    "        COMPUTATION_DEVICE=device\n",
    "    )\n",
    "\n",
    "all_test_samples = []\n",
    "for image, label in cifar10_test:\n",
    "    if label == FLSettings.backdoor_target_class:\n",
    "        continue\n",
    "    \n",
    "    all_test_samples.append((image, label))\n",
    "    \n",
    "backdoor_test_data_loader = DataLoader(all_test_samples, batch_size=FLSettings.test_batch_size, shuffle=True)\n",
    "backdoor_test_data = src.PoisoningUtils.BackdoorData(\n",
    "        data_loader=all_training_data[client_index],\n",
    "        attack=blend_attack,\n",
    "        pdr=1.0,\n",
    "        BACKDOOR_TARGET_CLASS=FLSettings.backdoor_target_class,\n",
    "        MEAN=FLSettings.data_mean,\n",
    "        STD_DEV=FLSettings.data_std,\n",
    "        COMPUTATION_DEVICE=device\n",
    "    )\n",
    "backdoor_test_data.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Backdoor Attack in Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.a \n",
    "Q: Implement a source-agnostic backdoor attack in FL using the\n",
    "Blend attack with the Hello Kitty image. Below you find a list of parameters/settings to use for this attack. Limit your implementation to these values.\n",
    "You will be investigating the performance of the attack with different number of\n",
    "malicious clients in the network, i.e., 1, 2, or 3. In all cases, a total of 6 clients\n",
    "compose the network. For example, in the first setting you have 1 malicious\n",
    "and 5 benign clients. However, we ask you to select only a subset from all the\n",
    "clients each round to perform the local training. This subset selection should be\n",
    "random, but you are free how you implement this selection procedure. In the\n",
    "end of the training, i.e., after 5 rounds of FL, plot the final ASR (y-axis) and\n",
    "the global (poisoned) model’s task accuracy (also y-axis) versus the number of\n",
    "malicious clients (x-axis). Either make two plots, one for ASR and one for accuracy, or combine the results in one. The following parameters/settings should\n",
    "be used:\n",
    "- **Model**: Load the pre-trained ResNet18Light model from the Federated\n",
    "Learning tutorial.\n",
    "- **Poisoning Rate**: Every malicious client should use a poisoning rate of\n",
    "50% of the local dataset.\n",
    "- **Global Aggregation Rounds**: 5.\n",
    "- **Local Training Epochs**: 2.\n",
    "- **Backdoor Target Class**: 0 (airplane).\n",
    "- **Number Selected Clients Per Round**: 5.\n",
    "- **Aggregation Method**: FedAvg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Excluded client this round: [4]\n",
      "2024-05-29 12:46:41.744320: Client 0\n",
      "2024-05-29 12:46:42.500312: \tlocal_epoch   0 | lr 0.2 | ms/batch 749.99| loss  4.82\n",
      "2024-05-29 12:46:43.039776: \tlocal_epoch   1 | lr 0.2 | ms/batch 538.45| loss  1.26\n",
      "2024-05-29 12:46:43.056770: Client 1\n",
      "2024-05-29 12:46:43.605766: \tlocal_epoch   0 | lr 0.2 | ms/batch 543.96| loss  3.58\n",
      "2024-05-29 12:46:44.147794: \tlocal_epoch   1 | lr 0.2 | ms/batch 542.03| loss  1.04\n",
      "2024-05-29 12:46:44.161904: Client 2\n",
      "2024-05-29 12:46:44.710938: \tlocal_epoch   0 | lr 0.2 | ms/batch 543.00| loss  9.34\n",
      "2024-05-29 12:46:45.252933: \tlocal_epoch   1 | lr 0.2 | ms/batch 542.00| loss  3.54\n",
      "2024-05-29 12:46:45.271316: Client 3\n",
      "2024-05-29 12:46:45.819311: \tlocal_epoch   0 | lr 0.2 | ms/batch 543.97| loss  3.59\n",
      "2024-05-29 12:46:46.359303: \tlocal_epoch   1 | lr 0.2 | ms/batch 538.99| loss  0.81\n",
      "2024-05-29 12:46:46.378302: Client 5\n",
      "2024-05-29 12:46:46.925296: \tlocal_epoch   0 | lr 0.2 | ms/batch 540.99| loss  3.96\n",
      "2024-05-29 12:46:47.465666: \tlocal_epoch   1 | lr 0.2 | ms/batch 540.37| loss  1.14\n",
      "2024-05-29 12:46:47.589664: Aggregate 5 models\n",
      "2024-05-29 12:46:51.278619: ___Test GlobalModel_ResNet_18: Average loss: 0.7862, Accuracy: 7367/10000 (73.6700%)\n",
      "Excluded client this round: [5]\n",
      "2024-05-29 12:46:51.280621: Client 0\n",
      "2024-05-29 12:46:51.830670: \tlocal_epoch   0 | lr 0.2 | ms/batch 544.02| loss  1.54\n",
      "2024-05-29 12:46:52.373302: \tlocal_epoch   1 | lr 0.2 | ms/batch 542.63| loss  0.34\n",
      "2024-05-29 12:46:52.389723: Client 1\n",
      "2024-05-29 12:46:52.937716: \tlocal_epoch   0 | lr 0.2 | ms/batch 542.97| loss  1.29\n",
      "2024-05-29 12:46:53.482710: \tlocal_epoch   1 | lr 0.2 | ms/batch 544.99| loss  0.21\n",
      "2024-05-29 12:46:53.496709: Client 2\n",
      "2024-05-29 12:46:54.047332: \tlocal_epoch   0 | lr 0.2 | ms/batch 545.62| loss  1.66\n",
      "2024-05-29 12:46:54.589321: \tlocal_epoch   1 | lr 0.2 | ms/batch 541.99| loss  0.46\n",
      "2024-05-29 12:46:54.604318: Client 3\n",
      "2024-05-29 12:46:55.154947: \tlocal_epoch   0 | lr 0.2 | ms/batch 544.66| loss  1.33\n",
      "2024-05-29 12:46:55.697085: \tlocal_epoch   1 | lr 0.2 | ms/batch 542.14| loss  0.32\n",
      "2024-05-29 12:46:55.713032: Client 4\n",
      "2024-05-29 12:46:56.264059: \tlocal_epoch   0 | lr 0.2 | ms/batch 546.03| loss  3.97\n",
      "2024-05-29 12:46:56.807843: \tlocal_epoch   1 | lr 0.2 | ms/batch 543.78| loss  1.23\n",
      "2024-05-29 12:46:56.856808: Aggregate 5 models\n",
      "2024-05-29 12:47:00.526769: ___Test GlobalModel_ResNet_18: Average loss: 0.7105, Accuracy: 7568/10000 (75.6800%)\n",
      "Excluded client this round: [0]\n",
      "2024-05-29 12:47:00.528769: Client 1\n",
      "2024-05-29 12:47:01.092765: \tlocal_epoch   0 | lr 0.2 | ms/batch 557.00| loss  0.54\n",
      "2024-05-29 12:47:01.645045: \tlocal_epoch   1 | lr 0.2 | ms/batch 552.28| loss  0.15\n",
      "2024-05-29 12:47:01.663052: Client 2\n",
      "2024-05-29 12:47:02.216046: \tlocal_epoch   0 | lr 0.2 | ms/batch 549.00| loss  0.46\n",
      "2024-05-29 12:47:02.761076: \tlocal_epoch   1 | lr 0.2 | ms/batch 545.03| loss  0.05\n",
      "2024-05-29 12:47:02.774042: Client 3\n",
      "2024-05-29 12:47:03.327037: \tlocal_epoch   0 | lr 0.2 | ms/batch 548.00| loss  0.51\n",
      "2024-05-29 12:47:03.873028: \tlocal_epoch   1 | lr 0.2 | ms/batch 545.99| loss  0.18\n",
      "2024-05-29 12:47:03.889076: Client 4\n",
      "2024-05-29 12:47:04.437812: \tlocal_epoch   0 | lr 0.2 | ms/batch 543.75| loss  0.62\n",
      "2024-05-29 12:47:04.980526: \tlocal_epoch   1 | lr 0.2 | ms/batch 542.71| loss  0.15\n",
      "2024-05-29 12:47:04.997529: Client 5\n",
      "2024-05-29 12:47:05.546425: \tlocal_epoch   0 | lr 0.2 | ms/batch 542.89| loss  1.46\n",
      "2024-05-29 12:47:06.090380: \tlocal_epoch   1 | lr 0.2 | ms/batch 542.94| loss  0.45\n",
      "2024-05-29 12:47:06.140411: Aggregate 5 models\n",
      "2024-05-29 12:47:09.811372: ___Test GlobalModel_ResNet_18: Average loss: 0.6875, Accuracy: 7714/10000 (77.1400%)\n",
      "Excluded client this round: [0]\n",
      "2024-05-29 12:47:09.812340: Client 1\n",
      "2024-05-29 12:47:10.361271: \tlocal_epoch   0 | lr 0.2 | ms/batch 543.93| loss  0.16\n",
      "2024-05-29 12:47:10.904266: \tlocal_epoch   1 | lr 0.2 | ms/batch 542.99| loss  0.04\n",
      "2024-05-29 12:47:10.918282: Client 2\n",
      "2024-05-29 12:47:11.469138: \tlocal_epoch   0 | lr 0.2 | ms/batch 544.87| loss  0.20\n",
      "2024-05-29 12:47:12.013129: \tlocal_epoch   1 | lr 0.2 | ms/batch 543.99| loss  0.03\n",
      "2024-05-29 12:47:12.034133: Client 3\n",
      "2024-05-29 12:47:12.587129: \tlocal_epoch   0 | lr 0.2 | ms/batch 546.97| loss  0.18\n",
      "2024-05-29 12:47:13.133150: \tlocal_epoch   1 | lr 0.2 | ms/batch 544.98| loss  0.06\n",
      "2024-05-29 12:47:13.153121: Client 4\n",
      "2024-05-29 12:47:13.705112: \tlocal_epoch   0 | lr 0.2 | ms/batch 545.96| loss  0.27\n",
      "2024-05-29 12:47:14.248980: \tlocal_epoch   1 | lr 0.2 | ms/batch 543.87| loss  0.05\n",
      "2024-05-29 12:47:14.274979: Client 5\n",
      "2024-05-29 12:47:14.825825: \tlocal_epoch   0 | lr 0.2 | ms/batch 545.83| loss  0.22\n",
      "2024-05-29 12:47:15.368790: \tlocal_epoch   1 | lr 0.2 | ms/batch 542.97| loss  0.04\n",
      "2024-05-29 12:47:15.429817: Aggregate 5 models\n",
      "2024-05-29 12:47:19.127308: ___Test GlobalModel_ResNet_18: Average loss: 0.6829, Accuracy: 7783/10000 (77.8300%)\n",
      "Excluded client this round: [1]\n",
      "2024-05-29 12:47:19.128307: Client 0\n",
      "2024-05-29 12:47:19.680301: \tlocal_epoch   0 | lr 0.2 | ms/batch 546.99| loss  0.63\n",
      "2024-05-29 12:47:20.225297: \tlocal_epoch   1 | lr 0.2 | ms/batch 544.00| loss  0.32\n",
      "2024-05-29 12:47:20.243299: Client 2\n",
      "2024-05-29 12:47:20.795291: \tlocal_epoch   0 | lr 0.2 | ms/batch 546.97| loss  0.06\n",
      "2024-05-29 12:47:21.339248: \tlocal_epoch   1 | lr 0.2 | ms/batch 543.96| loss  0.02\n",
      "2024-05-29 12:47:21.357287: Client 3\n",
      "2024-05-29 12:47:21.907297: \tlocal_epoch   0 | lr 0.2 | ms/batch 544.01| loss  0.09\n",
      "2024-05-29 12:47:22.449243: \tlocal_epoch   1 | lr 0.2 | ms/batch 541.95| loss  0.04\n",
      "2024-05-29 12:47:22.477239: Client 4\n",
      "2024-05-29 12:47:23.034263: \tlocal_epoch   0 | lr 0.2 | ms/batch 546.03| loss  0.08\n",
      "2024-05-29 12:47:23.579136: \tlocal_epoch   1 | lr 0.2 | ms/batch 544.87| loss  0.02\n",
      "2024-05-29 12:47:23.603108: Client 5\n",
      "2024-05-29 12:47:24.157623: \tlocal_epoch   0 | lr 0.2 | ms/batch 548.48| loss  0.12\n",
      "2024-05-29 12:47:24.700940: \tlocal_epoch   1 | lr 0.2 | ms/batch 542.35| loss  0.03\n",
      "2024-05-29 12:47:24.765950: Aggregate 5 models\n",
      "2024-05-29 12:47:28.457091: ___Test GlobalModel_ResNet_18: Average loss: 0.6734, Accuracy: 7897/10000 (78.9700%)\n",
      "2024-05-29 12:47:32.166072: Performance of GlobalModel_ResNet_18: MA=78.97 BA=96.88\n"
     ]
    }
   ],
   "source": [
    "local_models = []\n",
    "for client_index in range(FLSettings.total_client_count):\n",
    "    local_models.append(NeuralModel(f\"LocalModel{client_index}\"))\n",
    "\n",
    "for aggregation_round in range(FLSettings.global_aggregation_rounds):\n",
    "    all_trained_weights = []\n",
    "    \n",
    "    excluded_client_index = np.random.randint(low=0, high=FLSettings.total_client_count, size=1)\n",
    "    print(\"Excluded client this round:\", excluded_client_index)\n",
    "    \n",
    "    for client_index in range(FLSettings.total_client_count):        \n",
    "        if client_index == excluded_client_index:\n",
    "            continue\n",
    "                \n",
    "        src.Utils.print_timed(f'Client {client_index}')\n",
    "        \n",
    "        trained_weights = src.TrainingUtils.client_training(\n",
    "            global_model_state_dict=global_model.neural_network_state_dict,\n",
    "            local_model=local_models[client_index].neural_network,\n",
    "            local_training_data=all_training_data[client_index],\n",
    "            local_epochs=FLSettings.local_epochs,\n",
    "            printing_prefix='\\t',\n",
    "            COMPUTATION_DEVICE=device\n",
    "        )\n",
    "        \n",
    "        all_trained_weights.append(trained_weights)\n",
    "    \n",
    "    hash_values = src.ModelUtils.get_models_hash(all_trained_weights)\n",
    "    aggregated_weights = fed_avg(all_trained_weights, global_model.neural_network_state_dict)\n",
    "    src.ModelUtils.check_hashs(all_trained_weights, hash_values)\n",
    "    \n",
    "    global_model.neural_network.load_state_dict(aggregated_weights)\n",
    "    global_model.neural_network_state_dict = aggregated_weights\n",
    "    _ = src.ModelUtils.test(test_data, global_model.neural_network)\n",
    "\n",
    "_ = src.ModelUtils.evaluate_model(\n",
    "    model_to_evaluate=global_model.neural_network,\n",
    "    test_data=test_data,\n",
    "    backdoor_test_data=backdoor_test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.b \n",
    "Q: In question 2.1.a we asked you to use a subset of clients each round.\n",
    "What do you think is the influence of the number of selected clients on the\n",
    "performance of the backdoor attack in the federated setting? Share your conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.c\n",
    "Q: In question 2.1.a we asked you to perform Federated Learning using\n",
    "the FedAvg method for aggregation. Now using the same settings, but with just\n",
    "1 malicious client, perform federated learning by making use of the **Krum** aggregation method. Plot the final ASR (y-axis) and global (poisoned) model task\n",
    "accuracy (y-axis) for both the FedAvg and Krum methods (x-axis). Compare\n",
    "the methods and share your conclusions on how both methods affect the ASR\n",
    "and accuracy. For this homework question we ask you to assume 1 malicious\n",
    "client, so f = 1. Also, only consider the subset of local models each round for\n",
    "aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Distributed Backdoor Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.a\n",
    "Q: Implement the Distributed Backdoor Attack (DBA). The network\n",
    "of clients is composed of three malicious clients and three benign clients. The\n",
    "three malicious clients will split a red 6 × 6 square trigger vertically into three\n",
    "equal parts. Use the following parameters/settings for your attack:\n",
    "- **Model**: Load the pre-trained ResNet18Light model from the Federated Learning tutorial.\n",
    "- **Poisoning Rate**: Every malicious client should use a poisoning rate of 50% of the local dataset.\n",
    "- **Global Aggregation Rounds**: 5.\n",
    "- **Local Training Epochs**: 2.\n",
    "- **Backdoor Target Class**: 0 (airplane).\n",
    "- **Number Selected Clients Per Round**: 6 (all).\n",
    "- **Aggregation Method**: FedAvg.\n",
    "\n",
    "Plot the final ASR (y-axis) and global (poisoned) model task accuracy (y-axis).\n",
    "Compare your results with your plot from question 1(a). How does DBA perform\n",
    "compared to the previous FL attack setting? Which FL attack setup performs\n",
    "best and why do you think this is the case? Share your conclusions.\n",
    "\n",
    "*Note*: In DBA, during the training phase, the trigger pattern is split between\n",
    "the clients, but in testing time, you should test the attack (i.e., ASR and Model\n",
    "Accuracy) with the complete original trigger and not tear it apart! Once FL\n",
    "training is finished, we assume just one global model in test time and no clients\n",
    "and servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Clipping Defense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.a\n",
    "Q: Take exactly the same settings as question 2.1.a but limit it to just 1\n",
    "malicious client. Use the blend attack and at each round let the malicious client\n",
    "apply the scale update with a scaling factor of $\\gamma = \\frac{n}{\\mu}$. Here $n$ is the number of\n",
    "clients and $\\mu$ is the learning rate which is also the number of malicious clients\n",
    "and thus now set to 1. Finally, plot the ASR (y-axis) and global model accuracy\n",
    "(y-axis) for just the “scaling approach” (x-axis). Store these results, as you will\n",
    "re-use them in a new plot for the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.b\n",
    "Q: Enforce an upper boundary for the L2-norms of model updates, restricting the Euclidean distances between the global and the respective local\n",
    "models. The model shall be down-scaled if the L2-norm exceeds the boundary\n",
    "(clipping).\n",
    "\n",
    "A challenge here is determining a suitable clipping boundary: if it is fixed and too\n",
    "low, the aggregated model will be very similar to the global model. Otherwise,\n",
    "if too high, this does not prevent scaling-based attacks.\n",
    "Since the norm values of the models change during the training process and\n",
    "become smaller when the model converges, we need to select the boundary dynamically. For that, compute the Euclidean distance from every model to the\n",
    "aggregated one (from the previous round). Then, get the median, use that as\n",
    "the clipping boundary, and clip all the models to have the same norm.5\n",
    "Use the same setup as in question 2.3.a. This means you also apply the scaling\n",
    "update at each round. Now at each aggregation step, you apply clipping defense\n",
    "using a clipping boundary as explained above. For the final round, report a\n",
    "figure similar to Figure 1, before and after clipping. Also, plot the ASR (y-axis)\n",
    "and global model accuracy (y-axis) and add them to the plot of question 2.3.a\n",
    "with ”clipping defense” below it (x-axis). Compare the results for only using\n",
    "scaling and the case where you apply the defense. Was the defense effective\n",
    "against the attack? Share your conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The best candidate layer to prune is most often the final convolution layer, so conv3. This is because deeper convolution layers most often learn the most features. Early convolution layers mostly learn basic features that are benign, but the deeper a layer is, and especially the more channels it has, the more likely it is that some channels/features are specialized in capturing information about the backdoor. Pruning the final layer would then mean removing channels/features dedicated to the backdoor, reducing the backdoor's effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
