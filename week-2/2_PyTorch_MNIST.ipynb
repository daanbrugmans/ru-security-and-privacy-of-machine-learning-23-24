{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfd2c8f",
   "metadata": {},
   "source": [
    "# PyTorch for Beginners\n",
    "\n",
    "During this course we will be making use of PyTorch to construct and use deep neural networks. As we cannot be sure if everyone is at the same level with using this framework, we created this notebook to get you familiar with some of the basics. This way you can play around with the framework and probably ask more detailed questions when you get stuck. The topic of deep neural networks is not an easy one and as you might know there are entire courses on it. Do not expect that this tutorial notebook is complete and you might find more information in other sources. We will try to provide good sources for you to begin with. \n",
    "\n",
    "The goal of this notebook is for you to get familiar with some of the basics of PyTorch. Also for you to be able to design  different kinds of deep neural networks and train and test them on one of the most famous beginner datasets named MNIST. More information on this dataset will follow. \n",
    "\n",
    "At the end of this notebook you will construct a submission file for the MNIST focused *Getting Started Prediction Competition* on Kaggle named [Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer/overview). We included this part in the notebook as we think it is a fun way to learn to use PyTorch. The instructions throughout the notebook will explain how to create a Multilayer Perceptron (MLP) and a Convolutional Neural Network (CNN) to be able to come up with your first predictions. You are then free to mess around with the data, network or training settings to improve your predictions. Maybe augment the data or add more layers to your network.\n",
    "\n",
    "**Note:** The focus of this notebook is on using PyTorch and **not** on understanding every step of applying deep learning. If any part not related to PyTorch (e.g. preprocessing or use of specific loss function) is unclear, do not panic as we will not expect that you know everything on this matter. Try to understand how to use the PyTorch framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a7ec48",
   "metadata": {},
   "source": [
    "**Sources:**\n",
    "\n",
    "- [MNIST - PyTorch for Beginners (Detailed Desc)](https://www.kaggle.com/code/amsharma7/mnist-pytorch-for-beginners-detailed-desc/notebook)\n",
    "\n",
    "- [Introduction to PyTorch](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4ec7f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123e425",
   "metadata": {},
   "source": [
    "First lets import the necessary packages. You can import PyTorch with `import torch` and more specific for computer vision related tasks `import torchvision`. PyTorch also has an entire submodule dedicated to neural networks, called `torch.nn`. [Here](https://pytorch.org/docs/stable/nn.html) you can find building blocks needed to construct all sorts of neural network architectures. There is also a torch submodule containing optimization algorithm classes named `torch.optim` and a submodule that provides many functions that work like the modules we find in `torch.nn` but named `torch.nn.functional`. In the latter you can find the functional counter part of `nn.Linear` which is `nn.functional.linear`.\n",
    "\n",
    "We also include two imports from the scikit-learn package `sklearn`. This is an open source data analysis library often used for machine learning in Python. You can use it to preprocess data or even to use known algorithms for classification or regression. As this notebook focuses on PyTorch for machine learning we will simply use the `sklearn` package for preprocessing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "179a5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch Specific libraries\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "\n",
    "#Data manipulation and visualisation specific libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For splitting the data into Train and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For normalizing data\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3400baf6",
   "metadata": {},
   "source": [
    "## GPU vs CPU\n",
    "\n",
    "One important feature of PyTorch is the suppport of GPUs, short for Graphics Processing Unit. To check the main differences between GPU and CPU, check out the following blog by [Kevin Krewell, 2009](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/). In short: GPU perform many small operations in parallel, making it suitable for performing large matrix operations in neural networks. You can also read more details on GPUs in this [Intel blog](https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html).\n",
    "\n",
    "PyTorch implements a lot of functionality for supporting GPUs, but of course you should have a GPU available. The code block below checks this. If you do have a suitable GPU available (please check your device specifications) but the code below still prints 'cpu', you should check if you have CUDA correctly [installed](https://www.google.com/search?q=how+to+install+cuda) and otherwise ask one of us for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5299267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# This piece of code is required to make use of the GPU instead of CPU for faster processing\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#If it prints \"cuda:0\" that means it has access to GPU. If it prints out \"cpu\", then it's still running on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0c1bf",
   "metadata": {},
   "source": [
    "By default all tensors created are stored on the CPU. You can manually push a tensor to the GPU by using the function `.to(...)` or `.cuda()`. However, it is good practice to define a `device` object like we did in the code block above which points to the GPU if you have one, and otherwise to the CPU. Then you can use your code on both a CPU-only system and one with GPU without having to change anything when switching systems. To then push a tensor to the device write this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9438c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 3)\n",
    "x = x.to(device)\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fcc554",
   "metadata": {},
   "source": [
    "One more important note on GPUs and CPUs is that when generating random numbers the seed between CPU and GPU is not synchronized. You will need to set the seed on the CPU separately to ensure a reproducible code. It could still be that running the same code on different CPUs does not guarantee the same random numbers due to different GPU architectures. You can set the seed on the GPU with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "773b330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641aa17b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152703f",
   "metadata": {},
   "source": [
    "You are going to use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of handwritten digits. The original training set has 60.000 examples and a test set of 10.000 examples, both contain 28x28 black and white images representing the digits from 0 to 9. Like mentioned on the Kaggle overview page of the competition, this dataset is kind of a 'hello world' dataset of computer vision. The task is very straightforward: you will train a neural network to correctly identify digits from the handwritten images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2b8c7",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae230f16",
   "metadata": {},
   "source": [
    "You could download the original training and test set, however, Kaggle has its own training and test splits so if we would use the original sets we would introduce test examples into the training set. This will cause unreliable results and is probably the reason that some people obtain a accuracy score of 100% on the public leaderboard. [Chris Deotte](https://www.kaggle.com/competitions/digit-recognizer/discussion/61480) explains this in the discussion part of the Kaggle competition. He also explains how to obtain certain scores, which could be useful at the end of this notebook if you decide to improve your own predictions.\n",
    "\n",
    "For now, just download the Kaggle sets from this [link](https://www.kaggle.com/competitions/digit-recognizer/data) to start. Unzip the file and place the files somewhere and use the exact path to load the files with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = pd.read_csv('./path/to/train.csv')\n",
    "test_mnist = pd.read_csv('./path/to/test.csv')\n",
    "\n",
    "#Let's check if they have been loaded properly\n",
    "print('train.shape:\\n', train.shape)\n",
    "print('test.shape:\\n', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee918b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./digit-recognizer/train.csv')\n",
    "test = pd.read_csv('./digit-recognizer/test.csv')\n",
    "\n",
    "#Let's check if they have been loaded properly\n",
    "print('train.shape:\\n', train.shape)\n",
    "print('test.shape:\\n', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab041b",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c1847",
   "metadata": {},
   "source": [
    "#### Splitting features and target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5e0b0",
   "metadata": {},
   "source": [
    "The first column of the train set contains the label and the other columns are all the features. Here variable `X` will contain the features and variable `y` the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9834254",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.iloc[:,1:]\n",
    "y = train.iloc[:,:1] #Could have done like this \n",
    "y = train.label.values # but needed to convert it to np.ndarray for torch tensor conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d98e7a",
   "metadata": {},
   "source": [
    "As you can see there are two ways of obtaining the labels. Either by using `train.iloc[:,:1]` or by using `train.labels.values`. The difference is that the former results in a Series object and the latter results in a ndarray. We want a ndarray since we want to convert this into a PyTorch Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X.shape: ', X.shape, 'X.type: ', type(X) )\n",
    "print('y.shape: ', y.shape, 'y.type: ', type(y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7a058",
   "metadata": {},
   "source": [
    "#### Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f44b5",
   "metadata": {},
   "source": [
    "You might think why do we need to split the data into a train and test set, did we not already do this? The reason is that we will need another *test* set for validation during the training proces. The original test set does not contain labels and so we do not know if our model made correct predictions if we use it on this set. \n",
    "\n",
    "We will split the training set (which contains labels) into a training and test set so that we can use its labels for validating the predictions during training. To do so the `train_test_split()` [method](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from `sklearn.model_selection` is used. This method takes in the features (X) and label column (y) separately in the first 2 parameters. The 3rd parameter is `test_size`, which we can set in order to specify with what percentage we want to split our data. By setting it to `0.1` we will split the data into 90% training and 10% test. The last parameter is the `random_state`, which is an optional parameter used to eliminate any 'randomness' and make sure we produce the same results over multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.1, random_state = 1)\n",
    "\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e36e9e",
   "metadata": {},
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97dd751",
   "metadata": {},
   "source": [
    "An important step in an image classification task is to look at the data, make sure it is loaded correctly and then make any initial observations about patterns in that data.\n",
    "\n",
    "We will make use of the `matplotlib` library to plot the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b493ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "images, labels = X_train[:10], y_train[:10]\n",
    "images = images.iloc[:,:].values\n",
    "\n",
    "# making sure we can view the images\n",
    "images = [image.astype(np.uint8).reshape((28, 28, 1)) for image in images]\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(10):\n",
    "    ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(images[idx], cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab499d0",
   "metadata": {},
   "source": [
    "#### View an Image in More Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use squeeze to remove axes of length one so from (28, 28, 1) --> (28, 28)\n",
    "img = np.squeeze(images[1])\n",
    "\n",
    "fig = plt.figure(figsize = (12,12)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "ax.set_title(f\"Label is {str(labels[1].item())}\")\n",
    "\n",
    "# annotate each pixel in the image with its value\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "        ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y]<thresh else 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a448c9f",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfa2e9",
   "metadata": {},
   "source": [
    "### Rescaling values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e2e298",
   "metadata": {},
   "source": [
    "The pixel values of the original dataset are in the range of (0,255). For a neural network to be efficient, we will rescale these values to (0,1). All the values will be rescaled to be between 0 and 1. \n",
    "\n",
    "With this example we know that the range is of (0,255) and so we could simply rescale the values by dividing by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b60fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescaling values\n",
    "X_train_rescaled = X_train.values/255\n",
    "X_test_rescaled = X_test.values/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653c262",
   "metadata": {},
   "source": [
    "We could also use the scikit-learn `preprocessing.MinMaxScaler()` [function](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to rescale our data. This function will rescale the data to a specific range. The default range is set to (0,1), but you can pass along another range as tuple (min,max) to the parameter `feature_range`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036aeed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_train_scikit = scaler.fit_transform(X_train.values)\n",
    "X_test_scikit= scaler.fit_transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be153f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train_rescaled min: {np.min(X_train_rescaled)}, max: {np.max(X_train_rescaled)}\")\n",
    "print(f\"X_test_rescaled min: {np.min(X_test_rescaled)}, max: {np.max(X_test_rescaled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train_scikit min: {np.min(X_train_scikit)}, max: {np.max(X_train_scikit)}\")\n",
    "print(f\"X_test_scikit min: {np.min(X_test_scikit)}, max: {np.max(X_test_scikit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,100))\n",
    "X_train_scikit0_100 = scaler.fit_transform(X_train.values)\n",
    "X_test_scikit0_100 = scaler.fit_transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c07009",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train_scikit0_100 min: {np.min(X_train_scikit0_100)}, max: {np.max(X_train_scikit0_100):.2f}\")\n",
    "print(f\"X_test_scikit0_100 min: {np.min(X_test_scikit0_100)}, max: {np.max(X_test_scikit0_100):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf6e67",
   "metadata": {},
   "source": [
    "While the default `preprocessing.MinMaxScaler()` might seem to normalize data, normalizing data is not simply scaling the values to fall between 0 and 1. With normalization you scale vectors individually to a unit norm so that the vector has length of one. You can perform this action with another scikit-learn [function](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) named `preprocessing.normalize()`. Here the default norm for scaling vectors is the L2 norm also known as the Euclidean norm (square root of the sum of the squares of each value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c33e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example normalization\n",
    "x_array = np.array([2,3,5,6,7,4,8,7,6])\n",
    "\n",
    "normalized_arr = preprocessing.normalize([x_array])\n",
    "print(x_array)\n",
    "\n",
    "# The output shows that all the values are in the range 0 to 1\n",
    "print(normalized_arr)\n",
    "\n",
    "# if you square each value in the output and then add them together, the result is 1, or very close to 1.\n",
    "length = 0.0\n",
    "for v in normalized_arr[0]:\n",
    "    length += v**2\n",
    "print(f\"{length:.2f}\")\n",
    "\n",
    "# example output computation\n",
    "sum_squares_values = np.sum([x**2 for x in x_array])\n",
    "print(f\"Sum of the squeres of each value: {sum_squares_values}\")\n",
    "sqrt_sum_squares_values = np.sqrt(sum_squares_values)\n",
    "print(f\"Square root of the sum of the squares of each value: {sqrt_sum_squares_values}\")\n",
    "manually_normalized_arr = [round(x/sqrt_sum_squares_values,8) for x in x_array]\n",
    "print(manually_normalized_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20acd83",
   "metadata": {},
   "source": [
    "We will only rescale the MNIST data for now.\n",
    "\n",
    "Lets keep the variable naming short for the remaining of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a771fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values/255\n",
    "X_test = X_test.values/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4aeec3",
   "metadata": {},
   "source": [
    "### Torch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf7311d",
   "metadata": {},
   "source": [
    "PyTorch works with tensors and thus we will need to convert the data. With the function `torch.from_numpy()` you can convert a NumPy array or a Panda's dataframe to Torch Tensor (remember why we needed 'y' to be a NumPy array and not series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de94fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to Tensors\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "print('X_train.dtype:', X_train.dtype)\n",
    "print('X_test.dtype:', X_test.dtype)\n",
    "print('y_train.dtype:', y_train.dtype)\n",
    "print('y_test.dtype:', y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dce374",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.requires_grad)\n",
    "print(X_test.requires_grad)\n",
    "print(y_train.requires_grad)\n",
    "print(y_test.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ae03e",
   "metadata": {},
   "source": [
    "As you can see from the output above the tensors have `requires_grad` set to `False`. While we do need to compute gradients during training and so we would like for our tensors to have this attribute set to `True`, this is implicitly done by PyTorch. The [default mode](https://pytorch.org/docs/stable/notes/autograd.html) is `grad mode` and so when a function is applied on an input tensor it will switch this attribute to `True` unless we specify differently. So for now don't worry about this attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac85e7",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28de091",
   "metadata": {},
   "source": [
    "**Sources:**\n",
    "\n",
    "- [Data Loading Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
    "\n",
    "- [Data Tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "When working with neural networks you want to provide the right amount of balanced data and because you often work with large datasets this work could become messy very easily. Luckily, PyTorch provides two modules that can be used to handle datasets and data loading: `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`. They allow you to use pre-loaded datasets or your own data as `Dataset` stores the samples and their corresponding labels and `DataLoader` wraps an iterable around the `Dataset` to enable easy loading of samples. \n",
    "\n",
    "One more reason to use these modules is that datasets can sometimes have large amounts of records and loading them all at once would be very difficult. Here a technique called `batching` can be used, which means that you access records in batches instead of all at once.\n",
    "\n",
    "**Batch:** Small set of the larger dataset.\n",
    "\n",
    "**iteration:** 1 `batch` has completed 1 iteration when it has finished going from the input layer to the output layer.\n",
    "\n",
    "**epochs:** When every record of the dataset is passed through the network, then the network is said to have finished 1 `epoch`.\n",
    "\n",
    "Below, we will make our own dataset using `TensorDataset` and set the batch size to `100`. \n",
    "\n",
    "With the help of the DataLoader class, we will make a `train_loader` and `test_loader`. The `shuffle` parameter is set to `True` to specify that the records in a particular batch need to be randomly selected from the entire dataset and not selected in the original order as they appear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac556305",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "batch = 100\n",
    "\n",
    "# Set our data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b1a68",
   "metadata": {},
   "source": [
    "### Visualize a Batch of Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc09528",
   "metadata": {},
   "source": [
    "Here we show how to visualize a batch of training data using the DataLoader class. We need to transform our data a bit as we did some preprocessing in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c95f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.detach().numpy()\n",
    "\n",
    "# making sure we can view the images\n",
    "images = images*255\n",
    "images = [image.astype(np.uint8).reshape((28, 28, 1)) for image in images]\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(images[idx], cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cddc2",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf765c4",
   "metadata": {},
   "source": [
    "Next we will define our neural network architecture using PyTorch modules. In the next code cells:\n",
    "\n",
    "- `Net()`: is the main class which inherits from `nn.Module` class. The latter is a base class for all neural network modules. Our `Net()` class has the methods `__init__()` and `forward()`. \n",
    "- `__init__()`: the `__init__()` method, which is one of the 'special methods' that are part of Python, can be seen as some sort of constructor method. It stands for initalisation and as the name tells it is used to initialize everything in the class. It will inherit all the properties of the `nn.Module`'s own `__init__()` method when we write `super().__init__()`. All the different layers are defined in this method.\n",
    "- `forward()`: this method sets the course of our network and use all the layers that we have defined in the `__init__()` method. \n",
    "\n",
    "When we make a new object of class `Net()`, the init method will be called and everything will be initialized. When we then use the object of class `Net()` on data, the forward method will be executed on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f5e03",
   "metadata": {},
   "source": [
    "### Define Multilayer Perceptron Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8d94d",
   "metadata": {},
   "source": [
    "The example architecture we will use in text:\n",
    "\n",
    "- `Layer 1: Linear Layer 1 > Activation Function (ReLU) > Dropout Layer 1`\n",
    "- `Layer 2: Linear Layer 2 > Activation Function (ReLU) > Dropout Layer 2`\n",
    "- `Layer 3: Output Layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 512\n",
    "        hidden_2 = 512\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 10)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "#Making an object of the Net class\n",
    "mlp_model = MLP().to(device)\n",
    "print(mlp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa073ea1",
   "metadata": {},
   "source": [
    "### Define Convolutional Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72857503",
   "metadata": {},
   "source": [
    "The example architecture we will use in text:\n",
    "\n",
    "- `Layer 1: Convolution Layer 1 > Activation Function (ReLU) > Pooling Layer 1 > Dropout Layer 1`\n",
    "- `Layer 2: Convolution Layer 2 > Activation Function (ReLU) > Pooling Layer 2 > Dropout Layer 2`\n",
    "- `Flatten the output from layer 2.`\n",
    "- `Layer 3: Linear Layer 1 > Activation Function (ReLU) > Dropout Layer 3`\n",
    "- `Layer 4: Linear Layer 2 > Activation Function (ReLU) > Dropout Layer 4`\n",
    "- `Layer 5: Output Layer > Activation Function (Softmax)`\n",
    "\n",
    "A convolutional neural network has multiple hidden layers and usually contains a:\n",
    "1) Convolution Layer\n",
    "2) ReLU Layer\n",
    "3) Pooling Layer\n",
    "4) Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370709a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 128, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 224, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.drop2 = nn.Dropout(p=0.4)\n",
    "        \n",
    "        self.fc3 = nn.Linear(224*4*4, 64)\n",
    "        self.drop3 = nn.Dropout(p=0.4)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.drop4 = nn.Dropout(p=0.4)\n",
    "        \n",
    "        self.fc5 = nn.Linear(32, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.pool1(F.relu(self.conv1(x))))\n",
    "        x = self.drop2(self.pool2(F.relu(self.conv2(x))))\n",
    "        x = x.view(-1,224*4*4)\n",
    "        x = self.drop3(F.relu(self.fc3(x)))\n",
    "        x = self.drop4(F.relu(self.fc4(x)))\n",
    "        x = self.softmax(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "cnn_model = CNN().to(device)\n",
    "print(cnn_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f4f93",
   "metadata": {},
   "source": [
    "The following Q&A is copied directly from the [Kaggle notebook](https://www.kaggle.com/code/amsharma7/mnist-pytorch-for-beginners-detailed-desc/notebook) we have been following along. It explains certain decisions with regard to the network architecture and will probably help to understand how to use PyTorch to build your own network. We do, however, recommend that you check the official [PyTorch documentation](https://pytorch.org/docs/stable/index.html) to see what options you have when constructing your own network. Here you can find, among other things, other layer types and what parameters they expect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660f47c",
   "metadata": {},
   "source": [
    "**Q. Why these many layers? Why not less or more?**\n",
    "\n",
    "A. Choosing the number of layers is up to the network designer (you). It's with trial and error that you develop an intiution of how many layers (or neurons) a particular problem may require to get to the desired solution. You can play with the number of layers and neurons (numbers like 128, 224 etc) and see how the network is responsding to your changes.\n",
    "\n",
    "**Q. What does Conv2d(1, 128, 5) actually mean?**\n",
    "\n",
    "A. [Conv2d()](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) is a function that helps create a convolution layer. The three paramteres mentioned (in order) are: input_channels, output_channels and kernel_size.\n",
    "\n",
    "**input_channels:** input_channels refers to the number of channels the input has. If our dataset contained colored images, this argument would have been equal to 3. But our dataset contains only the black and white pictures (more generally called as the grayscale images), so a grayscale images have only 1 channel. Hence the argument is equal to 1.\n",
    "\n",
    "**output_channels:** output_channels is arbitrary number and can be set to any number you want. It basically sets the number of neurons that its input_channels would be connected to. It also sets the input_channels of the next convolution layer. So, `output_channel (previous layer) = input_channel (next layer)`\n",
    "\n",
    "**kernel_size:** kernel_size is a square matrix of a particular width and height. When we set the third parameter to 5, it basically means we are setting the square kernel of size 5 x 5. A kernel can be thought of as a window (matrix) that slides over our image to extract certain features out of it in order to learn. That's why kernels are also called as feature maps.\n",
    "\n",
    "There's another parameter that we haven't explicitly defined but by default is set to 1, and that is **'stride'**. A [stride](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) means by how much the kernel should slide in order to output the next feature map. `stride = 1` essentially means that it will slide by 1 pixel to right and 1 pixel to down.\n",
    "\n",
    "**Q. What is pooling?**\n",
    "\n",
    "A. Pooling is another type of layer in a neural network that down samples the feature maps created by the convolution layer. It basically summarises a portion of the image in a much lesser space. It is done to reduce variance and computations. But why [max-pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)? It's because max-pooling helps in extracting low-level features like edges, points, etc. There are other type of pooling methods such as average pooling and others.\n",
    "\n",
    "**Q. What is the importance of view() function in the forward() method?**\n",
    "\n",
    "A. The view() function takes in a tensor (for ex a 4D tensor) and outputs a lower dimensional tensor (for example a 1D tensor). This is required because the next set of layers are the linear layers. And linear layers only accept a 1D tensor (a \"flattened\" array) as an input. So, the view function is used to flatten the tensor so that it can then be fed to the linear layers.\n",
    "\n",
    "**Q. What is a Dropout Layer?**\n",
    "\n",
    "A. To help understand what [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) really is, we need to understand what regularisation in general is\n",
    "\n",
    "**Regularization**\n",
    "\n",
    "Regularization helps to solve over fitting problem in machine learning. It is nothing but adding a penalty term to the objective function and control the model complexity using that penalty term.\n",
    "\n",
    "Too complex?\n",
    "\n",
    "For now, consider it as something that tells our model to not cross a certain boundry (threshold). Because if it does, it will be overfitted. Overfiting is a problem that machine learning algortihms face when the model is 'too exact' to be used for actual real world data. That simply means that our model, when overfitted, wouldn't be accurate in preidction task when a new datapoint is fed into the model (for example a datapoint from the test dataset). So, in order to avoid overfitting, we use regularization. This is just an intuition. We don't need to get into the math of anything for now.\n",
    "\n",
    "Dropout is one of the regularization methods. Dropout helps avoid overfitting by simply 'switching off' some of the neurons in a particular iteration. Sometimes, the output of a particular neuron from a particular layer can shoot off to a high value or to a very low value. So, the dropout method helps the network to consider some of the other neurons that it might have been ignoring because of their lower valued outputs. So, by simply turning off some neurons, it makes sure that it takes into account every neuron and hence train in a more vigorous manner and hence avoid overfitting.\n",
    "\n",
    "**Q. What is ReLU and Softmax?**\n",
    "\n",
    "A. ReLU is short for Rectified Linear Unit. ReLU and Softmax are both activation functions. What are activation functions, you ask? Well, they can be thought of as something that restricts the values of the neurons after computations to explode to extremes (very high negative or very high positive) values. Activation functions makes sure the values are bounded within a range. For example, the ReLU function makes sure the values are between (0,max(val)) range, where 'val' is the actual value of the neuron.\n",
    "\n",
    "**Q. But why are only ReLU and Softmax chosen?**\n",
    "\n",
    "A. You can choose any activation function you want [[non-linear activations (weighted sum, nonlinearity)](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity), [non-linear activations (other)](https://pytorch.org/docs/stable/nn.html#non-linear-activations-other)]. However, ReLU has proven to perform well in almost all the cases as compared to other activation functions. `tanh` is another very commonly used activation function. We don't have the luxury of choosing just any activation function when it comes to the output layer. We want to choose an activation function according to our desired output. We have chosen softmax because softmax gives out a class score for every class which is basically a probablity distribution that in turn tells us how confident is the neural network about a certain class to be the actual output.\n",
    "\n",
    "**Note:** The two modules imported from the torch library: nn and functional can be used interchangeable. Having said that, all the functions that are present in the nn module are there in the functional module as well. The only difference is the names of these funtions.\n",
    "\n",
    "For example, Max pooling function from the functional module can be called as:\n",
    "\n",
    "`F.max_pool2d(x, kernel_size, stride)`\n",
    "\n",
    "and from the nn module:\n",
    "\n",
    "`nn.MaxPool2d(kernel_size, stride)`\n",
    "\n",
    "Why are there two implementations of the same things? It's to suit the coding style of different people. Some prefer a stateful approach while others prefer a more functional approach.\n",
    "\n",
    "More information and building blocks can be found on [pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8ed70",
   "metadata": {},
   "source": [
    "### Define Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175e49e",
   "metadata": {},
   "source": [
    "We need a loss function to calculate the prediction error of the network. With this function we can calculate which weight should be adjusted with what particular value. There are many [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) that can be used, where for classification problems the two most common ones are Cross Entropy Loss and NLL Loss. You can experiment with the function to see which one is more benificial for your problem. A good starting point to figure out which loss function you could use for which problem is this [guide](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/).\n",
    "\n",
    "During this tutorial we will make use of Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e82d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5c05f",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc28d3",
   "metadata": {},
   "source": [
    "An optimizer ties the loss function and model parameters together by updating the model in response to the output of the loss criterion. We will be using [Adam](https://pytorch.org/docs/stable/optim.html#algorithms) as our optimizer with a learning rate of 0.0015. This value is experimental. The learning rate is one of those parameters that are changable. After a while dealiing with and constructing neural networks, you'll develop an intiution of which values could possibly work for a particular problem. Otherwise, you can always decide to keep an additional set of data (validation set) aside to optimize these kind of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31892fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr = 0.0015)\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr = 0.0015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d4aae",
   "metadata": {},
   "source": [
    "## Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d04ab3a",
   "metadata": {},
   "source": [
    "Now we will start training our neural networks. We wrote some code for training in the code block below, where all code is nicely placed in one method so that you can call the same code for different models. It has some default settings for `epochs`, `steps` and `print_every`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model,criterion,optimizer,epochs=30,steps=0,print_every=100):\n",
    "    trainLoss, testLoss = [], []\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            steps += 1   # Forward pass\n",
    "            images = (images.view(-1,1,28,28)).type(torch.DoubleTensor)\n",
    "            optimizer.zero_grad()\n",
    "            log_ps = model(images.type(torch.FloatTensor).to(device))\n",
    "            labels = labels.to(device)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            loss.backward()   # Backward pass\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if steps % print_every == 0:\n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    for images, labels in test_loader:\n",
    "                        images = (images.view(-1,1,28,28)).type(torch.DoubleTensor)\n",
    "                        log_ps = model(images.type(torch.FloatTensor).to(device))\n",
    "                        labels = labels.to(device)\n",
    "                        test_loss += criterion(log_ps, labels)\n",
    "                        ps = torch.exp(log_ps)\n",
    "\n",
    "                        top_p, top_class = ps.topk(1, dim = 1)\n",
    "                        equals = top_class == labels.view(*top_class.shape)\n",
    "                        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                trainLoss.append(running_loss/len(train_loader))\n",
    "                testLoss.append(test_loss/len(test_loader))\n",
    "\n",
    "                print(\"Epoch: {}/{}.. \".format(e + 1, epochs),\n",
    "                      \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "    return (trainLoss, testLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb56357",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "Again, the following information is copied directly from the [Kaggle notebook](https://www.kaggle.com/code/amsharma7/mnist-pytorch-for-beginners-detailed-desc/notebook) we have been following along. It explains some of the (PyTorch) code from the above code block.\n",
    "\n",
    "We are using a `for` loop to go through a total of 30 epochs.\n",
    "\n",
    "The `images` tensor is a 2D tensor, we first convert it into a 4D tensor since our neural network (the Net() class) requires it to be a 4D tensor. Hence we use the `view()` function.\n",
    "\n",
    "`optimizer.zero_grad()` sets all the gradient's values to zero. It's an essential step while training the network.\n",
    "\n",
    "`log_ps = model(images.type(torch.FloatTensor))` simply gives a set of images to the network for training. Here the set is of 100, as we set it earlier while defining the DataLoader\n",
    "\n",
    "`loss = criterion(log_ps, labels)` compare the predicted and the actual labels and calculates the loss.\n",
    "\n",
    "`loss.backward()` is the step where the network starts backpropagating in order to adjust the weights and other parameters of the network and start the training process all over again.\n",
    "\n",
    "*Please note that the loss is calculated by the loss function but the weights are updated only during backpropagation.*\n",
    "\n",
    "`running_loss += loss.item()` is used to calculate `trainLoss` which eventually would be helpful in plotting the graph of training and validation loss\n",
    "\n",
    "Since we don't want every iteration to get printed on the screen, we only want every 100th iteration to get prinited with the accuracy score, hence we have used an `if` statment for the same.\n",
    "\n",
    "`with torch.no_grad()`: the `with` keyword is an elegant way of handling exceptions in python. The wrapper `torch.no_grad` temporarily sets all the `requires_grad` flag to false. Eventually, it will reduce the memory usage and speed up computations. While we did not explicitly set the tensor attribute `requires_grad` to true, this was implicitly done during the forward pass as the [default mode](https://pytorch.org/docs/stable/notes/autograd.html) is grad mode and we need to use `torch.no_grad()` to switch from this default mode. \n",
    "\n",
    "`model.eval()` it indicates the model that nothing new is to be learnt and the model is used for testing.\n",
    "\n",
    "`torch.exp(log_ps)` returns a new tensor with the exponential of the elements of the input tensor\n",
    "\n",
    "`ps.topk(1, dim = 1)` returns a sparse copy of the tensor\n",
    "\n",
    "`labels.view(*top_class.shape)` sees how many of the classes were correct\n",
    "\n",
    "`torch.mean(equals.type(torch.FloatTensor))` calculate the mean (gets the accuracy for this batch) and add it to the running accuracy for this epoch\n",
    "\n",
    "`model.train()` tells your model that you are training the model. So effectively, layers like dropout etc, which behave different on the train and test procedures, know what is going on and hence can behave accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47febd4",
   "metadata": {},
   "source": [
    "### MLP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_trainLoss, mlp_testLoss = model_training(mlp_model,criterion,mlp_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ab2b1",
   "metadata": {},
   "source": [
    "#### Plot Training Loss\n",
    "\n",
    "Let's now visualize the training and validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_testLoss = [loss.cpu() for loss in mlp_testLoss]\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(mlp_trainLoss, label = 'Training Loss')\n",
    "plt.plot(mlp_testLoss, label = 'Validation Loss')\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af8801",
   "metadata": {},
   "source": [
    "Plotting training and validation loss is important to understand how your model is performing on the data and what actions you can take to improve performance. This [link](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/) explaines the learning curves in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fdddd9",
   "metadata": {},
   "source": [
    "### CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_trainLoss, cnn_testLoss = model_training(cnn_model,criterion,cnn_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23e868",
   "metadata": {},
   "source": [
    "#### Plot Training Loss\n",
    "\n",
    "Let's now visualize the training and validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_testLoss = [loss.cpu() for loss in cnn_testLoss]\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(cnn_trainLoss, label = 'Training Loss')\n",
    "plt.plot(cnn_testLoss, label = 'Validation Loss')\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7eaff",
   "metadata": {},
   "source": [
    "## Test Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f01900",
   "metadata": {},
   "source": [
    "After succesfully training our model we can test it on the actual test set to get predictions. We do not have the labels of images in the test set so we cannot calculate our loss on this set. We can, however, make a submission file that includes our prediction for each image and then submit this on Kaggle to get a score. The code below will help to compute the predicitions and create a submission file for Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf08bb3c",
   "metadata": {},
   "source": [
    "### Predict the labels on actual test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./digit-recognizer/test.csv')\n",
    "\n",
    "finalTest = test.values/255\n",
    "\n",
    "finalTest = torch.from_numpy(finalTest)\n",
    "\n",
    "temp = np.zeros(finalTest.shape)\n",
    "temp = torch.from_numpy(temp)\n",
    "\n",
    "data = torch.utils.data.TensorDataset(finalTest, temp)\n",
    "\n",
    "submissionLoader = torch.utils.data.DataLoader(data, batch_size = batch, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing(model):\n",
    "    submission = [['ImageId', 'Label']]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        image_id = 1\n",
    "        for images, _ in submissionLoader:\n",
    "            images = (images.view(-1,1,28,28)).type(torch.DoubleTensor)\n",
    "            log_ps = model(images.type(torch.FloatTensor).to(device))\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim = 1)\n",
    "\n",
    "            for prediction in top_class:\n",
    "                submission.append([image_id, prediction.item()])\n",
    "                image_id += 1\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a92174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_submission = model_testing(mlp_model)\n",
    "cnn_submission = model_testing(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b63d6",
   "metadata": {},
   "source": [
    "Let's plot a batch of the test set and display the predictions from the MLP and CNN models. You can use the method below that includes similar code as the visualization code from earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba26de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_submission_predictions(submission,suptitle):\n",
    "    # obtain one batch of submission images\n",
    "    dataiter = iter(submissionLoader)\n",
    "    images, _ = next(dataiter)\n",
    "    images = images.detach().numpy()\n",
    "\n",
    "    # making sure we can view the images\n",
    "    images = images*255\n",
    "    images = [image.astype(np.uint8).reshape((28, 28, 1)) for image in images]\n",
    "\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    st = fig.suptitle(suptitle, fontsize=\"x-large\")\n",
    "    for idx in np.arange(20):\n",
    "        ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "        ax.imshow(images[idx], cmap='gray')\n",
    "        # print out the correct label for each image\n",
    "        # .item() gets the value contained in a Tensor\n",
    "        ax.set_title(str(submission[idx+1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_submission_predictions(mlp_submission,\"MLP Predictions\")\n",
    "show_submission_predictions(cnn_submission,\"CNN Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d614e1",
   "metadata": {},
   "source": [
    "### Make Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8e300",
   "metadata": {},
   "source": [
    "Now if you want you can use the code below to create submission files for the Kaggle competition [Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer/overview). Login or create an account at Kaggle and head over to the competition page where you can submit the files created below. You will get a score on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(submission,filename):\n",
    "    pytorchSubmission = pd.DataFrame(submission)\n",
    "    pytorchSubmission.columns = pytorchSubmission.iloc[0]\n",
    "    pytorchSubmission = pytorchSubmission.drop(0, axis = 0)\n",
    "\n",
    "    pytorchSubmission.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83599bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission_file(mlp_submission,\"mlp_submission.csv\")\n",
    "create_submission_file(cnn_submission,\"cnn_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c6fe0",
   "metadata": {},
   "source": [
    "Submit the file to [Kaggle](https://www.kaggle.com/competitions/digit-recognizer/overview) to see your score. In order to improve your score you have multiple options:\n",
    "- augment the data\n",
    "- train with more epochs\n",
    "- optimize hyperparameters (such as learning rate)\n",
    "- try out other loss or activation functions\n",
    "- try out a different network architecture\n",
    "- add more layers\n",
    "- etc...\n",
    "\n",
    "Like we mentioned above, a good start might be the discussion page written by [Chris Deotte](https://www.kaggle.com/competitions/digit-recognizer/discussion/61480). You could create a new notebook or just continue below the final section on saving models. Copy any part from above to get started, check the official [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for the options you have (e.g., different activation functions) and start experimenting to improve your score on Kaggle.\n",
    "\n",
    "**But first let us explain one final important thing: saving and loading models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19013fcb",
   "metadata": {},
   "source": [
    "### Saving a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ca8ab",
   "metadata": {},
   "source": [
    "After you are finished training a model, you can save the model to disk so that you can load the same weights at a later time. You should extract the so-called `state_dict` from the model which contains all learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = cnn_model.state_dict()\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53e15b",
   "metadata": {},
   "source": [
    "To save the dictionary, you can use `toch.save`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(object, filename). For the filename, any extension can be used\n",
    "torch.save(state_dict, \"our_cnn_model.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1bf66",
   "metadata": {},
   "source": [
    "To load a model from a state dict, you can use the function `torch.load` to load the state dict from the disk, and the module function `load_state_dict` to overwrite our parameters with the new values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state dict from the disk (make sure it is the same name as above)\n",
    "state_dict = torch.load(\"our_cnn_model.tar\")\n",
    "\n",
    "# Create a new model and load the state\n",
    "new_model = CNN()\n",
    "new_model.load_state_dict(state_dict)\n",
    "\n",
    "# Verify that the parameters are the same\n",
    "print(\"Original model\\n\", cnn_model.state_dict())\n",
    "print(\"\\nLoaded model\\n\", new_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86529c2c",
   "metadata": {},
   "source": [
    "More information on saving and loading models can be found [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb1e62",
   "metadata": {},
   "source": [
    "## Improve Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your own code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
