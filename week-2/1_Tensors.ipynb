{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d796dad",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "> Understanding tensors, the basics data structure in PyTorch \n",
    ">\n",
    "> \\- Deep Learning with PyTorch, by Eli Stevens, Luca Antiga and Thomas Viehmann -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64252818",
   "metadata": {},
   "source": [
    "In this notebook we will provide information on what tensors are and how you can use them to prepare your data. Tensors are an important part in using PyTorch to develop and train neural networks. A lot has been written on tensors and the PyTorch framework. We collected information from many sources and tried to summarize that information in this notebook while also providing relevant exercises to test your knowledge on the topic.\n",
    "\n",
    "As this is meant only as a refresher on the topic, this is not a complete summary. So feel free to consult the mentioned sources or look up other sources if certain parts are not clear or you are interested in more background information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00286f0a",
   "metadata": {},
   "source": [
    "**From Input to Tensor**\n",
    "\n",
    "The world as floating-point numbers. To be able to create an artificial neural network that can learn from any form of input and come up with the wanted output, we will need to transform the input into a form that the neural network can digest. When using the PyTorch framework to build networks, this means we need to turn our input into floating-point numbers which we group together in so called *tensors*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582eea3",
   "metadata": {},
   "source": [
    "![title](images/world_as_floating_points.png)\n",
    "\n",
    "Image is taken from the book *Deep Learning with PyTorch*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48af978",
   "metadata": {},
   "source": [
    "You could think of tensors like multidimensional arrays. The number of dimensions of a specific tensor is the number of indexes used to refer to the scalar value inside that tensor. Some important features of tensors are:\n",
    "\n",
    "- The ability to be stored on graphics processing units (GPU) for parallel and fast computations.\n",
    "- Operation distribution across multiple devices or machines.\n",
    "- Keeping track of the graph of computations that created them (comes in handy with backpropagation). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61376c7f",
   "metadata": {},
   "source": [
    "**Note:** The information and exercises below are taken from the following link: [Basics of PyTorch](https://deeplearning.neuromatch.io/tutorials/W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html#section-2-1-creating-tensors). You will find more information on PyTorch there but this notebook only focuses on tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb4b61",
   "metadata": {},
   "source": [
    "### Lets import PyTorch (and NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d7b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfda6ff",
   "metadata": {},
   "source": [
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c8cd7",
   "metadata": {},
   "source": [
    "#### Construct tensors directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a6074",
   "metadata": {},
   "source": [
    "You can create a tensor directly from some common python iterables, such as lists or tuples. Nested iterables can also be handled as long as the dimensions are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662889c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a: tensor([0, 1, 2])\n",
      "Tensor b: tensor([[1.0000, 1.1000],\n",
      "        [1.2000, 1.3000]])\n",
      "Tensor c: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# tensor from a list\n",
    "a = torch.tensor([0, 1, 2])\n",
    "\n",
    "#tensor from a tuple of tuples\n",
    "b = ((1.0, 1.1), (1.2, 1.3))\n",
    "b = torch.tensor(b)\n",
    "\n",
    "# tensor from a numpy array\n",
    "c = np.ones([2, 3])\n",
    "c = torch.tensor(c)\n",
    "\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b: {b}\")\n",
    "print(f\"Tensor c: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14294719",
   "metadata": {},
   "source": [
    "#### Some common tensor constructors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f50a9b8",
   "metadata": {},
   "source": [
    "The numerical arguments you pass to these constructors determine the shape of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ba0396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Tensor y: tensor([0., 0.])\n",
      "Tensor z: tensor([[[0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 3)\n",
    "y = torch.zeros(2)\n",
    "z = torch.empty(1, 1, 5)\n",
    "print(f\"Tensor x: {x}\")\n",
    "print(f\"Tensor y: {y}\")\n",
    "print(f\"Tensor z: {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12048e48",
   "metadata": {},
   "source": [
    "Notice that `.empty()` does not return zeros, but seemingly random numbers. Unlike `.zeros()`, which initialises the elements of the tensor with zeros, `.empty()` just allocates the memory. It is hence a bit faster if you are looking to just create a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6fb886",
   "metadata": {},
   "source": [
    "#### Creating random tensors and tensor like other tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ce0cb",
   "metadata": {},
   "source": [
    "You can also use constructors for random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5ffb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a: tensor([[0.5958, 0.4116, 0.7309]])\n",
      "Tensor b: tensor([[-0.4440, -0.3617,  0.3509,  0.8056],\n",
      "        [ 0.1305, -0.6926,  0.3647,  0.9627],\n",
      "        [-0.7011, -0.6628,  0.7141,  0.3672]])\n"
     ]
    }
   ],
   "source": [
    "# Uniform distribution\n",
    "a = torch.rand(1, 3)\n",
    "\n",
    "# Normal distribution\n",
    "b = torch.randn(3, 4)\n",
    "\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ba7a7",
   "metadata": {},
   "source": [
    "Or constructors that allow you to construct a tensor, just like the once earlier, but with dimensions equal to another tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b7a1f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor c: tensor([[0., 0., 0.]])\n",
      "Tensor d: tensor([[0.1130, 0.0768, 0.4136]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.zeros_like(a)\n",
    "d = torch.rand_like(c)\n",
    "\n",
    "print(f\"Tensor c: {c}\")\n",
    "print(f\"Tensor d: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c77e0",
   "metadata": {},
   "source": [
    "#### Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f56152",
   "metadata": {},
   "source": [
    "With PyTorch Random Number Generator (RNG), you can use the `torch.manual_seed()` to seed the RNG for all devices (both CPU and GPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5307f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x155119868d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28daefdb",
   "metadata": {},
   "source": [
    "for custom operators, you might need to set python seed as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b17e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb9915",
   "metadata": {},
   "source": [
    "For random number generators in other libraries (e.g., NumPy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9613d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9e18e",
   "metadata": {},
   "source": [
    "#### Numpy-like number ranges:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414268c",
   "metadata": {},
   "source": [
    "The `.arange()` and `.linspace()` behave how you would expect them to if you are familiar with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "798f928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "Numpy array b: [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "Tensor c: tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,\n",
      "        4.5000, 5.0000])\n",
      "\n",
      "Numpy array d: [0.  0.5 1.  1.5 2.  2.5 3.  3.5 4.  4.5 5. ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 10, step=1)\n",
    "b = np.arange(0, 10, step=1)\n",
    "\n",
    "c = torch.linspace(0, 5, steps=11)\n",
    "d = np.linspace(0, 5, num=11)\n",
    "\n",
    "print(f\"Tensor a: {a}\\n\")\n",
    "print(f\"Numpy array b: {b}\\n\")\n",
    "print(f\"Tensor c: {c}\\n\")\n",
    "print(f\"Numpy array d: {d}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec797e2d",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d47211",
   "metadata": {},
   "source": [
    "**Creating Tensors**\n",
    "\n",
    "Below you will find some incomplete code. Fill in the missing code to construct the specified tensors.\n",
    "\n",
    "We want the tensors:\n",
    "\n",
    "1) 20 by 21 tensor consisting of ones\n",
    "2) a tensor with elements equal to the elements of numpy array `Z`\n",
    "3) a tensor with the same number of elements as `A` but with values `~U(0,1)`*\n",
    "4) a 1D tensor containing the even numbers between 4 and 40 inclusive. \n",
    "\n",
    "*U($\\alpha$,$\\beta$) denotes the uniform distribution from $\\alpha$ to $\\beta$, with $\\alpha$,$\\beta$$\\in$ $\\mathbb{R}$\n",
    "\n",
    "**You will find the answers at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5665f267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = \n",
      " [[ 1  1  1  1]\n",
      " [ 8  4  2  1]\n",
      " [27  9  3  1]]\n",
      "Shape Z = (3, 4)\n",
      "\n",
      "A = \n",
      " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.]])\n",
      "Shape A = torch.Size([20, 21])\n",
      "\n",
      "B = \n",
      " tensor([[ 1,  1,  1,  1],\n",
      "        [ 8,  4,  2,  1],\n",
      "        [27,  9,  3,  1]], dtype=torch.int32)\n",
      "\n",
      "C = \n",
      " tensor([[0.3553, 0.6219, 0.4818, 0.4408],\n",
      "        [0.4073, 0.2054, 0.6650, 0.7849],\n",
      "        [0.2104, 0.6767, 0.1097, 0.5238]], dtype=torch.float64)\n",
      "Shape C = torch.Size([3, 4])\n",
      "\n",
      "D = \n",
      " tensor([ 4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38,\n",
      "        40])\n",
      "Shape D = torch.Size([19])\n"
     ]
    }
   ],
   "source": [
    "def tensor_creation(Z):\n",
    "  \"\"\"\n",
    "  A function that creates various tensors.\n",
    "\n",
    "  Args:\n",
    "    Z: numpy.ndarray\n",
    "      An array of shape (3,4)\n",
    "\n",
    "  Returns:\n",
    "    A : Tensor\n",
    "      20 by 21 tensor consisting of ones\n",
    "    B : Tensor\n",
    "      A tensor with elements equal to the elements of numpy array Z\n",
    "    C : Tensor\n",
    "      A tensor with the same number of elements as A but with values ∼U(0,1)\n",
    "    D : Tensor\n",
    "      A 1D tensor containing the even numbers between 4 and 40 inclusive.\n",
    "  \"\"\"\n",
    "  #################################################\n",
    "  ## TODO for students: fill in the missing code\n",
    "  #################################################\n",
    "  A = torch.ones(20, 21)\n",
    "  B = torch.tensor(Z)\n",
    "  C = torch.rand_like(B, dtype=torch.float64)\n",
    "  D = torch.arange(4, 41, step=2)\n",
    "\n",
    "  return A, B, C, D\n",
    "\n",
    "\n",
    "# numpy array to copy later\n",
    "Z = np.vander([1, 2, 3], 4)\n",
    "\n",
    "# Uncomment below to check your function!\n",
    "A, B, C, D = tensor_creation(Z)\n",
    "print(f\"Z = \\n {Z}\")\n",
    "print(f\"Shape Z = {Z.shape}\")\n",
    "print()\n",
    "print(f\"A = \\n {A}\")\n",
    "print(f\"Shape A = {A.shape}\")\n",
    "print()\n",
    "print(f\"B = \\n {B}\")\n",
    "print()\n",
    "print(f\"C = \\n {C}\")\n",
    "print(f\"Shape C = {C.shape}\")\n",
    "print()\n",
    "print(f\"D = \\n {D}\")\n",
    "print(f\"Shape D = {D.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c0111",
   "metadata": {},
   "source": [
    "### Operations in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852286e",
   "metadata": {},
   "source": [
    "#### Tensor-Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69755ddb",
   "metadata": {},
   "source": [
    "You can perform operations on tensors using methods under torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38902167",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5, 3)\n",
    "b = torch.rand(5, 3)\n",
    "c = torch.empty(5, 3)\n",
    "d = torch.empty(5, 3)\n",
    "\n",
    "# this only works if c and d already exist\n",
    "torch.add(a, b, out=c)\n",
    "\n",
    "# Pointwise Multiplication of a and b\n",
    "torch.multiply(a, b, out=d)\n",
    "\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b558161",
   "metadata": {},
   "source": [
    "However, in PyTorch, most common Python operators are overridden. The common standard arithmetic operators ($+$,$-$,$*$,$/$, and $**$) have all been lifted to elementwise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c1ae48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2,  4,  7, 12]),\n",
       " tensor([0, 0, 1, 4]),\n",
       " tensor([ 1,  4, 12, 32]),\n",
       " tensor([1.0000, 1.0000, 1.3333, 2.0000]),\n",
       " tensor([   1,    4,   64, 4096]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 4, 8])\n",
    "y = torch.tensor([1, 2, 3, 4])\n",
    "x + y, x - y, x * y, x / y, x**y  # The `**` is the exponentiation operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae799315",
   "metadata": {},
   "source": [
    "#### Tensor Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4a2cb",
   "metadata": {},
   "source": [
    "There are also a number of common arithmetic operations built in with tensors. You can find a full list of all methods by clicking on [this link](https://pytorch.org/docs/stable/tensors.html).\n",
    "\n",
    "All of these operations will have a similar syntax to their NumPy equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee72790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2783, 0.4820, 0.8198],\n",
      "        [0.9971, 0.6984, 0.5675],\n",
      "        [0.8352, 0.2056, 0.5932]])\n",
      "\n",
      "\n",
      "Sum of every element of x: 5.477059841156006\n",
      "Sum of the columns of x: tensor([2.1106, 1.3860, 1.9805])\n",
      "Sum of the rows of x: tensor([1.5800, 2.2631, 1.6340])\n",
      "\n",
      "\n",
      "Mean value of all elements of x 0.6085622310638428\n",
      "Mean values of the columns of x tensor([0.7035, 0.4620, 0.6602])\n",
      "Mean values of the rows of x tensor([0.5267, 0.7544, 0.5447])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 3)\n",
    "print(x)\n",
    "print(\"\\n\")\n",
    "# sum() - note the axis is the axis you move across when summing\n",
    "print(f\"Sum of every element of x: {x.sum()}\")\n",
    "print(f\"Sum of the columns of x: {x.sum(axis=0)}\")\n",
    "print(f\"Sum of the rows of x: {x.sum(axis=1)}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Mean value of all elements of x {x.mean()}\")\n",
    "print(f\"Mean values of the columns of x {x.mean(axis=0)}\")\n",
    "print(f\"Mean values of the rows of x {x.mean(axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a12e1",
   "metadata": {},
   "source": [
    "#### Matrix Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d7890",
   "metadata": {},
   "source": [
    "The `@` symbol is overridden to represent matrix multiplication. You can also use `torch.matmul()` to multiply tensors. For dot multiplication, you can use `torch.dot()`, or manipulate the axes of your tensors and do matrix multiplication (we will cover that in the next section).\n",
    "\n",
    "Transposes of 2D tensors are obtained using `torch.t()` or `Tensor.T`. Note the lack of brackets for `Tensor.T` - it is an attribute, not a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07b329e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([])\n",
      "tensor(2.3173)\n",
      "tensor(2.3173)\n",
      "\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "tensor([-2.5860,  1.1349,  0.3903])\n",
      "tensor([-2.5860,  1.1349,  0.3903])\n",
      "\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3])\n",
      "tensor([[-0.5891,  0.5291,  0.0727],\n",
      "        [-0.3065, -0.4123, -0.1747],\n",
      "        [ 1.0066,  0.3399,  1.0905],\n",
      "        [ 0.1664, -0.4457, -0.3017],\n",
      "        [-0.0118,  0.5034,  0.2226],\n",
      "        [ 0.6001,  0.3434,  1.0768],\n",
      "        [-0.3405, -0.3924,  0.9940],\n",
      "        [-0.0932, -1.0798,  0.4816],\n",
      "        [ 0.2131, -0.8353, -1.5588],\n",
      "        [ 0.6071,  0.3234, -0.3470]])\n",
      "tensor([[-0.5891,  0.5291,  0.0727],\n",
      "        [-0.3065, -0.4123, -0.1747],\n",
      "        [ 1.0066,  0.3399,  1.0905],\n",
      "        [ 0.1664, -0.4457, -0.3017],\n",
      "        [-0.0118,  0.5034,  0.2226],\n",
      "        [ 0.6001,  0.3434,  1.0768],\n",
      "        [-0.3405, -0.3924,  0.9940],\n",
      "        [-0.0932, -1.0798,  0.4816],\n",
      "        [ 0.2131, -0.8353, -1.5588],\n",
      "        [ 0.6071,  0.3234, -0.3470]])\n",
      "\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n",
      "tensor([[[-3.7305e-01,  1.3946e+00,  6.2611e-01,  1.0271e+00, -1.0569e+00],\n",
      "         [ 6.5234e-01,  1.5451e+00,  3.2699e+00,  1.5178e+00, -1.5165e+00],\n",
      "         [ 3.0157e+00,  2.1562e+00,  2.4063e+00, -1.3024e+00, -1.5939e+00]],\n",
      "\n",
      "        [[ 9.9829e-01,  8.9068e-02,  1.7183e+00,  8.7290e-01, -8.6410e-01],\n",
      "         [-5.1593e-01,  1.6115e+00,  6.6441e-01,  1.3175e+00, -1.3732e+00],\n",
      "         [ 1.4204e+00,  6.5032e-01,  2.9159e-01,  1.6606e+00, -1.0300e+00]],\n",
      "\n",
      "        [[ 2.9984e+00,  8.1796e+00,  5.6044e+00,  2.2510e+00,  6.3810e-01],\n",
      "         [-4.3358e-01,  2.0050e+00,  2.6696e+00,  9.7125e-01, -1.6847e-01],\n",
      "         [-2.2408e+00, -3.9169e+00, -1.6709e+00, -1.0762e+00, -4.9372e-01]],\n",
      "\n",
      "        [[ 1.2470e+00, -3.3389e+00,  1.6478e+00, -1.8794e+00,  8.5604e-01],\n",
      "         [ 1.4013e+00, -3.0907e+00,  2.6869e+00,  1.2102e-01,  2.6583e+00],\n",
      "         [ 2.3875e+00, -5.4500e+00,  2.1545e+00, -2.8622e+00,  1.8888e+00]],\n",
      "\n",
      "        [[ 1.0713e-01, -9.3154e-01, -1.1323e+00, -1.5916e-01,  2.3587e+00],\n",
      "         [ 5.7095e-01, -9.1413e-01, -6.0673e-01, -6.5684e-01,  1.3709e+00],\n",
      "         [-5.1365e-01,  9.2221e-03, -4.0419e-01,  6.7687e-01,  2.8007e-01]],\n",
      "\n",
      "        [[ 6.7769e-01,  2.0274e+00,  1.1777e-01,  1.3541e-02, -7.6899e-01],\n",
      "         [ 7.0819e-01,  1.5289e+00,  3.2043e+00,  9.5491e-01,  1.0448e+00],\n",
      "         [ 9.7958e-01,  3.6954e-01,  3.9106e-03,  4.5419e-01,  5.5800e-01]],\n",
      "\n",
      "        [[ 1.2136e+00,  1.3808e+00,  4.5679e-01,  5.4856e-01, -6.9747e-02],\n",
      "         [-1.0796e+00, -1.0446e+00,  1.4011e+00, -1.3419e+00,  1.0877e-03],\n",
      "         [-1.4721e+00, -4.9222e+00, -1.6180e+00, -6.9641e-01, -3.7112e-01]],\n",
      "\n",
      "        [[-1.8663e+00, -3.2084e+00, -8.6752e+00,  5.8174e+00, -1.5556e+00],\n",
      "         [ 3.4183e+00,  5.8097e-01, -8.1136e+00, -1.3646e+00, -4.3548e+00],\n",
      "         [ 2.3549e+00,  1.7997e+00,  8.2411e+00, -4.5296e+00,  1.4947e+00]],\n",
      "\n",
      "        [[-4.3356e-01,  1.2109e-03,  3.7642e+00, -5.0598e-01, -2.7947e-01],\n",
      "         [-6.6425e-01,  5.4691e-01, -4.4531e-02, -5.1273e-01, -1.1320e+00],\n",
      "         [-9.0990e-01, -2.1202e+00, -2.1079e+00, -8.3936e-01,  3.3460e-01]],\n",
      "\n",
      "        [[-3.9059e-01,  3.9780e+00, -2.6858e-01, -5.4402e-01,  1.3530e+00],\n",
      "         [-2.5073e-01,  3.0709e+00,  1.6420e+00, -2.7058e-01,  6.8859e-01],\n",
      "         [ 4.6138e-01, -3.5268e+00, -1.6221e-01, -7.0819e-02, -8.6284e-01]]])\n",
      "tensor([[[-3.7305e-01,  1.3946e+00,  6.2611e-01,  1.0271e+00, -1.0569e+00],\n",
      "         [ 6.5234e-01,  1.5451e+00,  3.2699e+00,  1.5178e+00, -1.5165e+00],\n",
      "         [ 3.0157e+00,  2.1562e+00,  2.4063e+00, -1.3024e+00, -1.5939e+00]],\n",
      "\n",
      "        [[ 9.9829e-01,  8.9068e-02,  1.7183e+00,  8.7290e-01, -8.6410e-01],\n",
      "         [-5.1593e-01,  1.6115e+00,  6.6441e-01,  1.3175e+00, -1.3732e+00],\n",
      "         [ 1.4204e+00,  6.5032e-01,  2.9159e-01,  1.6606e+00, -1.0300e+00]],\n",
      "\n",
      "        [[ 2.9984e+00,  8.1796e+00,  5.6044e+00,  2.2510e+00,  6.3810e-01],\n",
      "         [-4.3358e-01,  2.0050e+00,  2.6696e+00,  9.7125e-01, -1.6847e-01],\n",
      "         [-2.2408e+00, -3.9169e+00, -1.6709e+00, -1.0762e+00, -4.9372e-01]],\n",
      "\n",
      "        [[ 1.2470e+00, -3.3389e+00,  1.6478e+00, -1.8794e+00,  8.5604e-01],\n",
      "         [ 1.4013e+00, -3.0907e+00,  2.6869e+00,  1.2102e-01,  2.6583e+00],\n",
      "         [ 2.3875e+00, -5.4500e+00,  2.1545e+00, -2.8622e+00,  1.8888e+00]],\n",
      "\n",
      "        [[ 1.0713e-01, -9.3154e-01, -1.1323e+00, -1.5916e-01,  2.3587e+00],\n",
      "         [ 5.7095e-01, -9.1413e-01, -6.0673e-01, -6.5684e-01,  1.3709e+00],\n",
      "         [-5.1365e-01,  9.2221e-03, -4.0419e-01,  6.7687e-01,  2.8007e-01]],\n",
      "\n",
      "        [[ 6.7769e-01,  2.0274e+00,  1.1777e-01,  1.3541e-02, -7.6899e-01],\n",
      "         [ 7.0819e-01,  1.5289e+00,  3.2043e+00,  9.5491e-01,  1.0448e+00],\n",
      "         [ 9.7958e-01,  3.6954e-01,  3.9106e-03,  4.5419e-01,  5.5800e-01]],\n",
      "\n",
      "        [[ 1.2136e+00,  1.3808e+00,  4.5679e-01,  5.4856e-01, -6.9747e-02],\n",
      "         [-1.0796e+00, -1.0446e+00,  1.4011e+00, -1.3419e+00,  1.0877e-03],\n",
      "         [-1.4721e+00, -4.9222e+00, -1.6180e+00, -6.9641e-01, -3.7112e-01]],\n",
      "\n",
      "        [[-1.8663e+00, -3.2084e+00, -8.6752e+00,  5.8174e+00, -1.5556e+00],\n",
      "         [ 3.4183e+00,  5.8097e-01, -8.1136e+00, -1.3646e+00, -4.3548e+00],\n",
      "         [ 2.3549e+00,  1.7997e+00,  8.2411e+00, -4.5296e+00,  1.4947e+00]],\n",
      "\n",
      "        [[-4.3356e-01,  1.2109e-03,  3.7642e+00, -5.0598e-01, -2.7947e-01],\n",
      "         [-6.6425e-01,  5.4691e-01, -4.4531e-02, -5.1273e-01, -1.1320e+00],\n",
      "         [-9.0990e-01, -2.1202e+00, -2.1079e+00, -8.3936e-01,  3.3460e-01]],\n",
      "\n",
      "        [[-3.9059e-01,  3.9780e+00, -2.6858e-01, -5.4402e-01,  1.3530e+00],\n",
      "         [-2.5073e-01,  3.0709e+00,  1.6420e+00, -2.7058e-01,  6.8859e-01],\n",
      "         [ 4.6138e-01, -3.5268e+00, -1.6221e-01, -7.0819e-02, -8.6284e-01]]])\n",
      "\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n",
      "tensor([[[ 0.5285, -0.1895,  1.0453,  0.1706, -0.2633],\n",
      "         [ 0.9242,  2.0868, -2.6850,  0.0373, -0.4418],\n",
      "         [-5.3711,  0.6676, -7.7488, -2.6569,  2.8568]],\n",
      "\n",
      "        [[ 1.1285,  1.3412, -4.3092,  0.9412, -1.0146],\n",
      "         [-0.3111, -0.5504, -6.3053, -0.2802, -0.6010],\n",
      "         [ 0.5103,  3.2584,  1.0654,  1.8717,  0.2455]],\n",
      "\n",
      "        [[-1.9196,  0.9276,  0.8460, -1.4507,  1.6329],\n",
      "         [-2.2041,  0.3509, -2.5382, -0.9516,  1.2433],\n",
      "         [-0.2501,  2.6318, -1.1921,  1.2647,  0.3997]],\n",
      "\n",
      "        [[-0.3677, -1.8918,  4.1494, -1.3303,  0.4696],\n",
      "         [-0.5081,  2.7714,  2.9139,  0.6725,  1.1225],\n",
      "         [-1.5346, -0.8975,  2.0890, -1.7545,  1.2028]],\n",
      "\n",
      "        [[-1.0592,  0.4888, -1.9092,  0.5382,  0.4615],\n",
      "         [-0.3334, -2.7088, -1.3665, -0.0102, -0.5058],\n",
      "         [-1.1233,  3.2282, -8.1977,  1.0580,  0.2873]],\n",
      "\n",
      "        [[ 0.7780,  4.0484,  2.1972,  1.4353,  0.4268],\n",
      "         [ 1.9586,  0.7866, -1.4882,  2.4187, -1.4980],\n",
      "         [ 2.7225, -0.9340, -1.6548,  0.4998, -2.0953]],\n",
      "\n",
      "        [[ 0.5748,  1.3208,  0.6395,  0.2322, -0.0353],\n",
      "         [ 0.5392, -3.0188,  6.8002, -1.6189,  0.0226],\n",
      "         [-2.3379, -1.1973, -2.0798, -1.7801,  1.1620]],\n",
      "\n",
      "        [[-0.2579,  0.9563,  0.1192, -0.3707,  0.4124],\n",
      "         [ 4.2828, -2.6561, -1.2195,  1.9391, -3.5266],\n",
      "         [ 0.9831, -2.9405, -1.9556, -0.4621, -1.3639]],\n",
      "\n",
      "        [[ 2.2526,  2.1252,  2.1995,  1.2911, -0.8456],\n",
      "         [-0.2550, -0.8535, -1.3748, -1.0259, -0.0477],\n",
      "         [ 3.9941,  4.9765, -2.7660,  2.9168, -2.0825]],\n",
      "\n",
      "        [[-0.3887, -1.4563, -1.1764, -0.7058, -0.1074],\n",
      "         [ 2.9704, -3.0156, -2.4269,  0.6419, -2.7777],\n",
      "         [-0.4795,  1.3800,  0.1464,  0.4107,  0.5398]]])\n",
      "tensor([[[ 0.5285, -0.1895,  1.0453,  0.1706, -0.2633],\n",
      "         [ 0.9242,  2.0868, -2.6850,  0.0373, -0.4418],\n",
      "         [-5.3711,  0.6676, -7.7488, -2.6569,  2.8568]],\n",
      "\n",
      "        [[ 1.1285,  1.3412, -4.3092,  0.9412, -1.0146],\n",
      "         [-0.3111, -0.5504, -6.3053, -0.2802, -0.6010],\n",
      "         [ 0.5103,  3.2584,  1.0654,  1.8717,  0.2455]],\n",
      "\n",
      "        [[-1.9196,  0.9276,  0.8460, -1.4507,  1.6329],\n",
      "         [-2.2041,  0.3509, -2.5382, -0.9516,  1.2433],\n",
      "         [-0.2501,  2.6318, -1.1921,  1.2647,  0.3997]],\n",
      "\n",
      "        [[-0.3677, -1.8918,  4.1494, -1.3303,  0.4696],\n",
      "         [-0.5081,  2.7714,  2.9139,  0.6725,  1.1225],\n",
      "         [-1.5346, -0.8975,  2.0890, -1.7545,  1.2028]],\n",
      "\n",
      "        [[-1.0592,  0.4888, -1.9092,  0.5382,  0.4615],\n",
      "         [-0.3334, -2.7088, -1.3665, -0.0102, -0.5058],\n",
      "         [-1.1233,  3.2282, -8.1977,  1.0580,  0.2873]],\n",
      "\n",
      "        [[ 0.7780,  4.0484,  2.1972,  1.4353,  0.4268],\n",
      "         [ 1.9586,  0.7866, -1.4882,  2.4187, -1.4980],\n",
      "         [ 2.7225, -0.9340, -1.6548,  0.4998, -2.0953]],\n",
      "\n",
      "        [[ 0.5748,  1.3208,  0.6395,  0.2322, -0.0353],\n",
      "         [ 0.5392, -3.0188,  6.8002, -1.6189,  0.0226],\n",
      "         [-2.3379, -1.1973, -2.0798, -1.7801,  1.1620]],\n",
      "\n",
      "        [[-0.2579,  0.9563,  0.1192, -0.3707,  0.4124],\n",
      "         [ 4.2828, -2.6561, -1.2195,  1.9391, -3.5266],\n",
      "         [ 0.9831, -2.9405, -1.9556, -0.4621, -1.3639]],\n",
      "\n",
      "        [[ 2.2526,  2.1252,  2.1995,  1.2911, -0.8456],\n",
      "         [-0.2550, -0.8535, -1.3748, -1.0259, -0.0477],\n",
      "         [ 3.9941,  4.9765, -2.7660,  2.9168, -2.0825]],\n",
      "\n",
      "        [[-0.3887, -1.4563, -1.1764, -0.7058, -0.1074],\n",
      "         [ 2.9704, -3.0156, -2.4269,  0.6419, -2.7777],\n",
      "         [-0.4795,  1.3800,  0.1464,  0.4107,  0.5398]]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "\n",
    "# vector x vector\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3)\n",
    "print(torch.matmul(tensor1, tensor2).size())\n",
    "print((tensor1 @ tensor2).size())\n",
    "print(torch.matmul(tensor1, tensor2))\n",
    "print(tensor1 @ tensor2)\n",
    "\n",
    "print()\n",
    "# matrix x vector\n",
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "print(torch.matmul(tensor1, tensor2).size())\n",
    "print((tensor1 @ tensor2).size())\n",
    "print(torch.matmul(tensor1, tensor2))\n",
    "print(tensor1 @ tensor2)\n",
    "\n",
    "print()\n",
    "# batched matrix x broadcasted vector\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "print(torch.matmul(tensor1, tensor2).size())\n",
    "print((tensor1 @ tensor2).size())\n",
    "print(torch.matmul(tensor1, tensor2))\n",
    "print(tensor1 @ tensor2)\n",
    "\n",
    "print()\n",
    "# batched matrix x batched matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(tensor1, tensor2).size())\n",
    "print((tensor1 @ tensor2).size())\n",
    "print(torch.matmul(tensor1, tensor2))\n",
    "print(tensor1 @ tensor2)\n",
    "\n",
    "print()\n",
    "# batched matrix x broadcasted matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4, 5)\n",
    "print(torch.matmul(tensor1, tensor2).size())\n",
    "print((tensor1 @ tensor2).size())\n",
    "print(torch.matmul(tensor1, tensor2))\n",
    "print(tensor1 @ tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f98898c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product\n",
    "\n",
    "torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0270110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5694)\n",
      "tensor(0.5694)\n",
      "tensor(0.5694)\n",
      "\n",
      "tensor([-1.0221,  0.9506,  0.5398])\n",
      "tensor([-1.0221,  0.9506,  0.5398])\n",
      "tensor([-1.0221,  0.9506,  0.5398])\n",
      "\n",
      "tensor([[-0.4088, -0.4296, -0.6978],\n",
      "        [ 0.1943, -0.2045,  0.4200]])\n",
      "tensor([[-0.4088,  0.1943],\n",
      "        [-0.4296, -0.2045],\n",
      "        [-0.6978,  0.4200]])\n",
      "tensor([[-0.4088,  0.1943],\n",
      "        [-0.4296, -0.2045],\n",
      "        [-0.6978,  0.4200]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daan\\AppData\\Local\\Temp\\ipykernel_18180\\1559610677.py:6: UserWarning: Tensor.T is deprecated on 0-D tensors. This function is the identity in these cases. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3645.)\n",
      "  print(x.T)\n",
      "C:\\Users\\Daan\\AppData\\Local\\Temp\\ipykernel_18180\\1559610677.py:12: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3641.)\n",
      "  print(x.T)\n"
     ]
    }
   ],
   "source": [
    "# transpose\n",
    "\n",
    "x = torch.randn(())\n",
    "print(x)\n",
    "print(torch.t(x))\n",
    "print(x.T)\n",
    "\n",
    "print()\n",
    "x = torch.randn(3)\n",
    "print(x)\n",
    "print(torch.t(x))\n",
    "print(x.T)\n",
    "\n",
    "print()\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(torch.t(x))\n",
    "print(x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffd6e8",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d04768",
   "metadata": {},
   "source": [
    "**Simple tensor operations**\n",
    "\n",
    "Below are two expressions involving operations on matrices:\n",
    "\n",
    "$$\\mathbb{A}= \\begin{bmatrix} 2 & 4 \\\\ 5 & 7\\end{bmatrix}\\begin{bmatrix} 1 & 1 \\\\ 2 & 3 \\end{bmatrix} + \\begin{bmatrix} 10 & 10 \\\\ 12 & 1\\end{bmatrix}$$\n",
    "\n",
    "$$\\text{and}$$\n",
    "\n",
    "$$\\mathbb{b}= \\begin{bmatrix} 3 \\\\ 5 \\\\ 7 \\end{bmatrix} . \\begin{bmatrix} 2 \\\\ 4 \\\\ 8 \\end{bmatrix}$$\n",
    "\n",
    "The code block below that computes these expressions using PyTorch is incomplete - fill in the missing lines.\n",
    "\n",
    "**You will find the answers at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b290be04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20, 24],\n",
      "        [31, 27]])\n"
     ]
    }
   ],
   "source": [
    "def simple_operations(a1: torch.Tensor, a2: torch.Tensor, a3: torch.Tensor):\n",
    "  \"\"\"\n",
    "  Helper function to demonstrate simple operations\n",
    "  i.e., Multiplication of tensor a1 with tensor a2 and then add it with tensor a3\n",
    "\n",
    "  Args:\n",
    "    a1: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "    a2: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "    a3: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "\n",
    "  Returns:\n",
    "    answer: Torch tensor\n",
    "      Tensor of size ([2,2]) resulting from a1 multiplied with a2, added with a3\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## TODO for students:  complete the first computation using the argument matricies\n",
    "  ################################################\n",
    "  #\n",
    "  answer = a1 @ a2 + a3\n",
    "  return answer\n",
    "\n",
    "# Computing expression 1:\n",
    "\n",
    "# init our tensors\n",
    "a1 = torch.tensor([[2, 4], [5, 7]])\n",
    "a2 = torch.tensor([[1, 1], [2, 3]])\n",
    "a3 = torch.tensor([[10, 10], [12, 1]])\n",
    "## uncomment to test your function\n",
    "A = simple_operations(a1, a2, a3)\n",
    "print(A)\n",
    "assert torch.equal(A,torch.tensor([[20, 24],[31, 27]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "093e933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(82)\n"
     ]
    }
   ],
   "source": [
    "def dot_product(b1: torch.Tensor, b2: torch.Tensor):\n",
    "  ###############################################\n",
    "  ## TODO for students:  complete the first computation using the argument matricies\n",
    "  ###############################################\n",
    "  \"\"\"\n",
    "  Helper function to demonstrate dot product operation\n",
    "  Dot product is an algebraic operation that takes two equal-length sequences\n",
    "  (usually coordinate vectors), and returns a single number.\n",
    "  Geometrically, it is the product of the Euclidean magnitudes of the\n",
    "  two vectors and the cosine of the angle between them.\n",
    "\n",
    "  Args:\n",
    "    b1: Torch tensor\n",
    "      Tensor of size ([3])\n",
    "    b2: Torch tensor\n",
    "      Tensor of size ([3])\n",
    "\n",
    "  Returns:\n",
    "    product: Tensor\n",
    "      Tensor of size ([1]) resulting from b1 scalar multiplied with b2\n",
    "  \"\"\"\n",
    "  # Use torch.dot() to compute the dot product of two tensors\n",
    "  product = b1.dot(b2)\n",
    "  return product\n",
    "\n",
    "# Computing expression 2:\n",
    "b1 = torch.tensor([3, 5, 7])\n",
    "b2 = torch.tensor([2, 4, 8])\n",
    "## Uncomment to test your function\n",
    "b = dot_product(b1, b2)\n",
    "print(b)\n",
    "assert torch.equal(b, torch.tensor(82))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bf125",
   "metadata": {},
   "source": [
    "### Manipulating Tensors in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39db6d",
   "metadata": {},
   "source": [
    "#### Tensor Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd9ed9",
   "metadata": {},
   "source": [
    "You can access elements in a tensor by index. Like with any numpy array, the first element has index 0 and ranges are specified to include the first to second to last (n-1). You can access elements according to their relative position to the end of the list by using negative indices. Another name for indexing is slicing.\n",
    "\n",
    "For example, `[-1]` picks the last element from a tensor; `[1:3]` selects the second and the third elements and `[:-2]` will select all elements excluding the last and second-to-last elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3e7089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor(9)\n",
      "tensor([1, 2])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 10)\n",
    "print(x)\n",
    "print(x[-1])\n",
    "print(x[1:3])\n",
    "print(x[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcbec5",
   "metadata": {},
   "source": [
    "When you have multidimensional tensors, indexing rules work the same way as in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d17e959c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape of x[0]:torch.Size([2, 3, 4, 5])\n",
      " shape of x[0][0]:torch.Size([3, 4, 5])\n",
      " shape of x[0][0][0]:torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# make a 5D tensor\n",
    "x = torch.rand(1, 2, 3, 4, 5)\n",
    "\n",
    "print(f\" shape of x[0]:{x[0].shape}\")\n",
    "print(f\" shape of x[0][0]:{x[0][0].shape}\")\n",
    "print(f\" shape of x[0][0][0]:{x[0][0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e0c38",
   "metadata": {},
   "source": [
    "#### Flatten and Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ddfb5",
   "metadata": {},
   "source": [
    "To reshape tensors, you can make use of various methods. It is common to have to express 2D data in 1D format or vice versa. You can achieve this with the `.flatten()` and `.reshape()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f48f0b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original z: \n",
      " tensor([[ 0,  1],\n",
      "        [ 2,  3],\n",
      "        [ 4,  5],\n",
      "        [ 6,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 11]])\n",
      "Flattened z: \n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Reshaped (3x4) z: \n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "Flattened z: \n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Using view to reshape to (6x2) z: \n",
      " tensor([[ 0,  1],\n",
      "        [ 2,  3],\n",
      "        [ 4,  5],\n",
      "        [ 6,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 11]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.arange(12).reshape(6, 2)\n",
    "print(f\"Original z: \\n {z}\")\n",
    "\n",
    "# 2D -> 1D\n",
    "z = z.flatten()\n",
    "print(f\"Flattened z: \\n {z}\")\n",
    "\n",
    "# and back to 2D\n",
    "z = z.reshape(3, 4)\n",
    "print(f\"Reshaped (3x4) z: \\n {z}\")\n",
    "\n",
    "z = z.flatten()\n",
    "print(f\"Flattened z: \\n {z}\")\n",
    "\n",
    "z = z.view(6, 2)\n",
    "print(f\"Using view to reshape to (6x2) z: \\n {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a32eb",
   "metadata": {},
   "source": [
    "You will also see the `.view()` methods used a lot to reshape tensors. There is a subtle [difference](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch) between `.view()` and `.reshape()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddc5d5",
   "metadata": {},
   "source": [
    "#### Squeezing Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c5082",
   "metadata": {},
   "source": [
    "When processing batches of data, you will quite often be left with singleton dimensions. E.g., `[1,10]` or `[256, 1, 3]`. This dimension can quite easily mess up your matrix operations if you don’t plan on it being there.\n",
    "\n",
    "In order to compress tensors along their singleton dimensions we can use the `.squeeze()` method. We can use the `.unsqueeze()` method to do the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92dda4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "x[0]: tensor([ 0.5369, -0.8644,  1.0802, -0.2326,  1.0315,  0.7151,  1.0977,  0.0353,\n",
      "        -1.5014,  1.3895])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "# printing the zeroth element of the tensor will not give us the first number!\n",
    "\n",
    "print(x.shape)\n",
    "print(f\"x[0]: {x[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6b9d0",
   "metadata": {},
   "source": [
    "Because of the singleton dimension, `x[0]` gives us the first row instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e296bae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "x[0]: 0.5368556380271912\n"
     ]
    }
   ],
   "source": [
    "# Let's get rid of that singleton dimension and see what happens now\n",
    "x = x.squeeze(0)\n",
    "print(x.shape)\n",
    "print(f\"x[0]: {x[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42331695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y: torch.Size([5, 5])\n",
      "Shape of y: torch.Size([5, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Adding singleton dimensions works a similar way, and is often used when tensors\n",
    "# being added need same number of dimensions\n",
    "\n",
    "y = torch.randn(5, 5)\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# lets insert a singleton dimension\n",
    "y = y.unsqueeze(1)\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b9fb9",
   "metadata": {},
   "source": [
    "#### Permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a49ac0",
   "metadata": {},
   "source": [
    "Sometimes your dimensions will be in the wrong order. For example, you may be dealing with RGB images with dim `[3×48×64]`, but your pipeline expects the colour dimension to be the last dimension, i.e., `[48×64×3]`. To get around this you can use the `.permute()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0d2ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "# `x` has dimensions [color,image_height,image_width]\n",
    "x = torch.rand(3, 48, 64)\n",
    "\n",
    "# We want to permute our tensor to be [ image_height , image_width , color ]\n",
    "x = x.permute(1, 2, 0)\n",
    "# permute(1,2,0) means:\n",
    "# The 0th dim of my new tensor = the 1st dim of my old tensor\n",
    "# The 1st dim of my new tensor = the 2nd\n",
    "# The 2nd dim of my new tensor = the 0th\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a29c31",
   "metadata": {},
   "source": [
    "You may also see `.transpose()` used. This works in a similar way as permute, but can only swap two dimensions at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d23ede",
   "metadata": {},
   "source": [
    "#### Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f1144",
   "metadata": {},
   "source": [
    "In this example, two matrices are concatenated along rows (axis 0, the first element of the shape) vs. columns (axis 1, the second element of the shape). You can see that the first output tensor’s axis-0 length (`6`) is the sum of the two input tensors’ axis-0 lengths (`3+3`); while the second output tensor’s axis-1 length (`8`) is the sum of the two input tensors’ axis-1 lengths (`4+4`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec3cba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated by rows: shape[6, 4] \n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "\n",
      " Concatenated by colums: shape[3, 8]  \n",
      " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors of the same shape\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n",
    "\n",
    "# Concatenate along rows\n",
    "cat_rows = torch.cat((x, y), dim=0)\n",
    "\n",
    "# Concatenate along columns\n",
    "cat_cols = torch.cat((x, y), dim=1)\n",
    "\n",
    "# Printing outputs\n",
    "print('Concatenated by rows: shape{} \\n {}'.format(list(cat_rows.shape), cat_rows))\n",
    "print('\\n Concatenated by colums: shape{}  \\n {}'.format(list(cat_cols.shape), cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508d935",
   "metadata": {},
   "source": [
    "#### Conversion to Other Python Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f0cb5",
   "metadata": {},
   "source": [
    "Converting a tensor to a numpy.ndarray, or vice versa, is easy, and the converted result does not share memory. This minor inconvenience is quite important: when you perform operations on the CPU or GPUs, you do not want to halt computation, waiting to see whether the NumPy package of Python might want to be doing something else with the same chunk of memory.\n",
    "\n",
    "When converting to a NumPy array, the information being tracked by the tensor will be lost, i.e., the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47eba6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([-0.6709,  0.8899,  1.3470, -1.7338,  0.2982])  |  x type:  torch.FloatTensor\n",
      "y: [-0.67085654  0.8898792   1.3470085  -1.7338079   0.29823947]  |  y type:  <class 'numpy.ndarray'>\n",
      "z: tensor([-0.6709,  0.8899,  1.3470, -1.7338,  0.2982])  |  z type:  torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "print(f\"x: {x}  |  x type:  {x.type()}\")\n",
    "\n",
    "y = x.numpy()\n",
    "print(f\"y: {y}  |  y type:  {type(y)}\")\n",
    "\n",
    "z = torch.tensor(y)\n",
    "print(f\"z: {z}  |  z type:  {z.type()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff006be2",
   "metadata": {},
   "source": [
    "To convert a size-1 tensor to a Python scalar, you can invoke the item function or Python’s built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17f3ac3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c0439",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de25bd4",
   "metadata": {},
   "source": [
    "**Simple tensor operations**\n",
    "\n",
    "Using a combination of the methods discussed above, complete the functions below:\n",
    "\n",
    "**Function A**\n",
    "\n",
    "This function takes in two 2D tensors $A$ and $B$ and returns the column sum of $A$ multiplied by the sum of all elements of $B$, i.e., a scalar, e.g.,\n",
    "\n",
    "$$\\text{If } A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\text{and } B = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix} \\text{then } Out = \\begin{bmatrix} 2 & 2 \\end{bmatrix} . 12 = \\begin{bmatrix} 24 & 24 \\end{bmatrix}$$\n",
    "\n",
    "**Function B**\n",
    "\n",
    "This functions takes in a square matric $C$ and returns a 2D tensor consisting of a flattened $C$ with the index of each element appended to this tensor in the row dimension, e.g.,\n",
    "\n",
    "$$\\text{If } C = \\begin{bmatrix} 2 & 3 \\\\ -1 & 10 \\end{bmatrix} \\text{then } Out = \\begin{bmatrix} 0 & 2 \\\\ 1 & 3 \\\\ 2 & -1 \\\\ 3 & 10 \\end{bmatrix}$$\n",
    "\n",
    "**Hint:** Pay close attention to singleton dimensions.\n",
    "\n",
    "**Function C**\n",
    "\n",
    "This function takes in two 2D tensors $D$ and $E$. If the dimensions allow it, this function returns the elementwise sum of D-shaped $E$, and $D$; else this function returns a 1D tensor that is the concatenation of the two tensors, e.g.,\n",
    "\n",
    "$$\\text{If } D = \\begin{bmatrix} 1 & -1 \\\\ -1 & 3 \\end{bmatrix} \\text{and } E = \\begin{bmatrix} 2 & 3 & 0 & 2 \\end{bmatrix} \\text{then } Out = \\begin{bmatrix} 3 & 2 \\\\ -1 & 5 \\end{bmatrix}$$\n",
    "\n",
    "$$\\text{If } D = \\begin{bmatrix} 1 & -1 \\\\ -1 & 3 \\end{bmatrix} \\text{and } E = \\begin{bmatrix} 2 & 3 & 0 \\end{bmatrix} \\text{then } Out = \\begin{bmatrix} 1 & -1 & -1 & 3 & 2 & 3 & 0 \\end{bmatrix}$$\n",
    "\n",
    "**Hint:** `torch.numel()` is an easy way of finding the number of elements in a tensor.\n",
    "\n",
    "**You will find the answers at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f695cfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24, 24])\n",
      "tensor([[ 0,  2],\n",
      "        [ 1,  3],\n",
      "        [ 2, -1],\n",
      "        [ 3, 10]])\n",
      "tensor([[ 3,  2],\n",
      "        [-1,  5]])\n",
      "tensor([ 1, -1, -1,  3,  2,  3,  0])\n"
     ]
    }
   ],
   "source": [
    "def functionA(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`\n",
    "  and returns the column sum of\n",
    "  `my_tensor1` multiplied by the sum of all the elmements of `my_tensor2`,\n",
    "  i.e., a scalar.\n",
    "\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "\n",
    "  Retuns:\n",
    "    output: torch.Tensor\n",
    "      The multiplication of the column sum of `my_tensor1` by the sum of\n",
    "      `my_tensor2`.\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## TODO for students: complete functionA\n",
    "  ################################################\n",
    "  # TODO multiplication the sum of the tensors\n",
    "  output = my_tensor1.sum(axis=1) * my_tensor2.sum()\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionB(my_tensor):\n",
    "  \"\"\"\n",
    "  This function takes in a square matrix `my_tensor` and returns a 2D tensor\n",
    "  consisting of a flattened `my_tensor` with the index of each element\n",
    "  appended to this tensor in the row dimension.\n",
    "\n",
    "  Args:\n",
    "    my_tensor: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## TODO for students: complete functionB\n",
    "  ################################################\n",
    "  # TODO flatten the tensor `my_tensor`\n",
    "  my_tensor = my_tensor.flatten()\n",
    "  # TODO create the idx tensor to be concatenated to `my_tensor`\n",
    "  idx_tensor = torch.arange(len(my_tensor))\n",
    "  # TODO concatenate the two tensors\n",
    "  output = torch.cat((idx_tensor.unsqueeze(dim=1), my_tensor.unsqueeze(dim=1)), 1)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionC(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`.\n",
    "  If the dimensions allow it, it returns the\n",
    "  elementwise sum of `my_tensor1`-shaped `my_tensor2`, and `my_tensor2`;\n",
    "  else this function returns a 1D tensor that is the concatenation of the\n",
    "  two tensors.\n",
    "\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## TODO for students: complete functionC\n",
    "  ################################################\n",
    "  # TODO check we can reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "  if len(my_tensor1.flatten()) == len(my_tensor2.flatten()):\n",
    "    # TODO reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "    my_tensor2 = torch.reshape(my_tensor2, my_tensor1.shape)\n",
    "    # TODO sum the two tensors\n",
    "    output = my_tensor1 + my_tensor2\n",
    "  else:\n",
    "    # TODO flatten both tensors\n",
    "    my_tensor1 = my_tensor1.flatten()\n",
    "    my_tensor2 = my_tensor2.flatten()\n",
    "    # TODO concatenate the two tensors in the correct dimension\n",
    "    output = torch.cat((my_tensor1, my_tensor2), axis=0)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "# Implement the functions above and then uncomment the following lines to test your code\n",
    "print(functionA(torch.tensor([[1, 1], [1, 1]]), torch.tensor([[1, 2, 3], [1, 2, 3]])))\n",
    "print(functionB(torch.tensor([[2, 3], [-1, 10]])))\n",
    "print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0, 2]])))\n",
    "print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0]])))\n",
    "\n",
    "assert torch.equal(functionA(torch.tensor([[1, 1], [1, 1]]), torch.tensor([[1, 2, 3], [1, 2, 3]])), torch.tensor([24, 24]))\n",
    "assert torch.equal(functionB(torch.tensor([[2, 3], [-1, 10]])), torch.tensor([[0, 2], [1, 3], [2, -1], [3, 10]]))\n",
    "assert torch.equal(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0, 2]])), torch.tensor([[3, 2], [-1, 5]]))\n",
    "assert torch.equal(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0]])), torch.tensor([1, -1, -1, 3, 2, 3, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472347d7",
   "metadata": {},
   "source": [
    "### Dynamic Computation Graph and Backpropagation\n",
    "\n",
    "The information on backpropagation and the dynamic computation graph can be found in the notebook [Introduction to PyTorch](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.ipynb). There you will also find additional information on Tensors similar to the information we already shared in this notebook. \n",
    "\n",
    "As we already shared, one of the features of tensors is there ability to keep track of the graph of computations that created them (which comes in handy with backpropagation). With PyTorch tensors you can automatically get **gradients/derivatives** of functions that you define. In this course we will mainly implement neural networks, which are just fancy functions. \n",
    "\n",
    "In those functions, the weight matrices we want to learn are called **parameters** or simply the **weights**. If a neural network outputs a single scalar value, we talk about taking the derivative of that value. However, often times we have to deal with **multiple** output values, which means that we talk about **gradients**.\n",
    "\n",
    "We define our function by **manipulating** a given input, usually bu matrix-multiplications with weight matrices and additions with so-called bias vectors. A **computation graph** is automatically created when manipulating our input. This graph tells us how to arrive at our output from our input. PyTorch is a **define-by-run** framework, which means that we can just do our manipulations and PyTorch will keep track of the computation graph for us (dynamically).\n",
    "\n",
    "In short: we only need to compute the output using our function and we can ask PyTorch to automatically get the **gradients**. \n",
    "\n",
    "The first thing we have to do is to specify which tensors require gradients. The default setting is that a newly created tensor does not require gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3207f5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((3,))\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50124e86",
   "metadata": {},
   "source": [
    "You can change this for an existing tensor using the function `requires_grad_()` (the underscore indicates that this is a in-place operation). Alternatively, when creating a tensor, you can pass the argument `requires_grad=True` to most initializers shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e8ef9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(True)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2c134",
   "metadata": {},
   "source": [
    "In order to get familiar with the concept of a computation graph, we will create one for the following function:\n",
    "\n",
    "$$y = \\frac{1}{|x|}\\sum_i \\left[(x_i + 2)^2 + 3\\right]$$\n",
    "\n",
    "You could imagine that $x$ are our parameters, and we want to optimize (either maximize or minimize) the output $y$. For this, we want to obtain the gradients $\\partial y / \\partial \\mathbf{x}$. For our example, we'll use $\\mathbf{x}=[0,1,2]$ as our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c98b696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([0., 1., 2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # Only float tensors can have gradients\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb5ec8",
   "metadata": {},
   "source": [
    "Now let's build the computation graph step by step. You can combine multiple operations in a single line, but we will separate them here to get a better understanding of how each operation is added to the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1f246f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y tensor(12.6667, grad_fn=<MeanBackward0>)\n",
      "<MeanBackward0 object at 0x00000155150EFD30>\n"
     ]
    }
   ],
   "source": [
    "a = x + 2\n",
    "b = a ** 2\n",
    "c = b + 3\n",
    "y = c.mean()\n",
    "print(\"Y\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f487b",
   "metadata": {},
   "source": [
    "Using the statements above, we have created a computation graph that looks similar to the figure below:\n",
    "\n",
    "<center style=\"width: 100%\"><img src=\"images/pytorch_computation_graph.svg\" width=\"200px\"></center>\n",
    "\n",
    "We calculate $a$ based on the inputs $x$ and the constant $2$, $b$ is $a$ squared, and so on. The visualization is an abstraction of the dependencies between inputs and outputs of the operations we have applied.\n",
    "Each node of the computation graph has automatically defined a function for calculating the gradients with respect to its inputs, `grad_fn`. You can see this when we printed the output tensor $y$. This is why the computation graph is usually visualized in the reverse direction (arrows point from the result to the inputs). We can perform backpropagation on the computation graph by calling the function `backward()` on the last output, which effectively calculates the gradients for each tensor that has the property `requires_grad=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58127800",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadc3c6",
   "metadata": {},
   "source": [
    "`x.grad` will now contain the gradient $\\partial y/ \\partial \\mathcal{x}$, and this gradient indicates how a change in $\\mathbf{x}$ will affect output $y$ given the current input $\\mathbf{x}=[0,1,2]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce321d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3333, 2.0000, 2.6667])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccde7a1",
   "metadata": {},
   "source": [
    "We can also verify these gradients by hand. We will calculate the gradients using the chain rule, in the same way as PyTorch did it:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial c_i}\\frac{\\partial c_i}{\\partial b_i}\\frac{\\partial b_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_i}$$\n",
    "\n",
    "Note that we have simplified this equation to index notation, and by using the fact that all operation besides the mean do not combine the elements in the tensor. The partial derivatives are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_i}{\\partial x_i} = 1,\\hspace{1cm}\n",
    "\\frac{\\partial b_i}{\\partial a_i} = 2\\cdot a_i\\hspace{1cm}\n",
    "\\frac{\\partial c_i}{\\partial b_i} = 1\\hspace{1cm}\n",
    "\\frac{\\partial y}{\\partial c_i} = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "Hence, with the input being $\\mathbf{x}=[0,1,2]$, our gradients are $\\partial y/\\partial \\mathbf{x}=[4/3,2,8/3]$. The previous code cell should have printed the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf87380",
   "metadata": {},
   "source": [
    "**Note:** No Exercises for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc96200",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c87e9d",
   "metadata": {},
   "source": [
    "#### Exercise Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a323f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_creation(Z):\n",
    "  \"\"\"\n",
    "  A function that creates various tensors.\n",
    "  Args:\n",
    "    Z: numpy.ndarray\n",
    "      An array of shape (3,4)\n",
    "  Returns:\n",
    "    A : Tensor\n",
    "      20 by 21 tensor consisting of ones\n",
    "    B : Tensor\n",
    "      A tensor with elements equal to the elements of numpy array  Z\n",
    "    C : Tensor\n",
    "      A tensor with the same number of elements as A but with values ∼U(0,1)\n",
    "    D : Tensor\n",
    "      A 1D tensor containing the even numbers between 4 and 40 inclusive.\n",
    "  \"\"\"\n",
    "\n",
    "  A = torch.ones(20, 21)\n",
    "  B = torch.tensor(Z)\n",
    "  C = torch.rand_like(A)\n",
    "  D = torch.arange(4, 41, step=2)\n",
    "\n",
    "  return A, B, C, D\n",
    "\n",
    "# numpy array to copy later\n",
    "Z = np.vander([1, 2, 3], 4)\n",
    "\n",
    "# Uncomment below to check your function!\n",
    "A, B, C, D = tensor_creation(Z)\n",
    "print(f\"Z = \\n {Z}\")\n",
    "print(f\"Shape Z = {Z.shape}\")\n",
    "print()\n",
    "print(f\"A = \\n {A}\")\n",
    "print(f\"Shape A = {A.shape}\")\n",
    "print()\n",
    "print(f\"B = \\n {B}\")\n",
    "print()\n",
    "print(f\"C = \\n {C}\")\n",
    "print(f\"Shape C = {C.shape}\")\n",
    "print()\n",
    "print(f\"D = \\n {D}\")\n",
    "print(f\"Shape D = {D.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7830d",
   "metadata": {},
   "source": [
    "#### Exercises Operations in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47851d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_operations(a1: torch.Tensor, a2: torch.Tensor, a3: torch.Tensor):\n",
    "  \"\"\"\n",
    "  Helper function to demonstrate simple operations\n",
    "  i.e., Multiplication of tensor a1 with tensor a2 and then add it with tensor a3\n",
    "  Args:\n",
    "    a1: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "    a2: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "    a3: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "  Returns:\n",
    "    answer: Torch tensor\n",
    "      Tensor of size ([2,2]) resulting from a1 multiplied with a2, added with a3\n",
    "  \"\"\"\n",
    "  answer = a1 @ a2 + a3\n",
    "  return answer\n",
    "\n",
    "# Computing expression 1:\n",
    "\n",
    "# init our tensors\n",
    "a1 = torch.tensor([[2, 4], [5, 7]])\n",
    "a2 = torch.tensor([[1, 1], [2, 3]])\n",
    "a3 = torch.tensor([[10, 10], [12, 1]])\n",
    "## uncomment to test your function\n",
    "A = simple_operations(a1, a2, a3)\n",
    "print(A)\n",
    "assert torch.equal(A,torch.tensor([[20, 24],[31, 27]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666958e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(b1: torch.Tensor, b2: torch.Tensor):\n",
    "  \"\"\"\n",
    "  Helper function to demonstrate dot product operation\n",
    "  Dot product is an algebraic operation that takes two equal-length sequences\n",
    "  (usually coordinate vectors), and returns a single number.\n",
    "  Geometrically, it is the product of the Euclidean magnitudes of the\n",
    "  two vectors and the cosine of the angle between them.\n",
    "  Args:\n",
    "    b1: Torch tensor\n",
    "      Tensor of size ([3])\n",
    "    b2: Torch tensor\n",
    "      Tensor of size ([3])\n",
    "  Returns:\n",
    "    product: Tensor\n",
    "      Tensor of size ([1]) resulting from b1 scalar multiplied with b2\n",
    "  \"\"\"\n",
    "  # Use torch.dot() to compute the dot product of two tensors\n",
    "  product = torch.dot(b1, b2)\n",
    "  return product\n",
    "\n",
    "# Computing expression 2:\n",
    "b1 = torch.tensor([3, 5, 7])\n",
    "b2 = torch.tensor([2, 4, 8])\n",
    "## Uncomment to test your function\n",
    "b = dot_product(b1, b2)\n",
    "print(b)\n",
    "assert torch.equal(b, torch.tensor(82))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c0515b",
   "metadata": {},
   "source": [
    "#### Exercises Manipulating Tensors in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a869f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def functionA(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`\n",
    "  and returns the column sum of\n",
    "  `my_tensor1` multiplied by the sum of all the elmements of `my_tensor2`,\n",
    "  i.e., a scalar.\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      The multiplication of the column sum of `my_tensor1` by the sum of\n",
    "      `my_tensor2`.\n",
    "  \"\"\"\n",
    "  # TODO multiplication the sum of the tensors\n",
    "  output = my_tensor1.sum(axis=0) * my_tensor2.sum()\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionB(my_tensor):\n",
    "  \"\"\"\n",
    "  This function takes in a square matrix `my_tensor` and returns a 2D tensor\n",
    "  consisting of a flattened `my_tensor` with the index of each element\n",
    "  appended to this tensor in the row dimension.\n",
    "  Args:\n",
    "    my_tensor: torch.Tensor\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  # TODO flatten the tensor `my_tensor`\n",
    "  my_tensor = my_tensor.flatten()\n",
    "  # TODO create the idx tensor to be concatenated to `my_tensor`\n",
    "  idx_tensor = torch.arange(0, len(my_tensor))\n",
    "  # TODO concatenate the two tensors\n",
    "  output = torch.cat([idx_tensor.unsqueeze(1), my_tensor.unsqueeze(1)], axis=1)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionC(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`.\n",
    "  If the dimensions allow it, it returns the\n",
    "  elementwise sum of `my_tensor1`-shaped `my_tensor2`, and `my_tensor2`;\n",
    "  else this function returns a 1D tensor that is the concatenation of the\n",
    "  two tensors.\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  # TODO check we can reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "  if torch.numel(my_tensor1) == torch.numel(my_tensor2):\n",
    "    # TODO reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "    my_tensor2 = my_tensor2.reshape(my_tensor1.shape)\n",
    "    # TODO sum the two tensors\n",
    "    output = my_tensor1 + my_tensor2\n",
    "  else:\n",
    "    # TODO flatten both tensors\n",
    "    my_tensor1 = my_tensor1.reshape(1, -1)\n",
    "    my_tensor2 = my_tensor2.reshape(1, -1)\n",
    "    # TODO concatenate the two tensors in the correct dimension\n",
    "    output = torch.cat([my_tensor1, my_tensor2], axis=1).squeeze()\n",
    "\n",
    "  return output\n",
    "\n",
    "## Implement the functions above and then uncomment the following lines to test your code\n",
    "print(functionA(torch.tensor([[1, 1], [1, 1]]), torch.tensor([[1, 2, 3], [1, 2, 3]])))\n",
    "print(functionB(torch.tensor([[2, 3], [-1, 10]])))\n",
    "print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0, 2]])))\n",
    "print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0]])))\n",
    "\n",
    "assert torch.equal(functionA(torch.tensor([[1, 1], [1, 1]]), torch.tensor([[1, 2, 3], [1, 2, 3]])), torch.tensor([24, 24]))\n",
    "assert torch.equal(functionB(torch.tensor([[2, 3], [-1, 10]])), torch.tensor([[0, 2], [1, 3], [2, -1], [3, 10]]))\n",
    "assert torch.equal(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0, 2]])), torch.tensor([[3, 2], [-1, 5]]))\n",
    "assert torch.equal(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0]])), torch.tensor([1, -1, -1, 3, 2, 3, 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
